[
    {
        "page_content": "«% CENGAGE \nSOFTWARE TESTING \nISTQB CERTIFICATION \nm \no) \nc \n= \n- \nI \nm \nv \n= \nO \n&",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 0,
            "page_label": "1"
        }
    },
    {
        "page_content": "FOUNDATIONS OF \nSOFTWARE TESTING \nISTQB CERTIFICATION \nFOURTH EDITION \nDorothy Graham \nRex Black \nErik van Veenendaal \n7% CENGAGE \nAustralia « Brazil » Mexico » Singapore + United Kingdom + United States \ne Lo A1 R",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 1,
            "page_label": "2"
        }
    },
    {
        "page_content": "This is an electronic version of the print textbook. Due to electronic rights restrictions. \nsome third party content may be suppressed. Editorial review has deemed that any suppressed \ncontent does not materially affect the overall keaming experience. The publisher reserves the right \n1o remove content from this title at any time if subsequent rights restrictions require it. For \nvaluable information on pricing, previous editions, changes to current editions, and alternate \nformats, please visit www cengage comighered to scarch by ISBN#, author, title, or keyword for \nmaterials in your arcas of interest. \nImportant Notice: Media content referenced within the product description or the product \nfext may not be available in the eBook version. \nG N Crmpuge Lowming A3 gt Bt vt My et b o, s, o B . 0 s 18 . o, U 3 i, e, st sty comto ey b gyt B e el b At el e M e gyt oot s et Sy o oocrull g eTience Cengage Liwng AT e (0 4w ol o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 2,
            "page_label": "3"
        }
    },
    {
        "page_content": "«& CENGAGE \nFoundations of Software Testing: \nISTQB Certification, 4th Edition \nDorothy Graham, Rex Black, Erlk \nvan Veenendaal \nPublisher: Annabel Ainscow \nList Manager: Virginia Thorp \nMarketing Manager: Anna Reading. \nSenior Content Project Manager: \nMelissa Beavis \nManufacturing Buyer: Elaine Bevan \nTypesetter: SPi Global \nText Design: SPi Global \nCover Design: jonathan Bargus \nCover Image(sk © dem10/iStock/Getty \nImages \n© 2020, Cengage Learning EMEA \nWCN: 02-300 \nALL RIGHTS RESERVED. No part of this work may be \nreproduced, ransmitted, stored, distributed or used in any \nform or by any means, electronic, mechanical, phatocopying. \nrecording or otherwise, without the prior written permission \nof Cengage Learning or under license In the U.K. from the \nCopyright Licensing agency Ltd. \nThe Author(s) has/have asserted the right under the \nCopyright Designs and Patents Act 1983 to be identified as \nAuthor(s) of this Work., \nFor product information and technology assistance, \ncontact us at emea.info@cengage.com. \nFor permission to use material from this text or product \nand for permission queries, \nemail emea permissions@cengage.com. \n@ritish Ubrary Cotaloguing-in-Publication Doto \nAcatalogue record for this book is available from the British \nLibrary. \nISBN: 978-1-4737.6479-8 \nCengage Learning, EMEA \nCheriton House, North Way, \nAndaver, Hampshire, 5P10 58€ \nUnited Kingdom \nCengage Learning Is a leading provider of customized \nlearning solutions with employees residing in nearly 40 \ndifferent countries and sales in more than 125 countries \naround the workd. Find your local representative at: \nwww.cengage.co.uk. \nCengage Learning products are represented in Canada by \nNelson Education, Ltd. \nTo learn more about Cengage platforms and services, \nregister or access your online learning solution, or purchase \nmaterials for your course, visit www.cengage.com. \nPrinted in the United Kingdom by CPI, Antony Rowe \nPrint Number: 01 Print Year: 2019 \no N Compugs Losmng A3 R Bt vl Moy et b o, . o4 B 0 b 18 o, (ke 3 o, e, ot sty st ey b smppreneed b e ol smbis At el e M e gyt oo s o Sy S Tl g pericncs (g L TS e (W 4 Y ol s o s o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 3,
            "page_label": "4"
        }
    },
    {
        "page_content": "CONTENTS \nFigures and tables v \nAcknowledgements vi \nPreface vii \n1 Fundamentals of testing 1 \nSection | What is testing? | \nSection 2 Why is testing necessary? § \nSection 3 Seven testing principles 10 \nSection 4 Test process 15 \nSection 5 The psychology of testing 27 \nChapter review 33 \nSample exam questions 34 \n2 Testing throughout the software development life cycle 36 \nSection | Software development life cycle models 36 \nSection 2 Test levels 47 \nSection 3 Test types 62 \nSection 4 Maintenance testing 69 \nChapter review 72 \nSample exam questions 73 \n3 Static techniques 75 \nSection | Static techniques and the test process 75 \nSection 2 Review process 79 \nChapter review 100 \nSample exam questions 101 \nExercise 103 \nExercise solution 105 \n4 Test techniques 106 \nSection | Categories of test techniques 106 \nSection 2 Black-box test techniques 112 \nSection 3 White-box test techniques 132 \nSection 4 Experience-based test techniques 140 \nChapter review 143 \nSample exam questions 144 \nExercises 148 \nExercise solutions 149 \n5 Test management |54 \nSection 1 Test organization 154 \nSection 2 Test planning and estimation 161 \nSection 3 Test monitoring and control 175 \nSection 4 Configuration management 181 \no N Compoge Lossming. A3 K Bt vt My et b o, s, o4 b s, s 10 . o, U 3 o, g, s sty scmti ey b sy B e el smbs At e e b s oo st s & cvent barwny Conpage Lowmag muries e 141 10 ey ki o o 1} o € whacgacs A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 4,
            "page_label": "5"
        }
    },
    {
        "page_content": "Section 5 Risks and testing 183 \nSection 6 Defect management 190 \nChapler review 196 \nSample exam questions 197 \nExercises 200 \nExercise solutions 201 \n6 Tool support for testing 203 \nSection 1 Test tool considerations 203 \nSection 2 Effective use of tools 222 \nChapter review 225 \nSample exam questions 227 \n7 ISTQB Foundation Exam 22% \nSection | Preparing for the exam 228 \nSection 2 Taking the exam 230 \nSection 3 Mock exam 232 \nGlossary 241 \nAnswers to sample exam questions 253 \nReferences 257 \nAuthors 259 \nIndex 263 \no N Compogs Loy N3 Rt Bt vl Moy et b o, . o4 B i, 0 b 18 . o, (o 3 bt e, ot ot ey b smpppenmed b e ol b At 41 el e M e 0 gyt oo s e Sy o ol g eTience (g Lewing AT e (0 4 ¢ ol s o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 5,
            "page_label": "6"
        }
    },
    {
        "page_content": "FIGURES AND TABLES \nFigure 1.1 \nFigure 1.2 \nFigure 1.3 \nFigure 2.1 \nFigure 22 \nFigure 23 \nFigure 2.4 \nFigure 3.1 \nDocument 3.1 \nFigure 4.1 \nFigure 4.2 \nFigure 4.3 \nFigure 4.4 \nFigure 4.5 \nFigure 4.6 \nFigure 4.7 \nFigure 4.8 \nFigure 49 \nFigure 4.10 \nFigure 5.1 \nFigure 5.2 \nFigure 53 \nFigure 7.1 \nFigure 72 \nTable 1.1 \nTable 2.1 \nTable 3.1 \nTable 4.1 \nTable 4.2 \nTable 4.3 \nTable 4.4 \nTable 4.5 \nTable 4.6 \nTable 4.7 \nTable 4.8 \nTable 4.9 \nTable 4.10 \nTable 5.1 \nTable 5.2 \nTable 5.3 \nTable 5.4 \nTable 7.1 \nTable 7.2 \nFour typical scenarios 8 \nMultiplicative increases in cost 9 \nTime savings of carly defect removal 13 \nWaterfall model 38 \nV-model 39 \nTterative development model 41 \nStubs and drivers 49 \nBasic review roles for a work product under review 94 \nFunctional requirements specification 104 \nTest techniques 110 \nState diagram for PIN entry 128 \nPartial use case for PIN entry 131 \nControl flow diagram for Code samples 4.3 139 \nControl flow diagram for flight check-in 146 \nControl flow diagram for Question 15 146 \nState diagram for PIN entry 147 \nState diagram for shopping basket 151 \nControl flow diagram for drinks dispenser 153 \nControl flow diagram showing coverage of tests 153 \nTest case summary worksheet 177 \nTotal defects opened and closed chart 178 \nDefect report life cycle 195 \nControl flow diagram for flight check-in 234 \nState transition diagram 238 \nTesting principles 11 \nTest level characteristics 60 \nPotential defects in the functional requirements specification 105 \nEquivalence partitions and boundaries 116 \nEmpty decision table 122 \nDecision table with input combinations 123 \nDecision table with combinations and outcomes 123 \nDecision table with additional outcome 124 \nDecision table with changed outcomes 124 \nDecision table with outcomes in one row 125 \nDecision table for credit card example 126 \nCollapsed decision table for credit card example 126 \nState table for the PIN example 130 \nRisk coverage by defects and tests 180 \nA risk analysis template 189 \nExercise: Test execution schedule 200 \nSolution: Test execution schedule 201 \nDecision table for car rental 236 \nPriority and dependency table for Question 36 238 \not S04 Crmpoge Lowmng N3 Kt Bt vl Moy et b oo s o4 Bl 0 b 18 o (b 3 s, . ot ety st ey b smpqremed B e ol snbis gt s el e A e gyt oot s et Sy S Tl g ericncs (g Ly BTN e (W 14 Y ol s o s o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 6,
            "page_label": "7"
        }
    },
    {
        "page_content": "ACKNOWLEDGEMENTS \nThe materials in this book are based on the ISTQB Foundation Syllabus 2018, The \nFoundation Syllabus is copyrighted to the ISTQB (International Software Testing \nQualification Board). Permission has been granted by the ISTQB to the authors to \nuse these materials as the basis of a book, provided that recognition of authorship \nand copyright of the Syllabus itself is given. \nThe ISTQB Glossary of Testing Terms, released as version 3.2 by the ISTQB in \n2018 is used as the source of definitions in this book. \nThe co-authors would like to thank Dorothy Graham for her effort in updating \nthis book to be fully aligned with the 2018 version of the ISTQB Foundation Syllabus \nand version 3.2 of the ISTQB Glossary. \nBe aware that there are some defects in this book! The Syllabus, Glossary and this \nbook were written by people — and people make mistakes. Just as with testing, we \nhave applied reviews and tried to identify as many defects as we could, but we also \nneeded to release the manuseript 1o the publisher. Please let us know of defects that \nyou find in our book so that we can correct them in future printings. \nThe authors wish 1o acknowledge the contribution of Isabel Evans 10 a previous \nedition of this book. We also acknowledge contributions to this edition from Gerard \nBargh, Mark Fewster, Graham Frecburn, Tim Fretwell, Gary Rueda Sandoval, \nMelissa Tondi, Nathalie van Delft, Seretta Gamba, and Tebogo Makaba. \nDorothy Graham, Macclesfield, UK \nRex Black, Texas, USA \nErik van Veenendaal, Hato, Bonaire \n2019 \nvi \nG N Compoge Lossming. A3 Kigh Bt vt My et b o, . o4 B, 0 s 18 . o, (b 3 i, e, s sty st ey b gy B e el smbis AChapiti sl el s M e gyt oot s o Sy S el g ericnce (e Liwng TGS e (W 14 WY iRl s o s o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 7,
            "page_label": "8"
        }
    },
    {
        "page_content": "PREFACE \nThe purpose of this book is to support the ISTQB Foundation Syllabus 2018, which \nis the basis for the International Foundation Certificate in Software Testing. The \nauthors have been involved in helping to establish this qualification, donating their \ntime and energy 1o the Syllabus, terminology Glossary and the International Soft- \nware Testing Qualifications Board (ISTQB). \nThe authors of this book are all passionate about software testing. All have been \ninvolved in this area for most or all of their working lives, and have contributed 10 \nthe field through practical work. training courses and books. They have written this \nhook 10 help 1o promote the discipline of software testing. \nThe initial idea for this collaboration came from Erik van Veenendaal, author of \nThe Testing Practitioner, a book to support the ISEB Software Testing Practitioner \nCertificate. The other authors agreed to work together as equals on this book. Please \nnote that the order of the authors’ names does not indicate any seniority of authorship, \nbut simply which author was the last to update the book as the Foundation Syllabus \nevolved. \n‘We intend that this book will increase your chances of passing the Foundation \nCertificate exam. If you are taking a course (or class) to prepare for the exam, this \nhook will give you detailed and additional background about the topics you have \ncovered. If you are studying for the exam on your own, this book will help you be \nmore prepared. This book will give you information about the topics covered in the \nSyllabus, as well as worked exercises and practice exam questions (including a full \n40-question mock exam paper in Chapter 7). \nThis book is a useful reference work about software testing in general, even if you \nare not interested in the exam. The Foundation Certificate represents a distilling of \nthe essential aspects of software testing at the time of writing (2019), and this book \nwill give you a good grounding in software testing. \nISTQB AND CERTIFICATION \nISTQB stands for International Software Testing Qualifications Board and is an \norganization consisting of software testing professionals from cach of the countries \nwho are members of the ISTQB. Each representative is a member of a Software \nTesting Board in their own country. The purpose of the ISTQB is to provide \ninternationally accepted and consistent qualifications in software testing. ISTQB \nsets the Syllabus and gives guidelines for each member country to implement the \nqualification in their own country. The Foundation Certificate is the first interna- \ntionally accepted qualification in software testing and its Syllabus forms the basis \nof this book. \nFrom the first qualification in 1998 until the end of 2018, around 700,000 people \nhave taken the Foundation Certificate exam administered by a National Board of the \nISTQB, or by an Exam Board contracted to a National Board, This represents 86% \nof all ISTQB certifications. All ISTQB National Boards and Exam Boards recognize \ncach other's Foundation Certificates as valid. \no N Crmpoge Lossming. A3 Kgh Bt vt My et b o, s, o b, 0 s 10 . o, U 3 st g, s sty sntis ey b sy B e el smbs At el e A e gyt oo s e Sy S G el g eriencs Cengage Ly BTG e (W 14 WY iRl s o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 8,
            "page_label": "9"
        }
    },
    {
        "page_content": "The ISTQB qualification is independent of any individual training provider. Any \ntraining organization can offer a course based on this publicly available Syllabus. \nHowever, the National Boards associated with ISTQB give special approval to organi- \nzations that meet their requirements for the quality of the training. Such organizations \nare accredited and are allowed to have an invigilator or proctor from an authorized \nNational Board or Exam Board to give the exam as part of the accredited course. \nThe exam is also available independently from accrediting organizations or National \nBoards, \nWhy is certification of testers important? The objectives of the qualification are \nlisted in the Syllabus, They include: \n® Recognition for testing as an essential and professional software engineering \nspecialization. \n@ Enabling professionally qualified testers to be recognized by employers, cus- \ntomers and peers. \n@ Raising the profile of testers. \no Promoting consistent and good testing practices within all software engineering \ndisciplines internationally, for reasons of opportunity, communication and shar- \ning of knowledge and resources internationally. \nFINDING YOUR WAY AROUND THIS BOOK \nThis book is divided into seven chapters. The first six chapters of the book each \ncover one chapter of the Syllabus, and cach has some practice exam questions, \nChapter | is the start of understanding. We'll look at some fundamental questions: \nwhat is testing and why is it necessary”? We'll examine why testing is not just run- \nning tests. We'll also look at why testing can damage relationships and how bridges \nbetween colleagues can be rebuilt. \nIn Chapter 2, we'll concentrate on testing in relation 1o the common software \ndevelopment models, including iterative and waterfall models. We'll see that differemt \ntypes of testing are used at different stages in the software development life cycle. \nIn Chapter 3, we'll concentrate on test techniques that can be used early in the \nsoftware development life cycle. These include reviews and static analysis: tests done \nbefore compiling the code, \nChapter 4 covers test techniques. We'll show you technigues including equivalence \npartitioning, boundary value analysis, decision tables, state transition testing. use \ncase testing, statement and decision coverage and experience-based techniques, This \nchapter is about how to become a better tester in terms of designing tests. There are \nexercises for the most significant techniques included in this chapter, \nChapter 5 is about the management and control of testing, including estimation, \nrisk assessment, defect management and reporting. Writing a good defect report is a \nkey skill for a good tester, so we have an exercise for that too, \nIn Chapter 6, we'll show you how tools support all the activities in the test process, \nand how to select and implement tools for the greatest benefit. \nChapter 7 contains general advice about taking the exam and has the full 40-question \nmock paper. This is a key learning aid to help you pass the real exam. \no N Compoge Lossming. A3 K Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 s, g, s sty scnti ey b sy B e el smbs At el e A e gyt oo s o Sy w8 el g ericnce CEngage Ly AT e (W 14 Y iRl s o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 9,
            "page_label": "10"
        }
    },
    {
        "page_content": "The appendices of the book include a full list of references and a copy of the \nISTQB testing terminology Glossary, as well as the answers to all the practice exam \nquestions. \nTO HELP YOU USE THE BOOK \n1 Get a copy of the Syllabus: You should download the Syllabus from the ISTQR \nwebsite so that you have the current version, and so that you can check off the \nSyllabus objectives as you kearn. This is available at https:/fwww.istgborg/ \ndownloadsisyllabi/foundation-level-syllabus hml \n2 Understand what is meant by learning objectives and knowledge levels: In the \nSyllabus, you will see learning objectives and knowledge (or cognitive) levels at \nthe start of each section of each chapter. These indicate what you need to know \nand the depth of knowledge required for the exam. We have used the timings in \nthe Syllabus and knowledge levels to guide the space allocated in the book, both \nfor the text and for the exercises. You will sce the learning objectives and knowl- \nedge levels at the start of each section within each chapter. The knowledge levels \nexpected by the Syllabus are: \n@ KI: remember, recognize, recall: you will recognize. remember and recall \na term or concept. For example, you could recognize one definition of failure \nas ‘Non-delivery of service to an end user or any other stakeholder”. \n® K2: understand, explain, give reasons, compare, classify, summarize: \nyou can select the reasons or explanations for statements related to the topic, \nand can summarize, compare, classify and give examples for the testing \nconcept, For example, you could explain that one reason why tests should be \ndesigned as early as possible is to find defects when they are cheaper to remove. \no K3: apply: you can select the correct application of a concept or technique \nand apply it to a given context, For example, you could identify boundary \nvalues for valid and invalid partitions, and you could select test cases from a \ngiven state transition diagram in order to cover all transitions. \nRemember, as you go through the book, if a topic has a learning objective marked \nK1 you just need to recognize it. If it has a learning objective of K3 you will be \nexpected to apply your knowledge in the exam, for example. \n3 Use the Glossary of terms: Each chapter of the Syllabus has a number of terms \nlisted in it. You are expected 1o remember these terms at least at K1 level, even if \nthey are not explicitly mentioned in the learning objectives. You will see 4 number \nof definitions throughout this book. as in the sidebar. Definition A \nAll definitions of software testing terms (called keywords in the chapters)  description of the \nare taken from the ISTQB Glossary (version 3.2), which is available online at  Meaning of a word. \nwww glossary.isigb.org. A copy of this Glossary s also at the back of the book. \nAll the terms that are specifically mentioned in the Syllabus, that is, the ones you \nneed to learn for the exam, are mentioned in each section of this book. \nYou will notice that some terms in the Glossary at the back of this book are \nunderlined. These are terms that are mentioned specifically as keywords in the \nSyllabus. These are the terms that you need to be familiar with for the exam. \nG N Compoge Lossming. A3 Kghs Bt vt My et b o, s, o4 B . 0 s 18 . o, (b 3 i, e, s sty st ey b gy B e el smbs AChapiti s e e b s oo st s 8 cvent barwny Conpuge Lewmag murves e 14 10wy ke o o 1a} S € whargacm A (7 0Tv s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 10,
            "page_label": "11"
        }
    },
    {
        "page_content": "4 Use the references sensibly: We have referenced all the books used by the Syl- \nlabus authors when they constructed the Syllabus. You will see these underdined \nin the list at the end of the book. We also added references 1o some other books, \npapers and websites that we thought useful or which we referred to when writing. \nYou do not need 1o read all referenced books for the exam! However, you may \nfind some of them useful for further reading to increase your knowledge after the \nexam, and to help you apply some of the ideas you will come across in this book. \n5 Do the practice exams: When you get to the end of a chapter (for Chaplers 1 10 \n6), answer the exam questions, and then turn to ‘Answers to the Sample Exam \nQuestions’ to check if your answers were correct. After you have completed all of \nthe six chapters, then take the full mock exam in Chapter 7.1f you would like the \nmost realistic exam conditions. then allow yourself just an hour to take the exam \nin Chapter 7. Also take the free sample exams from the ISTQB web site. You can \ndownload both the exam and the answers including justifications for the correct \n(and wrong) answers. \not S0 Crmpoge Loty N3 Bt Bt ol My et b oo s o4 Bl 0 b 18 ot U 3 s, e, ot ety st ey b smpqremed B e ol sndis Ak 11 bl i M e gyt oot s ot Sy S G el g pericncs Cengage L BTG e (W 14 Y ol s o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 11,
            "page_label": "12"
        }
    },
    {
        "page_content": "Teaching & Learning \nSupport Resources \nCengage's peer reviewed content for higher and \nfurther education courses is accompanied by a range \nof digital teaching and learning support resources. \nThe resources are carefully tailored to the specific \nneeds of the instructor, student and the course. \n() A password protected area for instructors. \n[ An open-access area for students. \nLecturers: to discover the dedicated teaching digital \nsupport resources accompanying this textbook please \nregister here for access: \ncengage.com/dashboard/#login \nStudents: to discover the dedicated Learning digital \nsupport resources accompanying this textbook, please \nsearch for Foundations of Software Testing: ISTQB \nCertification, Fourth Edition on: cengage.com \nBE UNSTOPPABLE! \nLearn more at cengage.com",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 12,
            "page_label": "13"
        }
    },
    {
        "page_content": "",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 13,
            "page_label": "14"
        }
    },
    {
        "page_content": "CHAPTER ONE \nFundamentals of testing \nI n this chapter, we will introduce you to the fundamentals of testing: what software \ntesting is and why testing is needed, including its limitations, objectives and \npurpose; the principles behind testing: the process that testers follow, including \nactivities, tasks and work products; and some of the psychological factors that testers \nmust consider in their work. By reading this chapter you will gain an understanding \nof the fundamentals of testing and be able to describe those fundamentals. \nNote that the learning objectives start with *FL' rather than *LO’ to show that they \nare learning objectives for the Foundation Level qualification. \n1.1 WHAT IS TESTING? \nSYLLABUS LEARNING OBJECTIVES FOR 1.1 WHAT IS \nTESTING? (K2) \nFL-1.1.1  Identify typical objectives of testing (K1) \nFL-1.1.2  Differentiate testing from debugging (K2) \nIn this section, we will kick off the book by looking at what testing is, some miscon- \nceptions about testing, the typical objectives of testing and the difference between \ntesting and debugging. \nWithin cach section of this book, there are terms that are important - they are used \nin the section (and may be used elsewhere as well). They are listed in the Syllabus as \nkeywords, which means that you need to know the definition of the term and it could \nappear in an exam question. We will give the definition of the relevant keyword terms \nin the margin of the text, and they can also be found in the Glossary (including the \nISTQB online Glossary). We also show the keyword in bold within the section or \nsubsection where it is defined and discussed. \nIn this section, the relevant keyword terms are debugging. test object, test \nobjective, testing, validation and verification. \nSoftware is everywhere \nThe last 100 years have seen an amazing human triumph of technology. Diseases \nthat once killed and paralyzed are routinely treated or prevented — or even eradicated \nentirely, as with smallpox. Some children who stood amazed as they watched the \nfirst gasoline-powered automobile in their town are alive today, having seen people \nwalk on the moon, an event that happened before a large percentage of today's work- \nforce was even born, \n1 \noyt N Crmpogs Lossming. A3 Fightn Roerrnd My ot b copi, wamnd, o0 et o s 10 . e, U 3 vt g, acmms o sty commvs sy b sy B e ol snbhe gt T mview s devmed sy Je——— s & cven barwny o Conpae Lowmay mures e 1 1wt albmcnd o o 0} W € hacgacs A (7aTa s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 14,
            "page_label": "15"
        }
    },
    {
        "page_content": "2 Chapter 1 Fundamentals of testing \nPerhaps the most dramatic advances in technology have occurred in the arena \nof information technology. Software systems. in the sense that we know them, are \na recent innovation, less than 70 years old, but have already transformed daily life \naround the world. Thomas Watson, the one-time head of 1BM, famously predicted \nthat only about five computers would be needed in the whole world. This vastly \ninaccurate prediction was based on the idea that information technology was useful \nonly for business and govemnment applications, such as banking, insurance and con- \nducting a census, (The Hollerith punch-cards used by computers at the time Watson \nmade his prediction were developed for the United States census.) Now, everyone \nwho drives a car is using & machine not only designed with the help of computers, \nbut which also contains more computing power than the computers used by NASA \nto get Apollo missions to and from the Moon. Mobile phones are now essentially \nhandheld computers that get smarter with every new model. The Internet of Things \n(1oT) now gives us the ability to see who is at our door or turn on the lights when we \nare nowhere near our home. \nHowever, in the software world, the technological triumph has not been perfect. \nAlmost every living person has been touched by information technology, and most \nof us have dealt with the frustration and wasted time that occurs when software fails \nand exhibits unexpected behaviours. Some unfortunate individuals and companies \nhave experienced financial loss or damage 10 their personal or business reputations as \naresult of defective software. A highly unlucky few have even been injured or killed \nby software failures, including by self-driving cars. \nOne way to help overcome such problems is software testing, when it is done well, \nTesting covers activities throughout the life cycle and can have a number of different \nobjectives, as we will see in Section 111, \nTesting is more than running tests \nTesting The process An ongoing misperception, although less common these days, about testing is that \nconsisting of all kfe it only involves running tests. Specifically, some people think that testing involves \ncycle activities, both nothing beyond carrying out some sequence of actions on the system under test, sub- \nstatic and dynamic, mitting various inputs along the way and evaluating the observed results. Certainly, \nconcemed with these activities are one element of testing - specifically, these activities make up the \nplanning. preparation 1y of the test execution activities — but there are many other activities involved in \nm& and the test process. \nrelated work products We will discuss the test process in more detail later in this chapter (in Section 1.4), \n1o determine that but testing also includes (in addition to test execution): test planning, analyzing, \nthey satisfy specified designing and implementing tests, reporting test progress and results, and reporting \nrequirements, to defects. As you can see, there is a lot more to it than just running tests. \ndemonstrate that they Notice that there are major test activities both before and after test execution, \nare fit for purpose and In addition, in the ISTQB definition of software testing, you will see that testing \nto detect defects. includes both static and dynamic testing. Static testing is any evaluation of the soft- \nwitre or related work products (such as requirements specifications or user stories) \nthat occurs without executing the software itself. Dynamic testing is an evaluation \nof that software or related work products that does involve executing the software. \nAs such, the ISTQB definition of testing not only includes a number of pre-execution \nand post-execution activities that non-testers often do not consider ‘testing’, but also \nincludes software quality activities (for example, requirements reviews and static \nanalysis of code) that non-testers (and even sometimes testers) often do not consider \n“testing’ either. \nG N Crmpogs Lowming. A3 Kighs Bt vt My et b o, s, o4 e i, 0 s 10 . o, U 3 i, g, s sty comtr ey b gyt B e el snbs At Je—p—— e e b ety P Conpage Linmay murses e 1y 10wy akbacd o o 0} Vo € whacacs YA (70T s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 15,
            "page_label": "16"
        }
    },
    {
        "page_content": "Section 1 What is Testng? 3 \nThe reason for this broad definition is that both dynamic testing (at whatever level) \nand static testing (of whatever type) often enable the achievement of similar project \nobjectives. Dynamic testing and static testing also generate information that can help \nachieve an important process objective — that of understanding and improving the \nsoftware development and testing processes. Dynamic testing and static testing are \ncomplementary activities, each able to generate information that the other cannot. \nTesting is more than verification \nAnother common misconception about testing is that it is only about checking \ncorrectness; that is, that the system corresponds 1o ils requirements, user stories \nor other specifications. Checking against a specification (called verification) is  Verification \ncertainly part of testing, where we are asking the question, ‘Have we built the system  Confirmation by \ncorrectly?” Note the emphasis in the definition on ‘specified requirements’, examination and \nBut just conforming to a specification is not sufficient testing, as we will see in  1hrough provision of \nSection 1.37 (Absence-of-errors is a fallacy). We also need to test 1o see if the deliv- Objective evidence that \nered software and system will meet user and stakehokler needs and expectations in  SPeciied fequirements \nits operational environment. Often it is the tester who becomes the advocate for the \nend-user in this kind of testing. which is called validation. Here we are asking the  Validation \nquestion, *Have we built the right system?\" Note the emphasis in the definition on  Confirmation by \n“intended use’. examination and \nIn every development life cycle. a part of testing is focused on verification testing through provision of \nand a part is focused on validation testing. Verification is concerned with evaluating  ODIeCtive evidence that \nawork product, component or system to determine whether it meets the requirements e \nset. In fact, verification focuses on the question, ‘Is the deliverable built according :«m :nw \nto the specification? Validation is concerned with evaluating a work product, com- g ifiad \nponent or system to determine whether it meets the user needs and requirements. \nValidation focuses on the question, ‘Is the deliverable fit for purpose; for example, \ndoes it provide a solution to the problem?\" \n1.1.1 Typical objectives of testing \nThe following are some test objectives given in the Foundation Syllabus: Test objective A \no Toevaluate work products such as requirements, user stories, design and code l’ord:;hg and \nby using static testing techniques, such as reviews. executing a test. \no To verify whether all specified requirements have been fulfilled. for example, in \nthe resulting system. \n® To validate whether the test object is complete and works as the users and other \nstakeholders expect ~ for example, together with user or stakeholder groups. \n® Tobuild confidence in the level of quality of the test object, such as when those \ntests considered highest risk pass, and when the failures that are observed in the \nother tests are considered acceptable. \n® To prevent defects, such as when early test activities (for example, requirements \nreviews or carly test design) identify defects in requirements specifications that \nare removed before they cause defects in the design specifications and subse- \nquently the code itself. Both reviews and test design serve as a verification and \nvalidation of these test basis documents that will reveal problems that otherwise \nwould not surface until test execution, potentially much later in the project. \nG N Crmpogs Lowming. A3 K Bt vt My ek b o, s, o4 e i, 0 s 18 . o, U 3 ot g, s sty comto ey b gy B e el b At e e b et oo s et s 8 cmeni oy Conpuge Lonmay muries e 1y 10 ey kbacnd o o s} Vo € whacacm YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 16,
            "page_label": "17"
        }
    },
    {
        "page_content": "4  Chapter 1 Fundamentals of testing \no To find failures and defects; this is typically a prime focus for software testing. \no To provide sufficient information to stakeholders to allow them to make \nTest object The informed decisions, especially regarding the level of quality of the test object - \nw';m\\em or system for example, by the satisfaction of entry or exit criteria. \n10 be tested. ® To reduce the level of risk of inadequate software quality (e.g. previously \nundetected failures occurring in operation), \no To comply with contractual, legal or regulatory requirements or standards, \nandfor to verify the test object’s compliance with such requirements or \nstandards, \nThese objectives are not universal, Different test viewpoints, test levels and test \nstakeholders can have different objectives. While many levels of testing, such as \ncomponent, integration and system testing. focus on discovering as many failures as \npossible in order to find and remove defects, in acceptance testing the main objec- \ntive is confirmation of correct system operation (at least under normal conditions), \ntogether with building confidence that the system meets its requirements. The context \nof the test object and the software development life cycle will also affect what test \nobjectives are appropriate. Let's look at some examples to illustrate this, \nWhen evaluating a software puckage that might be purchased or integrated into o \nlarger software system, the main objective of testing might be the assessment of the \nquality of the software. Defects found may not be fixed, but rather might support a \nconclusion that the software be rejected. \nDuring component testing, one objective at this level may be to achieve a given \nlevel of code coverage by the component tests — that is, 1o assess how much of the \ncode has actually been exercised by a set of tests and to add additional tests to exercise \nparts of the code that have not yet been coveredftested. Another objective may be \n1o find as many failures as possible so that the underlying defects are identified and \nfixed as early as possible, \nDuring user acceptance testing, one objective may be to confirm that the sys- \ntem works as expected (validation) and satisfies requirements (verification). Another \nobjective of testing here is to focus on providing stakeholders with an evaluation of \nthe risk of releasing the system at a given time. Evaluating risk can be part of a mix \nof objectives, or it can be an objective of a separate level of testing, as when testing \na safety-critical system, for example. \nDuring maintenance testing, our objectives often include checking whether devel- \nopers have introduced any regressions (new defects not present in the previous ver- \nsion) while making changes, Some forms of testing, such as operational testing, \nfocus on assessing quality characteristics such as reliability, security, performance \nor availability. \n1.1.2 Testing and debugging \nLet's end this section by saying what testing is not, but is often thought to be. Testing \nDebugging The is not debugging. While dynamic testing often locates failures which are caused by \nprocess of finding, defects, and static testing often locates defects themselves, testing does not fix defects. \nanalyzing and removing It is during debugging. a development activity, that a member of the project team finds, \nthe causes of failures i 4 nlyzes and removes the defect, the underlying cause of the failure. After debugging., \nsoftware. there is a further testing activity associated with the defect, which is called confirma- \ntion testing. This activity ensures that the fix does indeed resolve the failure. \nG N Crmpoge Lowming. A3 g Bt vt My et b o, s, o4 e, s 10 . o, U 3 et g, s sty st ey b gy B e el smbs At s e e b ety oo s et e Tt Conpuge Lonmay muries e 1y 10 ey koo o o 1} o € whbacgacm A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 17,
            "page_label": "18"
        }
    },
    {
        "page_content": "Section 2 Why is Testing Necessary? S5 \nIn terms of roles, dynamic testing is a testing role, debugging is a development role \nand confirmation testing is again a testing role. However, in Agile teams, this distinc- \ntion may be blurred, as testers may be involved in debugging and component testing. \nFurther information about software testing concepts can be found in the ISO \nstandard ISO/IEC/IEEE 29119-1 [2013). \n1.2 WHY IS TESTING NECESSARY? \nSYLLABUS LEARNING OBJECTIVES FOR 1.2 WHY IS TESTING \n(K2) \nFL-1.2.1 Give examples of why testing is necessary (K2) \nFL-1.2.2  Describe the relationship between testing and quality \nassurance and give examples of how testing contributes to \nhigher quality (K2) \nFL-1.2.3  Distinguish between error, defect and failure (K2) \nFL-124  Distinguish between the root cause of a defect and its effects (K2) \nIn this section, we discuss how testing contributes to success and the relationship \nbetween testing and quality assurance. We will describe the difference between \nerrors, defects and failures and illustrate how software defects or bugs can cause \nproblems for people, the environment or a company. We will draw important distine- \ntions between defects, their root causes and their effects, \nAs we go through this section, watch for the Syllabus terms defect, error. failure, \nquality, quality assurance and root cause. \nTesting can help to reduce the risk of failures occurring during operation, provided \nit is carried out in a rigorous way, including reviews of documents and other work \nproducts. Testing both verifies that a system is correctly built and validates that it \nwill meet users” and stakeholders’ needs, even though no testing is ever exhaustive \n(see Principle 2 in Section 1.3, Exhaustive testing is impossible). In some situations, \ntesting may not only be helpful, but may be necessary 1o meet contractual or legal \nrequirements or to conform to industry-specific standards, such as automotive or \nsafety-critical systems. \n1.2.1 Testing's contributions to success \nAs we mentioned in Section 11, all of us have experienced software problems; for \nexample, an app fails in the middle of doing something. a website freezes while \ntaking your payment (did it go through or not?) or inconsistent prices for exactly \nthe same flights on travel sites. Failures like these are annoying. but failures in \nsafety-critical software can be life-threatening, such as in medical devices or \nself-driving cars. \nThe use of appropriate test techniques, applied with the right level of test expertise \nat the appropriate test levels and points in the software development life cycle, can \nbe of significant help in identifying problems so that they can be fixed before the \no N Crmpoge Lossming. A3 K Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 st g, s sty comtos ey b sy B e el smbs AChapati s e T Je——— s & cven sy Conpuge Lowmag muries e 1y 10 ey akbmcnd o o a1 S € whacgacm A (7 0Tu s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 18,
            "page_label": "19"
        }
    },
    {
        "page_content": "6 Chapter 1 Fundamentals of testing \nsoftware or system is released into use. Here are some examples where testing could \ncontribute to more successful systems: \n@ Having testers involved in requirements reviews or user story refinement could \ndetect defects in these work products before any design or coding is done for the \nfunctionality described. Identifying and removing defects at this stage reduces \nthe risk of the wrong software (incorrect or untestable) being developed, \n® Having testers work closely with system designers while the system is being \ndesigned can increase each party's understanding of the design and how to test \nit. Since misunderstandings are often the cause for defects in software, having a \nbetter understanding at this stage can reduce the risk of design defects. A bonus \nis that tests can be identified from the design ~ thinking about how to test the \nsystem at this stage often results in better design. \n® Having testers work closely with developers while the code is under develop- \nment can increase cach party’s understanding of the code and how 1o test it. As \nwith design, this increased understanding, and the knowledge of how the code \nwill be tested, can reduce the risk of defects in the code (and in the tests). \no Having testers verify and validate the software prior 1o release can detect \nfailures that might otherwise have been missed — this is traditionally where \nthe focus of testing has been. As we see with the previous examples, if we \nleave it until release, we will not be nearly as efficient as we would have \nbeen if we had caught these defects carlier. However, it is still necessary \nto test just before release, and testers can also help to support debugging \nactivities, for example, by running confirmation and regression tests. \nThus, testing can help the software meet stakeholder needs and satisfy \nrequirements. \nIn addition to these examples, achieving the defined test objectives (see \nSection 1.1.1) also contributes to the overall success of software development and \nmaintenance. \n1.2.2 Quality assurance and testing \nIs quality assurance (QA) the same as testing? Many people refer to ‘doing QA\" \nwhen they are actually doing testing, and some job titles refer to QA when they \nreally mean testing. The two are not the same. Quality assurance is actually one \npart of a larger concept, quality management, which refers to all activities that direct \nQuality assurance and control an organization with regard to quality in all aspects, Quality affects \nPart of uality not only software development but also human resources (HR) procedures, delivery \nW'L:m processes and even the way people answer the company’s telephones. \nm\"l ed Mp m‘\"gl Quality management consists of a number of activities, including quality \nrequirements will assurance and quality control (as well as setting quality objectives, quality plan- \nbe fulfilled. ning and quality improvement). Quality assurance is associated with ensuring that a \ncompany’s standard ways of performing various tasks are carried out correctly. Such \nQuality The degree 10 procedures may be written in a quality handbook that everyone is supposed to follow, \nwhich a component, The idea is that if processes are carried out correctly, then the products produced \nsystem or process meets ;) o of higher quality. Root cause analysis and retrospectives are used to help to \nspecified requiements ... rocesses for more effective quality assurance, If they are following a rec- \nexpectations. ognized quality management standard, companies may be audited to ensure that they \ni * doactually follow their prescribed processes (say what you do, and do what you say). \nGt N Crmpoge Lowming. A3 K Bt vt My ek b o, s, o4 e, 0 s 18 . o, U 3 s g, s sty s ey b gy B e el snbs At s Je—p—— e e b et P Conpage Linmay muries e 1y 10 ey kbacnd o o a1 o € whacacs YA (7T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 19,
            "page_label": "20"
        }
    },
    {
        "page_content": "Section 2 Why 15 Testing Necessary? 7 \nQuality control is concerned with the quality of products rather than processes, to \nensure that they have achieved the desired level of quality. Testing is looking at work \nproducts, including software, so it is actually a quality control activity rather than a qual- \nity assurance activity, despite common usage. However, testing also has processes that \nshould be followed correctly, so quality assurance does support good testing in this way. \nSections L1.1 and 1.2.1 describe how testing contributes to the achievement of quality. \nSo, we see that testing plays an essential supporting role in delivering quality soft- \nware. However, testing by itsell is not sufficient, Testing should be integrated into a \ncomplete, team-wide and development process-wide set of activities for quality assur- \nance. Proper application of standards, training of staff, the use of retrospectives to \nlearn lessons from defects and other important elements of previous projects, rigorous \nand appropriate software testing: all of these activities and more should be deployed \nby organizations to ensure acceptable levels of quality and quality risk upon release. \n1.2.3 Errors, defects and failures \nWhy does software fail? Part of the problem is that, ironically. while computeriza- \ntion has allowed dramatic automation of many professions, software engineering \nremains a human-intensive activity. And humans are fallible beings. So, software is \nfallible because humans are fallible. \nThe precise chain of events goes something like this. A developer makes an error \n(or mistake), such as forgetting about the possibility of inputting an excessively long  Ervor (mistake) A \nstring into a field on a screen. The developer thus puts a defect (or fault or bug) into  human action that \nthe program, such as omitting a check on input strings for length prior to process-  Produces an incorrect \ning them. When the program is executed, if the right conditions exist (or the wrong result. \nconditions, depending on how you look at it), the defect may result in unexpected  pefect (bug, fault) An \nbehaviour: that is. the system exhibits a failure, such as accepting an over-long input  imperfection or \nthat it should reject, with subsequent corruption of other data. deficiency in a work \nOther sequences of events can result in eventual failures, too. A business analyst  product where it \ncan introduce a defect into a requirement, which can escape into the design of the  does not meet its \nsystem and further escape into the code. For example, a business analyst might say ~ réquirements or \nthat an e-commerce system should support 100 simultancous users, but actually peak specifications. \nload should be 1,000 users. If that defect is not detected in a requirements review (see¢  Fajlure An event in \nChapter 3), it could escape from the requirements phase into the design and implementa-  which a component \ntion of the system. Once the load exceeds 100 users, resource utilization may eventually  or system does not \nspike to dangerous levels, leading to reduced response time and reliability problems. perform a required \nAtechnical writer can introduce a defect into the online help screens, For example,  function within \nsuppose that an accounting system is supposed to multiply two numbers together, but specified limits. \nthe help screens say that the two numbers should be added. In some cases, the system \nwill appear to work properly, such as when the two numbers are both 0 or both 2. \nHowever, most frequently the program will exhibit unexpected results (at least based \non the help screens). \nSo, human beings are fallible and thus, when they work, they sometimes introduce \ndefects. It is important to point out that the introduction of defects is not a purely \nrandom accident, though some defects may be introduced randomly, such as when \na phone rings and distracts a systems engineer in the middle of a complex series of \ndesign decisions. The rate at which people make errors increases when they are under \ntime pressure, when they are working with complex systems, interfaces or code, and \nwhen they are dealing with changing technologies or highly interconnected systems. \nG N Compoge Lowming. A3 Kighs Bt vt My et b o, s, o4 e, 0 s 10 . o, (b 3 i g, s sty comto ey b gy B e el smbs AChagit s el e A e s gyt oot s o Sy o el g pericncs Cengg Liwing TGS e (W 14 WY ol o o e o € g 30 (s s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 20,
            "page_label": "21"
        }
    },
    {
        "page_content": "8 Chapter 1 Fundamentals of testing \nWhile we commonly think of failures being the result of ‘bugs in the code’, a \nsignificant number of defects are introduced in work products such as requirements \nspecifications and design specifications. Capers Jones reports that about 20% of \ndefects are introduced in requirements, and about 25% in design. The remaining 55% \nare introduced during implementation or repair of the code, metadata or documen- \ntation [Jones 2008]. Other experts and researchers have reached similar conclusions, \nwith one organization finding that as many as 75% of defects oniginate in require- \nments and design, Figure 1.1 shows four typical scenarios, the upper stream being \ncorrect requirements, design and implementation, the lower three streams showing \ndefect introduction at some phase in the software life cycle. \nIdeally, defects are removed in the same phase of the life cycle in which they are \nintroduced. (Well, ideally defects are not introduced at all, but this is not possible \nbecause, as discussed before, people are fallible.) The extent 1o which defects are \nremoved in the phase of introduction is called phase containment. Phase containment \nis important because the cost of finding and removing a defect increases each time \nthat defect escapes to a later life cycle phase. Multiplicative increases in cost, of the \nsort seen in Figure 1.2, are not unusual. The specific increases vary considerably, \nwith Bochm reporting cost increases of 155 (from requirements to after release) for \nsimple systems, to as high as 1:100 for complex systems [Bochm 1986]. If you are \ncurious about the economics of software testing and other quality-related activities, \nyou can see Gilb [1993], Black [2004] or Black [2009]. \nDefects may result in failures, or they may not, depending on inputs and other con- \nditions. In some cases, a defect can exist that will never cause a failure in actual use, \nbecause the conditions that could cause the failure can never arise. In other cases, a defect \ncan exist that will not cause a failure during testing, but which always results in failures \nin production. This can happen with security, reliability and performance defects, espe- \ncially if the test environments do not closely replicate the production environment(s). \nBusiness Work Requre-  System Work Program- Work \nA:? Product s ment  Architect Productls  Desgn mer Products  Code \nwph A § =)o § o) \nBusiness  Work Requre-  System Work Progeam-  Work \nFIGURE 1.1 Four typical scenarios \nG N Crmpoge Lowming. A3 Kighs Bt vt My et b o, s, o4 s, s 18 . o, (b 3 i g, s sty cmto ey b gy B e el b At s el e M e gyt oot s o Sy o el g pericnce Cengage Lewing BTN e (W 14 Y adlbncnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 21,
            "page_label": "22"
        }
    },
    {
        "page_content": "Section 2 Why is Testing Necessary? 9 \nRequirement Desgn CodM.Im Inﬂqmd'm Am \nCost \nto \nRepair \nFIGURE 1.2 Multiphcative ncreases in cost \nIt can also happen that expected and actual results do not match for reasons other \nthan a defect. In some cases, environmental conditions can lead to unexpected results \nthat do not relate to a software defect. Radiation, magnetism, electronic fiekds and \npollution can damage hardware or firmware, or simply change the conditions of the \nhardware or firmware temporarily in a way that causes the software to fail. \n1.2.4 Defects, root causes and effects \nTesting also provides a learning opportunity that allows for improved quality \nif lessons are learned from cach project. If root cause analysis is carried out for \nthe defects found on each project, the team can improve its software development \nprocesses to avoid the introduction of similar defects in future systems. Through \nthis simple process of learning from past mistakes, organizations can continuously \nimprove the quality of their processes and their software. A root cause is generally  Root cause A source \nan organizational issue, whereas a cause for a defect is an individual action. So, for  of a defect such that \nexample, if a developer puts a ‘less than” instead of “greater than’ symbol, this error  If ILis removed, the \nmay have been made through carclessness. but the carclessness may have been made  Occurrence of the \nworse because of intense time pressure 1o complete the module quickly. With more dofect type & decreased \ntime for checking his or her work, or with better review processes. the defect would o \nnot have got through to the final product. It is human nature to blame individuals \nwhen in fact organizational pressure makes errors almost inevitable. \nThe Syllabus gives a good example of the difference between defects, root couses \nand effects: suppose that incorrect interest payments result in customer complaints. \nThere is just a single line of code that is incorrect. The code was written for a user \nstory that was ambiguous, so the developer interpreted it in a way that they thought \nwas sensible (but it was wrong). How did the user story come to be ambiguous? In \nthis example, the product owner misunderstood how interest was 1o be calculated, \nso was unable to clearly specify what the interest calculation should have been. This \nmisunderstanding could lead 1o a lot of similar defects, due to ambiguities in other \nuser stories as well. \not S Compogs Loy N3 B Bt vl My et b oo, . o4 B 0 b 18 ot U 3 o, e, ot oty st ey b gt b e ol snbis At s el i M e 5 gyt oot s e sy 4 el ey ericncs Cengage Ly BTG e (W 14 Y adiBicnd o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 22,
            "page_label": "23"
        }
    },
    {
        "page_content": "10 Chapter 1 Fundamentals of testing \nThe failure here is the incorrect interest calculations for customers. The defect \nis the wrong calculation in the code. The root cause was the product owner’s lack \nof knowledge about how interest should be calculated, and the effect was customer \ncomplaints. \nThe root cause can be addressed by providing additional training in interest rate \ncaleulations 1o the product owner, and possibly additional reviews of user stories by \ninterest calculation experts. If this is done, then incorrect interest cakculations due o \nambiguous user stories should be a thing of the past. \nRoot cause analysis is covered in more detail in two other ISTQB qualifications: \nExpert Level Test Management, and Expert Level Improving the Test Process, \n1.3 SEVEN TESTING PRINCIPLES \nSYLLABUS LEARNING OBJECTIVES FOR 1.3 SEVEN TESTING \nPRINCIPLES (K2) \nFL-13.1 Explain the seven testing principles (K2) \nIn this section, we will review seven fundamental principles of testing that have been \nobserved over the last 404 years. These principles. while not always understood or \nnoticed, are in action on most if not all projects. Knowing how 10 spot these princi- \nples, and how to take advantage of them, will make you a better tester. \nIn addition to the descriptions of each principle below, you can refer to Table 1.1 \nfor a quick reference of the principles and their text as written in the Syllabus. \nPrinciple 1. Testing shows the presence of defects, not their absence \nAs mentioned in the previous section, a typical objective of many testing efforts is 1o \nfind defects. Many testing organizations that the authors have worked with are quite \neffective @t doing so. One of our exceptional clients consistently finds, on average, \n99.5% of the defects in the software it tests. In addition, the defects left undiscovered \nare less important and unlikely to happen frequently in production. Sometimes, it \nturns out that this test team has indeed found 100% of the defects that would matter \nto customers, as no previously unreported defects are reported after release. Unfor- \ntunately, this level of effectiveness is not common. \nHowever, no test team, test technique or test strategy can guarantee 1o achieve \n100% defect-detection percentage (DDP) — or even 95%. which is considered \nexcellent. Thus, it is important to understand that, while testing can show that defects \nare present, it cannot prove that there are no defects left undiscovered. Of course, \nas testing continues, we reduce the likelihood of defects that remain undiscovered, \nbut eventually a form of Zeno's paradox takes hold: each additional test run may cut \nthe risk of a remaining defect in half, but only an infinite number of tests can cut \nthe risk down 1o zero. \nThat said, testers should not despair or let the perfect be the enemy of the good. \nWhile testing can never prove that the software works, it can reduce the remaining \nlevel of risk to product quality to an acceptable level, as mentioned before. In any \nendeavour worth doing, there is some risk. Software projects — and software testing ~ \nare endeavours worth doing, \nG N Crmpoge Lowming. A3 Kighs Bt vt My et b o, . o4 e . 0 s 18 . o, (b 3 i e, s sty st ey b gy B e el b At s e e dd sy commem st s o cvent barwny Conpge Lowmay muries e 1y 10 ey akbmcnd o o e} o € hacacs A (7 0Tv s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 23,
            "page_label": "24"
        }
    },
    {
        "page_content": "Section 3 Seven Testing Principles 11 \nTABLE 1.1 Testing prnciples \nPrinciple 1: Testing shows Testing can show that defects are present, but cannot prove \nthe presence of that there are no defects. Testing reduces the probabiity of \ndefects, not their undiscovered defects remaining in the software but, even if no \nabsence defects are found, testing & not a proof of correctness. \nPrinciple 2: Exhaustive testing  Testing everything (all combinations of inputs and preconditions) is \nis impossible not feasible except for trivial cases. Rather than attempting to test \nexhaustively, risk analysis, test techniques and priorities should be \nused to focus test efforts. \nPrinciple 3: Early testing saves  To find defects early, both static and dynamic test actmities should be \ntime and money started as early as possible in the software development life cycle, \nEarly testing is sometimes referred to as 'shift left’. Testing early \nin the software development life cyde helps reduce or eliminate \ncostly changes (see Chapter 3, Section 3.1). \nPrinciple 4: Defects duster A small number of modules usually contains most of the defects \ntogether discovered during pre-release testing, or they are responsible for \nmost of the operational failures. Predicted defect dlusters, and \nthe actual observed defect clusters in test or operation, are an \nimportant input into a risk analysis used to focus the test effort \n{as mentioned in Principle 2). \nPrinciple 5: Beware of the If the same tests are repeated over and over again, eventually these \npesticide paradox tests no longer find any new defects. To detect new defects, \nexisting tests and test data are changed and new tests need to be \nwritten. (Tests are no longer effective at finding defects, just as \npesticides are no longer effectve at killing insects after a while) \nIn some cases, such as automated regression testing, the pesticide \nparadox has a beneficial outcome, which is the relatively low \nnumber of regression defects. \nPrinciple 6: Testing is context  Testing is done differently in different contexts. For example, \ndependent safety-critical software is tested differently from an e-commerce \nmobde app. As another example, testing in an Agile project is \ndone differently to testing in a sequential life cycle project (see \nChapter 2, Section 2.1). \nPrinciple 7: Absence-of-errors  Some organizatiors expect that testers can run all possible tests and \nis a fallacy find all possbie defects, but Principles 2 and 1, respectively, tell us \nthat this is impossible. Further, it is a fallacy to expect that just finding \nand fixing a large number of defects will ensure the success of a \nsystem. For example, thoroughly testing al specified requirements. \nand fixing all defects found could still produce a system that is \ndifficult to use, that does not fulfil the users’ needs and expectations \nor that is inferior compared to other competing systems. \nG N Compuge Lowming. A3 K Bt vt My et b o, s, o4 b, 0 s 10 . o, (b 3 i e, s sty commo ey b gy B e el smbs At e e dmd ety o st s 8 cmen barwny Conprge Lowmay muries e 1y 10 ey koo o o 0} o € whbacgacm A (70T ts g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 24,
            "page_label": "25"
        }
    },
    {
        "page_content": "12 Chapter 1 Fundamentals of testing \nPrinciple 2. Exhaustive testing is impossible \nThis principle is closely related to the previous principle. For any real-sized system \n(anything beyond the trivial software constructed in first-year software engineering \ncourses), the number of possible test cases is either infinite or so close 1o infinite as \nto be practically innumerable. \nInfinity is a tough concept for the human brain to comprehend or accept, so let’s \nuse an example. One of our clients mentioned that they had caleulated the number \nof possible internal data value combinations in the Unix operating system as greater \nthan the number of known molecules in the universe by four orders of magnitude. \nThey further calculated that, even with their fastest automated tests, just to test all \nof these internal state combinations would require more time than the current age of \nthe universe. Even that would not be a complete test of the operating system; it would \nonly cover all the possible data value combinations. \nSo, we are confronted with a big. infinite cloud of possible tests: we must select \na subset from it. One way to select tests is to wander aimlessly in the cloud of tests, \nselecting at random until we run out of time. While there is a place for antomated \nrandom testing, by itself it is a poor strategy. We'll discuss testing strategies further \nin Chapter 5, but for the moment let’s look at two, \nOne strategy for selecting tests is risk-based testing. In risk-based testing, we \nhave a cross-functional team of project and product stakeholders perform a special \ntype of risk analysis. In this analysis, stakeholders identify risks to the quality of the \nsystem, and assess the level of risk (often using likelihood and impact) associated \nwith cach risk item. We focus the test effort based on the level of nisk, using the level \nof risk to determine the appropriate number of test cases for each risk item, and also \nto sequence the test cases. \nAnother strategy for selecting tests is requirements-based testing, In \nrequirements-based testing, testers analyze the requirements specification (which \nwould be user stories in Agile projects) to identify test conditions. These test condi- \ntions inherit the priority of the requirement or user story they derive from. We focus \nthe test effort based on the priority to determine the appropriate number of test cases \nfor each aspect, and also 1o sequence the test cases. \nPrinciple 3. Early testing saves time and money \nThis principle tells us that we should start testing as early as possible in order to find \nas many defects as possible. In addition, since the cost of finding and removing a \ndefect increases the longer that defect is in the system, early testing also means we \nare likely to minimize the cost of removing defects. \nSo, the first principle tells us that we cannot find all the bugs, but rather can only \nfind some percentage of them, The second principle tells us that we cannot run every \npossible test. The third principle tells us to start testing carly. What can we conclude \nwhen we put these three principles together” \nImagine that you have a system with 1,000 defects. Suppose we wait until the very \nend of the project and run one level of testing, system test. You find and fix 90% of \nthe defects. That still leaves 100 defects, which presumably will escape to the cus- \n1OmErs OF users. \nInstead, suppose that you start testing early and continue throughout the life \ncycle. You perform requirements reviews, design reviews and code reviews. You \nperform unit testing, integration testing and system testing. Suppose that, during \neach test activity, you find and remove only 45% of the defects — half as effective \nG N Compogs Lowming. A3 g Bt vt My et b o, s, o4 e, 0 s 10 . o, (b 3 o, e, s sty comtr ey b gy B e el b At s el s M e gyl oot s et Sy o ol g pericncs Cengage Lewing BTG e (W 14 Y ol o o e o € g 430 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 25,
            "page_label": "26"
        }
    },
    {
        "page_content": "Section 3 Seven Testing Principles 13 \nas the previous system test level. Nevertheless, at the end of the process, fewer than \n30 defects remain. Even though each test activity was only 45% effective at finding \ndefects, the overall sequence of activities was 97% effective. Note that now we are \ndoing both static testing (the reviews) and dynamic testing (the running of tests at the \ndifferent test levels). This approach of starting test activities as early as possible is also \ncalled ‘shift left” because the test activities are no longer all done on the right-hand \nside of a sequential life cycle diagram. but on the left-hand side at the beginning of \ndevelopment. Although unit test execution is of course on the right side of a sequential \nlife cycle diagram, improving and spending more effort on unit testing early on is a \nvery important part of the shift left paradigm. \nIn addition, defects removed early cost less to remove. Further, since much of the \ncost in software engineering is associated with human effort, and since the size of \na project team is relatively inflexible once that project is underway, reduced cost of \ndefects also means reduced duration of the project. That situation is shown graphi- \ncally in Figure 1.3, \nNow, this type of cumulative and highly efficient defect removal only works if each \nof the test activities in the sequence is focused on different, defined objectives. If we \nsimply test the same test conditions over and over, we will not achieve the cumulative \neffect, for reasons we will discuss in a moment. \nTime Savings of Eardy \nDefect Removal Fechataimavbiads \nT 4 Y \nI > \nS/ \nr / \nY. 7 \n>4 !/ \ne r / Removed ] | \n/ 1 \n¥ | \n] I \nJ o I \nI I \n! | \ne + 2 \nFIGURE 1.3 Time savings of early defect removal \nPrinciple 4. Defects cluster together \nThis principle relates to something we discussed previously, that relying entirely \non the testing strategy of a random walk in the infinite cloud of possible tests is \nrelatively weak. Defects are not randomly and uniformly distributed throughout the \nsoftware under test. Rather, defects tend to be found in clusters, with 20% (or fewer) \not S04 Crmpoge Loy N3 Bt Bt ol My et b o . o4 bl 0 b 18 ot (b 3 s, . ot ety st ey b smpqpemed B e ol snbis gt 11 el s M e gyt oo s o Sy S Tl g eriencs Cengage L BTG e (W 14 WY iRl s o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 26,
            "page_label": "27"
        }
    },
    {
        "page_content": "14 Chapter 1 Fundamentals of testing \nof the modules accounting for 80% (or more) of the defects. Tn other words, the \ndefect density of modules varies considerably. While controversy exists about why \ndefect clustering happens, the reality of defect clustering is well established, It was \nfirst demonstrated in studies performed by 1BM in the 1960s [Jones 2008], and is \nmentioned in Myers [2011). We continue to see evidence of defect clustering in our \nwork with clients, \nDefect clustering is helpful to us as testers, because it provides a useful guide. If we \nfocus our test effort (at least in part) based on the expected (and ultimately observed) \nlikelihood of finding a defect in a certain area, we can make our testing more effective \nand efficient, at least in terms of our objective of finding defects. Knowledge of and \npredictions about defect clusters are important inputs to the risk-based testing strategy \ndiscussed carlier. In a metaphorical way, we can imagine that bugs are social creatures \nwho like 1o hang out together in the dark corners of the software. \nPrinciple 5. Beware of the pesticide paradox \nThis principle was coined by Boris Beizer [Beizer 1990]. He observed that, just as \na pesticide repeatedly sprayed on a field will kill fewer and fewer bugs each time \nit is used, 5o too a given set of tests will eventually stop finding new defects when \nre-run against a system under development or maintenance. If the tests do not pro- \nvide adequate coverage, this slowdown in defect finding will result in a false level of \nconfidence and excessive optimism among the project team. However, the air will be \nlet out of the balloon once the system is released to customers and users, \nUsing the right test strategies is the first step towards achieving adequate coverage. \nHowever, no strategy is perfect. You should plan to regulardy review the test results \nduring the project, and revise the tests based on your findings. In some cases, you need \nto write new and different tests to exercise different parts of the software or system. \nThese new tests can lead to discovery of previously unknown defect clusters, which \nis a good reason not 1o wait until the end of the test effort to review your test results \nand evaluate the adequacy of test coverage. \nThe pesticide paradox is important when implementing the multilevel testing dis- \ncussed previously in regards to the principle of early testing. Simply repeating our \ntests of the same conditions over and over will not result in good cumulative defect \ndetection. However, when used properly, each type and level of testing has its own \nstrengths and weaknesses in terms of defect detection, and collectively we can assem- \nble a very effective sequence of defect filters from them. After such a sequence of \ncomplementary test activities, we can be confident that the coverage is adequate, and \nthat the remaining level of risk is acceptable. \nSometimes the pesticide paradox can work in our favour, if it is not new defects that \nwe are looking for. When we run automated regression tests, we are ensuring that the \nsoftware that we are testing is still working as it was before; that is, there are no new \nunexpected side-effect defects that have appeared as a result of a change elsewhere. \nIn this case, we are pleased that we have not found any new defects. \nPrinciple 6. Testing is context dependent \nOur safety-critical clients test with a great deal of rigour and care ~ and cost. When \nlives are at stake, we must be extremely careful to minimize the risk of undetected \ndefects, Our clients who release software on the web, such as e-commerce sites, or \nwho develop mobile apps, can take advantage of the possibility to quickly change \nthe software when necessary, leading to a different set of testing challenges — and \nopportunities, If you tried to apply safety-critical approaches to a mobile app, you \nG N Crmpogs Lowming. A3 Kighs Bt vt My et b o, s, o4 B, 0 s 18 . o, U 3 i e, s sty comto ey b gyt B e el snbs At Je—p—— e e b et P Conpage Lonmay murses e 1y 10 ey kbacnd o o 0} S € whacgacs YIS (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 27,
            "page_label": "28"
        }
    },
    {
        "page_content": "might put the company out of business: if you tried to apply e-commerce approaches \nto safety-critical software, you could put lives in danger. So, the context of the test- \ning influences how much testing we do and how the testing is done. \nAnother example is the way that testing is done in un Agile project as opposed to \no sequential life cycle project. Every sprint in an Agile project includes testing of the \nfunctionality developed in that sprint; the testing is done by everyone on the Agile \nteam (ideally) and the testing is done continually over the whole of development. In \nsequential life cycle projects, testing may be done more formally, documented in more \ndetail and may be focused towards the end of the project. \nPrinciple 7. Absence-of-errors is a fallacy \nThroughout this section we have expounded the idea that a sequence of test activ- \nities, started early and targeting specific and diverse objectives and areas of the \nsystem, can effectively and efficiently find — and help a project team to remove - a \nlarge percentage of the defects. Surely that is all that is required to achieve project \nsuccess? \nSadly, it is not. Many systems have been built that failed in user acceptance testing \nor in the marketplace, such as the initial launch of the US healthcare.gov website, \nwhich suffered from serious performance and web access problems. \nConsider desktop computer operating systems. In the 1990s, as competition \npeaked for dominance of the PC operating system market, Unix and its variants had \nhigher levels of quality than DOS and Windows. However, 25 years on, Windows \ndominates the desktop marketplace. One major reason is that Unix and its variants \nwere too difficult for most users in the early 1990s. \nConsider a system that perfectly conforms to its requirements (if that were possi- \nble), which has been tested thoroughly and all defects found have been fixed. Surely \nthis would be a success, right? Wrong! If the requirements were flawed, we now have a \nperfectly working wrong system. Perhaps it is hard to use, as in the previous example. \nPerhaps the requirements missed some major features that users were expecting or \nneeded 1o have, Perhaps this system is quite OK, but a competitor has come out with \nacompeting system that is easier to use, includes the expected features and is cheaper. \nOur ‘perfect’ system is not looking so good after all, even though it has effectively \n‘no defects’ in terms of ‘conformance 1o requirements’. \n1.4 TEST PROCESS \nSYLLABUS LEARNING OBJECTIVES FOR 1.4 TEST \nPROCESS (K2) \nFL-1.4.1  Explain the impact of context on the test process (K2) \nFL-1.42 Describe the test activities and respective tasks within the test \nprocess (K2) \nFL-143  Differentiate the work products that support the test process (K2) \nFL-144  Explain the value of maintaining traceability between the test \nbasis and test work products (K2) \nCopp N Compoge Lowming. A3 g Bt vt My et b o, s, o4 b, 4 s 18 . o, (b 3 st e, ot sty st \nSection 4 Test Process 15 \nP e —— e e dmd et sy oo st s 8 cvent barwny Conpge Lewmay merses e 1y 1wy o o sy e £ - .",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 28,
            "page_label": "29"
        }
    },
    {
        "page_content": "16 Chapter 1 Fundamentals of testing \nIn this section, we will describe the test process: tasks, activities and work products, \nWe will talk about the influence of context on the test process and the importance \nof traceability. \nIn this section, there are a large number of Glossary keywords (19 in all): coverage, \ntest analysis, test basis, test case, test completion. test condition, test control, test \ndata, test design, test execution, test execution schedule, test implementation, \ntest monitoring, test oradle. test planning. test procedure. test suite. testware \nand traceability. \nIn Section L1, we looked at the definition of testing, and identified misperceptions \nabout testing, including that testing is not just test execution. Certainly, test execution \nis the most visible testing activity. However, effective and efficient testing requires \ntest approaches that are properly planned and carried out, with tests designed and \nimplemented to cover the proper areas of the system, executed in the right sequence \nand with their results reviewed regularly. This is a process, with tasks and activities \nthat can be identified and need to be done, sometimes formally and other times very \ninformally. In this section, we will look at the test process in detail. \nThere is no ‘one size fits all’ test process, but testing does need to include com- \nmon sets of activities, or it may not achieve its objectives. An organization may have \na test strategy where the test activities are specified, including how they are imple- \nmented and when they occur within the life cycle. Another organization may have \na test strategy where test activities are not formally specified, but expertise about \ntest activities is shared among team members informally. The ‘right’ test process \nfor you is one that achieves your test objectives in the most efficient way. The best \ntest process for you would not be the best for another organization (and vice versa). \nSimply having a defined test strategy is not enough. One of our clients recently \nwas a law firm that sued a company for a serious software failure. It turned out that \nwhile the company had a written test strategy, this strategy was not aligned with \nthe testing best practices described in this book or the Syllabus. Further, upon close \nexamination of their test work products, it was clear that they had not even carried \nout the strategy properly or completely. The company ended up paying a substantial \npenalty for their lack of quality. So, you must consider whether your actual test activ- \nities and tasks are sufficient. \n1.4.1 Test process in context \nAs mentioned above, there is no one right test process that applies to everyone: \neach organization needs 1o adapt their test process depending on their context, The \nfactors that influence the particular test process include the following (this list is not \nexhaustive): \no Software development life cycle model and project methodologies being used. \nAn Agile project developing mobile apps will have quite a different test process \nto an organization producing medical devices such as pacemakers, \n® Test levels and test types being considered; for example, a large complex project \nmay have several types of integration testing, with a test process reflecting that \ncomplexity. \n® Product and project risks (the lower the risks, the less formal the process needs \nto be, and vice versa). \nG N Compoge Loswming. A3 Kt Bt vt My et b o, s, o4 B s, 0 s 10 . o, L 3 i g, s sty comtr ey b gy B e el smbs A e e v et oo st s & cven arwny Conpge Lewmay murses e 10 10 ey akoacnd o o} Vow € whacacs A (7 0TA s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 29,
            "page_label": "30"
        }
    },
    {
        "page_content": "Section 4 Test Process 17 \n® Business domain (e.g. mobile apps versus medical devices). \n® Operational constraints, including: \n= budgets and resources \n= timescales \n= complexity \n= contractual and regulatory requirements. \n® Organizational policies and practices. \n® Required internal and external standards. \nIn the following sections, we will look in detail at the following topics: \n@ Test activities and tasks — the various things that testers do. \n® Test work products ~ the things produced by and used by testers. \no Traceability between the test basis and test work products and why this is \nimportant. \nFirst, one aspect to consider is coverage. Coverage is a partial measure of the  Coverage The degree \nthoroughness of testing. When examining the test basis, which is whatever the tests 1o which specified \nare being derived from (such as a requirement, user story, design or even code), a  COverage items have \nnumber of things can be identified as coverage items (we can tell whether or pot we  Déen determined to \nhave tested them). For example, system level testing may want to ensure that every \nuser story has been tested at least once: integration level testing may want 1o ensure a test suite expressed as \nthat every communication path has been tested at least once; and, in component \ntesting, developers may want to ensure that every code module, branch or statement  Test basis The body of \nhas been tested at least once. When a test exercises the coverage item (user story,  knowledge used as the \ncommunication path or code element). then that item has been covered. Coverage is  Dasis for test analysis \nthe percentage of coverage items that were exercised in a given test run. and design. \nIt is useful to know what you want to cover with your testing right from the start; \ncoverage can act as a Key Performance Indicator (KPI) and help 1o measure the \nachievement of test objectives (if they are related to coverage). \nIn addition to the coverage items relating to specifications or code, we may also \nhave environmental aspects that we want to cover. For example, tests for mobile apps \nmay need to be tested on a number of mobile devices and configurations — these are \nalso coverage items or coverage criteria. We could also consider coverage items of \nuser personas, stakeholders or user success criteria. Measuring and reporting the \ncoverage of these aspects can also give confidence to stakeholders that failures in \noperation would be less likely. \nThere is more about coverage in Section 4.3. Test processes are described in more \ndetail in ISOMIEC/IEEE 29119-2 [2013]. \n1.4.2 Test activities and tasks \nA test process consists of the following main groups of activities: \no Test planning. \n® Test monitoring and control. \no Test analysis. \n® Test design. \nG N Compoge Loswming. A3 Kt Bt My et b o, s, o4 B, s 10 . o, U 3 i, g, s sty comtos ey b gy B e el snbs AChapiti s bl s M e s gyt oo s e Sy S B Tl W pericncs Cengage Ly BTN e (W 14 Y ol o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 30,
            "page_label": "31"
        }
    },
    {
        "page_content": "18 Chapter 1 Fundamentals of testing \no Test implementation, \no Test execution. \n@ Test completion. \nThese activities appear to be logically sequential, in the sense that tasks within \neach activity often create the preconditions or precursor work products for tasks \nin subsequent activities. However, in many cases, the activitics in the process may \noverlap or take place concurrently or Reratively, provided that these dependencies are \nfulfilled. Each group of activities consists of many individual tasks: these will vary \nfor different projects or releases. For example, in Agile development, we have small \niterations of software design, build and test that happen continuously, and planning \nTest planning The is also a very dynamic activity throughout. If there are multiple teams, some teams \nactivity of establishing muy be doing test analysis while other teams are in the middle of test implementation, \nOr updating @ fest iy, for example. \nTest plan Note that this is “a’ test process, not ‘the’ test process, We have found that most \nDocumentation of these activities, and many of the tasks within these activities, are carried out in \ndescribing the test some form or another on most successful test efforts, However, you should expect \nobjectives to be 10 have to tailor your test process, its main activities and the constituent tasks based \nachieved and the Means o, the organizational, project. process and product needs. constraints and other con- \ntextual realities. In sequential development, there will also be overlap, combination, \n:f‘:mmm them, e concurrency oreven omission of some tasks; this is why a test process is tailored for \nivities. each project. \n(Note that we have \nincluded the defintion 1@t planning \nof test plan here, even Test planning involves defining the objectives of testing and the approach for meet- \nthough it is not listed ing those objectives within project constraints and contexts. This includes deciding \nin the Syllabus as a on suitable test techniques to use, deciding what tasks need to be done, formulating \nterm that you need to a test schedule and other things. \nknow for this chapter; Metaphorically, you can think of test planning as similar to figuring out how to \notherwise the definition et from one place to another (without using your GPS ~ there is no GPS for testing). \nof testplanning 5001 For small, simple and familiar projects, finding the route merely involves taking an \nexisting map, highlighting the route and jotting down the specific directions, For \nTest monitoring Atest  large, complex or new projects, finding the route can involve a sophisticated process \nmanagement activity of creating a new map, exploring unknown territory and blazing a fresh trail. \nthat involves checking We will discuss test planning in more detail in Section 5.2. \nactivities, identifying Test monitoring and control \nany vanances from the T continue our metaphor, even with the best map and the clearest directions, getting \nplanned or expected from one place to another involves careful attention, watching the dashboard. minor \nm;‘:&mg'“ (and sometimes major) course corrections, talking with our companions about the \njourney, looking ahead for trouble, tracking progress towards the ultimate destina- \nTest control A test tion and coping with finding an alternate route if the road we wanted is blocked. \nmanagement task that So, in test monitoring, we continuously compare actual progress against the plan, \ndelbwﬂhdwdm check on the progress of test activities and report the test status and any necessary \nand applying a set of deviations from the plan. In test control, we take whatever actions are necessary to \ncorrective actions 10 9t ey e mission and objectives of the project, and/or adjust the plan. \nTest monitoring is the ongoing comparison of actual progress against the test plan, \nmm‘m using any test monitoring metrics that we have defined in the test plan. Test progress. \nwas planned. against the plan is reported to stakeholders in test progress reports or stakeholder \nmeetings. One option that is often overlooked is that if things are going very wrong, \nG N Crmpugs Lowming. A3 Kighs Bt vt My et b o, s, o4 e, 0 s 18 . o, U 3 i e, s sty commo ey b gyt B e el smbs At s Je—p—— e e b ety P Conpuge Linmay murses e 1y 10wy kbmcnd o o a1 wow € whacacs YA (7T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 31,
            "page_label": "32"
        }
    },
    {
        "page_content": "Section 4 Test Process 19 \nit may be time to stop the testing or even stop the project completely. In our driving \nanalogy. once you find out that you are headed in completely the wrong direction, \nthe best option is 10 stop and re-evaluate, not continue driving to the wrong place. \nOne way we can monitor test progress is by using exit criteria, also known as \n“definition of done' in Agile development. For example. the exit eriteria for test exe- \ncution might include: \n® Checking test results and logs against specified coverage criteria (we have not \nfinished testing until we have tested what we planned 1o test). \n® Assessing the level of component or system quality based on test results and \nlogs (e.g. the number of defects found or ease of use). \n® Assessing product risk and determining if more tests are needed to reduce \nthe risk to an acceptable level, \n‘We will discuss test planning, monitoring and control tasks in more detail in \nChapter 5. \nTest analysis \nIn test analysis, we analyze the test basis to identify testable features and define  Test analysis The \nassociated test conditions. Test analysis determines ‘what to test’, including meas-  activity that identifies \nurable coverage criteria. We can say colloquially that the test basis is everything  1estcondtions by \nupon which we base our tests. The test basis can include requirements, user stories, ~ analyaing the test basis, \ndesign specifications, risk analysis reports, the system design and architecture, inter-  Tagt condition (charter) \nface specifications and user expectations. An aspect of the test \nIn test analysis, we transform the more general testing objectives defined in the  basis that is relevant in \ntest plan into tangible test conditions. The way in which these are specifically docu-  order to achieve spedific \nmented depends on the needs of the testers, the expectations of the project team, any ~ test objectives. See also: \napplicable regulations and other considerations. exploratory testing. \nTest analysis includes the following major activities and tasks: \n® Analyze the test basis appropriate 1o the test level being considered. Examples \nof a test basis include: \n= Requirement specifications, for example, business requirements, functional \nrequirements, system requirements, user stories, epics, use cases or similar \nwork products that specify desired functional and non-functional component \nor system behaviour. These specifications say what the component or sys- \ntem should do and are the source of tests to assess functionality as well as \nnon-functional aspects such as performance or usability, \n= Design and implementation information, such as system or software architecture \ndiagrams or documents, design specifications, call flows, modelling diagrams \n(for example, UML or entity-relationship diagrams), interface specifications or \nsimilar work products that specify component or system structure. Structures for \nimplemented systems or components can be a useful source of coverage criteria \nto ensure that sufficient testing has been done on those structures. \n= The implementation of the component or system itself, including code, data- \nbase metadata and queries, and interfaces. Use all information about any \naspect of the system to help identify what should be tested, \n= Risk analysis reports, which may consider functional, non-functional and \nstructural aspects of the component or system. Testing should be more thor- \nough in the areas of highest risk, so more test conditions should be identified \nin the highest-risk areas. \nom0 300 Compg Lo 8 s Rt Ay b i e et o .t dms s e Gy s o e ey o e ot b et e e b et P Conpuge Linmay murses e 1y 10wy kbmcnd o o a1 wow € whacacs YA (7T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 32,
            "page_label": "33"
        }
    },
    {
        "page_content": "20 Chapter 1 Fundamentals of testing \no Evaluate the test basis and test items to identify various types of defects that \nmight occur (typically done by reviews), such as: \n= ambiguitics \n= omissions \n~ inconsistencies \n= inaccuracies \n= contradictions \n= superfluous statements, \no Identify features and sets of features o be tested. \no ldentify and prioritize test conditions for each feature, based on analysis of the \ntest basis, and considering functional, non-functional and structural characteris- \ntics, other business and technical factors, and levels of risks. \no Capture bi-directional traceability between each element of the test basis \nand the associated test conditions, This traceability should be bi-directional \n(we can trace in both forward and backward directions) so that we can check \nwhich test basis clements go with which test conditions (and vice versa) and \ndetermine the degree of coverage of the test basis by the test conditions. See \nSections 1.4.3 and 1.4.4 for more on traceability. Traceability is also very \nimportant for maintenance testing, as we will discuss in Chapter 2, Section 2.4, \nHow are the test conditions actually identified from a test basis? The test tech- \nniques, which are described in Chapter 4, are used to identify test conditions. Black- \nbox techniques identify functional and non-functional test conditions, white-box \ntechniques identify structural test conditions and experience-based technigues can \nidentify other important test conditions. Using techniques helps to reduce the likeli- \nhood of missing important conditions and helps to define more precise and accurate \ntest conditions. \nSometimes the test conditions identified can be used as test objectives for a \ntest charter. In exploratory testing, an experience-based technique (see Chapter 4, \nSection 4.4.2), test charters are used as goals for the testing that will be carried out \nin an exploratory way ~ that is, test design, execution and learing in parallel. When \nthese test objectives are traceable to the test basis, the coverage of those test condi- \ntions can be measured. \nOne of the most beneficial side effects of identifying what to test in test analysis is \nthat you will find defects; for example, inconsistencies in requirements, contradictory \nstatements between different documents, missing requirements (such as no “other- \nwise' for a selection of options) or descriptions that do not make sense. Rather than \nbeing a problem, this is a great opportunity 1o remove these defects before develop- \nment goes any further. This verification (and validation) of specifications is particu- \nlarly important if no other review processes for the test basis documents are in place. \nTest analysis can also help to validate whether the requirements properly \ncapture customer, user and other stakeholder needs. For example, techniques such \nas behaviour-driven development (BDD) and acceptance test-driven development \n(ATDD) both involve generating test conditions (and test cases) from user stories. \nBDD focuses on the behaviour of the system and ATDD focuses on the user view \nof the system, and both techniques involve defining acceptance criteria. Since these \nacceptance criteria are produced before coding, they also verify and validate the user \nstories and the acceptance criteria. More about this is found in the ISTQB Foundation \nLevel Agile Tester Extension qualification, \nG N Crmpogs Loswming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At s el i M e gyl oot s ot Sy 4 el g pericncs Cengage Lewing TGS e (4 14 Y ol o o e o € g 30 (E s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 33,
            "page_label": "34"
        }
    },
    {
        "page_content": "Section 4 Test Process 21 \nTest design \nTest analysis addresses ‘what to test’ and test design addresses the question *how  Test design The \nto test’; that is, what specific inputs and data are needed in order to exercise the soft-  activity of derving and \nware for a particular test condition. In test design, test conditions are elaborated (ata  Specifying test cases \nhigh level) in test cases, sets of test cases and other testware, Test analysis identifies 170m test conditions, \ngeneral ‘things’ to test, and test design makes these general things specific for the Yot case A sot \ncomponent or system that we are testing. of preconditions, \nTest design includes the following major activities: inputs, actions \n® Design and prioritize test cases and sets of test cases. W’m' ] \n® ldentify the necessary test data to support the test conditions and test cases as and postconditions, \nthey are identified and designed. developed based on \n@ Design the test environment, including set-up, and identify any required infra- Test conditions. \nstructure and tols, Test data Data created \n® Capture bi-directional traceability between the test basis, test conditions, test O Selected to satisfy the \nnd test proced Iso Section 1.4.4 execution preconditions cases and test procedures (see also on ). wa 1o \nAs with the identification of test conditions, test techniques are used to derive or  one or more test cases. \nelaborate test cases from the test conditions. These are described in Chapter 4, where \ntest analysis and test design are discussed in more detail. \nJust as in test analysis, test design can also identify defects ~ in the test basis and \nin the existing test conditions. Because test design is a deeper level of detail, some \ndefects that were not obvious when looking at test basis at a high level, may become \nclear when deciding exactly what values to assign to test cases. For example, a test \ncondition might be to check the boundary values of un input field, but when deter-  Je5t implementation \nmining the exact values, we realize that a maximum value has not been specified in \nthe test basis. Identifying defects at this point is a good thing because if they are fixed  pogioq for test \nnow, they will not cause problems later, execution based on test \nWhich of these specific tasks applies to a particular project depends on various  analysis and design. \ncontextual issues relevant to the project, and these are discussed further in Chapter 5. \nTest implementation in execution order, and \nIn test implementation, we specify test procedures (or test scripts). This involves 8Ny associated actions \ncombining the test cases in a particular order, as well as including any other infor-  that may be required \nmation needed for test execution. Test implementation also involves setting up the 1o set up the Nﬂ: \ntest environment and anything else that needs to be done to prepare for test execu- p'mdiwrsm \ntion, such as creating testware, Test design asked “how to test’, and test implementa- post execution. \ntion asks ‘do we now have everything in place to run the tests?” \nTest implementation includes the following major activities: Test suite (test case \no = suite, test set) A set \n® Develop and prioritize the test procedures and, potentially, create automated of test cases or test \ntest scripts. procedures to be \n@ Create test suites from the test procedures and automated test scripts (if any). executed in a specific \nSee Chapter 6 for test automation. test cycle. \n® Arrange the test suites within a test execution schedule in a way that results in Test execution \nefficient test execution (see Chapter 5, Section 5.2.4). schedule A schedule \nfor the execution of ® Build the test environment (possibly including test hamesses, service virtualization, \nsimulators and other infrastructure items) and verify that everything needed has cyde. \nbeen set up correctly, \nG N Crmpoge Lowming. A3 Kighs Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 3 i g, s sty comtr ey b gy B e el smbs At Je—p—— e e b ety e Conpage Lonmay murses e 1y 10 ey kbacnd o o 0} S € whacgacs YIS (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 34,
            "page_label": "35"
        }
    },
    {
        "page_content": "22 Chapter 1 Fundamentals of testing \no Prepare test data and ensure that it is properly loaded in the test environment \n(including inputs, data resident in databases and other data repositories, and \nsystem configuration data), \n® Verify and update the bi-directional traceability between the test basis, \ntest conditions, test cases, test procedures and test suites (see also \nSection 1.4.4), \nIdeally, all of these tasks are completed before test execution begins, because \notherwise precious, limited test execution time can be lost on these types of prepara- \ntory tasks. One of our clients reported losing as much as 25% of the test execution \nperiod to what they called ‘environmental shakedown’, which turned out to consist \nalmost entirely of test implementation activities that could have been completed \nbefore the software was delivered. \nNote that although we have discussed test design and test implementation as sep- \narate activities, in practice they are often combined and done together. \nNot only are test design and implementation combined, but many test activities \nmay be combined and carried out concurrently. For example, in exploratory test- \ning (see Chapter 4. Section 4.4.2), test analysis, test design, test implementation \nand test execution are done in an interactive way throughout an exploratory test \nsesswon. \nTest execution \nTest execution The In test execution, the test suites that have been assembled in test implementation are \nprocess of running a run, according to the test execution schedule. \ntest on the mp;:m Test execution includes the following major activities: \nof system under \nproducing actual ® Record the identities and versions of all of the test items (parts of the test object \nresult(s). to be tested), test objects (system or component to be tested), test tools and other \ntestware. Testware Work \nproducts produced ® Execute the tests either manually or by using an automated test execution tool, \nduring the test process according to the planned sequence. \nfor use in planning,  Compare actual results with expected results, observing where the actual and \ndesigning, executing, expected results differ. These differences may be the result of defects, but at this \n“mc::mm point we do not know, so we refer to them as anomalies. \n® Analyze the anomalies in order to establish their likely causes. Failures may \noccur due to defects in the code or they may be false-positives. (A false-positive \nis where a defect is reported when there is no defect.) A failure may also be due \nto a test defect, such as defects in specified test data, in a test document or \nthe test environment, or simply due to a mistake in the way the test was \nexecuted. \n® Report defects based on the failures observed (see Chapter 5, Section 5.6). A \nfailure due to a defect in the code means that we can write a defect report. Some \norganizations track test defects (i.e. defects in the tests themselves), while others \ndo not, \n® Log the outcome of test execution (e.g. pass, fail or blocked). This includes \nnot only the anomalies observed and the pass/fail status of the test cases, \nbut also the identitics and versions of the software under test, test tools and \ntestware. \nG N Compoge Lossming. A3 g Bt vt My et b o, s, o b, 0 s 10 . o, U 3 st e, s sty comto ey b sy B e el smbs At el i A e gyt oo s e Sy 4 G Tl g pericnce Cengg Ly BTG e (W 14 WY adibicnd o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 35,
            "page_label": "36"
        }
    },
    {
        "page_content": "Section 4 Test Process 23 \n® As necessary, repeat test activities when actions are taken 1o resolve discrepan- \ncies. For example, we might need to re-run a test that previously failed in order \n1o confirm a fix (confirmation testing). We might need 1o run an updated test, \n‘We might also need to run additional, previously executed tests to see whether \ndefects have been introduced in unchanged areas of the software or to see \nwhether a fixed defect now makes another defect apparent (regression testing). \no Verify and update the bi-directional traceability between the test basis, test \nconditions, test cases, test procedures and test results, \nAs before, which of these specific tasks applies 10 a particular project depends \non various contextual issues relevant to the project; these are discussed further in \nChapter 5. \nTest completion \nTest completion activities collect data from completed test activities to consolidate  Test completion The \nexperience, testware and any other relevant information. Test completion activities  activity that makes. \nshould occur at major project milestones, These can include when a software system 185t assets avadable \nis released, when a fest project is completed (or cancelled), when an Agile project 1O later use, leaves \niteration is finished (c.g. as part of a retrospective meeting), when a test level has :“wmml;‘\"‘ \nbeen completed or when a maintenance release has been completed. The specific il ociony cond the \nmilestones that involve test completion activities should be specified in the test plan. results of testing to \nTest completion includes the following major activities: relevant stakeholders. \n® Check whether all defect reports are closed, entering change requests or prod- \nuct backlog items for any defects that remain unresolved at the end of test \nexecution, \no Create a test summary report o be communicated to stakeholders. \no Finalize and archive the test environment, the test data, the test infrastructure \nand other testware for later reuse. \n® Hand over the testware to the maintenance teams, other project teams, and/or \nother stakeholders who could benefit from its use. \n® Analyze lessons learned from completed test activities to determine changes \nneeded for future iterations, releases and projects (i.e. perform a retrospective). \n® Use the information gathered to improve test process maturity, especially as \nan input to test planning for future projects. \nThe degree and extent to which test completion activities occur, and which specific \ntest completion activities do occur, depends on various contextual issues relevant 1o \nthe project, which are discussed further in Chapter 5. \n1.4.3 Test work products \n“Work products” is the generic name given 1o any form of documentation, informal \ncommunication or artefact that is used in testing (or indeed in development). When \ntesting is more formal, the majority of work products may be written documentation; \nwhen testing is very informal, the corresponding work product may just be a scrap \nof paper, or a note in someonc’s mobile phone. “Work product’ is a general term \ncovering any type of information needed to do the work (testing or development). \no N Compoge Lossming. A3 K Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 o, g, s sty comto ey b gy B e el smbs At el e M e gyt oot s o Sy 4 G Tl g pericncs Cengage Ly BTG e (W 14 WY adibicnd o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 36,
            "page_label": "37"
        }
    },
    {
        "page_content": "24 Chapter 1 Fundamentals of testing \nTest work products are created as part of the test process, and there is significant \nvariation in the types of work products created, in the ways they are organized and \nmanaged, and in the names used for them. The work products described in this section \nare in the ISTQB Glossary of terms. More information can be found in ISO/TEC/ \nIEEE 29119-3 [2013). \nTest work products can be captured, stored and managed in configuration man- \nagement tools, or possibly in test management tools or defect management tools. \nTest planning work products \nYou will not be surprised to find that test planning work products include test plans. \nThere may be different test plans for different test levels. The test plan typically \nincludes information about the test basis. to which all the other work products will be \nrelated via traceability information (see Section 1.4.4). Test plans also include entry \nand exit criteria (also known as definition of ready and definition of done) for the test- \ning within their scope ~ the exit criteria are used during test monitoring and control. \nBeware of what people call a ‘test plan’; we have seen this name applied to any \nkind of test document, including test case specifications and test execution schedules. \nA test plan is a planning document - it contains information about what is intended \nto happen in the future, and is similar to a project plan. It does not contain detail of \ntest conditions, test cases or other aspects of testing, \nThe test plan needs to be understandable to those who need to know the infor- \nmation contained in it. The two-page cryptic diagram that was called a “test plan’ at \none organization would not be the right sort of work product for other organizations. \nTest plans can cover a whole project, or be specific to a test level or type of testing. \nTest plans are covered in more detail in Chapter 5, Section 5.2, \nTest monitoring and control work products \nThe work products associated with test monitoring and control typically include \ndifferent types of test reports. Test progress reports are produced on an ongoing \nand/or a regular basis to keep stakeholders updated about progress, on a weekly or \nmonthly basis, for example. Test summary reports are produced at test completion \nmilestones, as a way of summarizing the testing for a particular unit of work or test \nproject. Any test report needs to include details relevant to its intended audience, the \ndate of the report and the time period covered by the report. Test reports may include \ntest execution results once those are available, in summary form (e.g number of tests \nrun, failed, passed or blocked and number of defects raised of different severity), and \nthe status compared 1o the exit criteria or definition of done. \nTest monitoring and control work products should address project management \nconcerns such as budget and schedule, task completion, resource allocation, usage and \neffort, If action needs 1o be taken on the basis of information reported in a test report, \nthose actions should be summarized in the next report to ensure that the desired effect \nof the action has been achieved. \nThese work products are further explained in Chapter 5, Section 5.3. \nTest analysis work products \nTest analysis work products include mainly test conditions, as this is the output of \nthe test analysis activity. Each test condition is ideally traceable to the test basis (and \nvice versa). In exploratory testing, a test charter misy be a test analysis work product. \nDefect reports about defects found in the test basis as a result of test analysis can also \nbe considered a work product from test analysis, \nG N Compoge Lowming. A3 K Bt vt My et b o, s, o4 A i, 0 s 18 . o, U 3 i e, s sty comto ey b gyt B e el snbs At s e e b et oo s et e Conpage Lonmay murses e 1y 10 ey kbacnd o o 0} S € whacgacs YIS (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 37,
            "page_label": "38"
        }
    },
    {
        "page_content": "Section 4 Test Process 25 \nHere is an example: The specification for an ordering system describes a sales \ndiscount feature when customers put in large orders. The test conditions might be to \ntest all the discount values for various order values, The discount values would be \ncoverage items — we want to make sure we have tested each of them at least once. The \nwork product would be the list of test conditions, that is, the discount values. \nTest conditions are further discussed in Chapter 4. \nTest design work products \nThe main work products resulting from test design are test cases and sets of test \ncases that exercise the test conditions identified in test analysis, \nSometimes the test cases at this stage are still rather vague and high-level, that is, \nwithout concrete values for inputs and expected results. For example, a test case for \nthe sales discount might be to set up four existing customers, one who orders only a \nsmall amount so does not qualify for a discount, and the other three who order enough \nto qualify for a discount at each of the three discount levels respectively. \nHaving the test cases ot a high level means that we can use the same test case \nacross multiple test cycles with different specific or concrete data. For example, one \napplication may have discounts of 2%, 5% and 10%, and another may have discounts \nof 10%, 20% and 25%. Our high-level test case adequately documents the scope of \nthe test even though the details will be different in each application. The test case is \ntraceable to and from the test condition that it is derived from. \n‘We have seen that high-level test cases can have advantages, but there are also \nsome aspects that you need to be aware of with high-level test cases. For example, it \nmay be difficult to reproduce the test exactly: different testers may use different test \ndata, so the test case is not exactly repeatable. A high-level test case is not directly \nautomatable; the tool needs exact instructions and specific data in order to exccute \nthe test. The skill and domain knowledge of the tester is also critical; a junior new- \nhire with no domain knowledge may struggle to know what they are supposed to be \ndoing. unless they are well supported by more experienced testers, These are not \ninsurmountable problems, but they do need to be considered. \nTest design work products may also include test data, the design of the test envi- \nronment and the identification of infrastructure and tools. The extent and way in \nwhich these are documented may vary significantly from project to project or from \none company to another. \nWhen deriving test cases from the test conditions, we may also find defects or \nimprovements that we could make to the test conditions, so the test conditions them- \nselves may be further refined during test design. In our sales discount example, in test \nanalysis we identified the three discounts as test conditions, but in test design, by look- \ning at the test cases, we identified the “no discount” test condition ~ a discount of 0%. \nTest cases are further discussed in Chapter 4. \nTest implementation work products \nWork products for test implementation include: \n® test procedures and the sequencing of those procedures. \n® test suites. \n® atest execution schedule. \nAt this point, because we are preparing for test execution, we need to further refine \nany high-level test cases into low-level test cases that use concrete and specific data, \nhoth for test inputs and test data, In our loyalty discount example, we would now \nG N Crmpogs Loswming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At s Je—p—— e e v et P Conpge Lewmay mures e 13 10 ey akoacd o o sy Vo € whacacs A (7O s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 38,
            "page_label": "39"
        }
    },
    {
        "page_content": "26 Chapter 1 Fundamentals of testing \nneed to specify the details of our existing customers, decide on exactly how much \neach order will come to and calculate the final amount they would pay, including the \ndiscount. So, for example. Mres Smith puts in an order for $50.01. Because her order \nis over $50, she gets a 10% discount, 5o she pays $45.01. We calculate the expected \nTest orade (oracke) A result for the test using a test oracle — the source of what the correct answer should \nsource to determine be (in this case simple arithmetic). We would also need 1o set up Mrs Smith and other \nexpected results to customers in the database as part of the preconditions of running the test, and this \nwm‘hﬂ; would be included in the test procedure. \ns;mmm ot In exploratory testing, we may be creating work products for test design and test \nimplementation while doing test execution; traceability may be more difficult in this case. \nTest implementation may also create work products that will be used by tools, for \nexample, test scripts for test execution tools, and sometimes work products are created \nby 100ls, such as a test execution schedule. Service virtualization may also create test \nimplementation work products. \nAs in test design, we may further refine test conditions (and high-level test cases) \nduring test implementation. For example, by deciding on the concrete values for our sales \ndiscount example, we realize that i test condition we omitted was to consider two differ- \nent ways of clients paying between $45.01 and $50.00 @that is, with or without a discount). \nThis may not be important to include in our tests, but it is an additional test condition. \nTest execution work products \nWork products for test execution include: \no documentation of the status of individual test cases or test procedures (e.g. ready \nto run, passed, failed, blocked, deliberately skipped, etc.) \n@ defect reports (see Chapter 5, Section 5.6) \no documentation about which test item(s), test object(s), test tools and testware \nwere involved in the testing. \nIn test execution, we want to know what happened when the tests were run. We \nwant to know about any problems encountered that may have blocked some tests \nrunning, for example if the network was down for a time. We want to know whether \nor not we were able 1o execute all of the tests that we planned 1o execute (which is not \nnecessarily all of the tests that we have designed or implemented). \nWhen test execution is finished (or is stopped), we should be able 10 report test \nresults based on traceability, so that if a set of tests that were all related to the same \nrequirement or user story failed, we will know about that. We also want to know \nwhich requirements have passed all of their planned tests, and any that have not yet \nbeen tested — that is, the tests are still waiting to be run. This is particularly important \nwhen test execution is stopped rather than completed. \nStakeholders are more likely to appreciate the implications of failing tests if they \ncan be related to user stories or requirements, rather than just being told that a certain \nnumber of tests passed or failed. This is why traceability is important. \nTest completion work products \nThe work products for test completion include the following: \n® Lest summary reports \n® action items for improvement of subsequent projects or iterations (e.g. following \nan Agile project retrospective) \n@ change requests or product backlog items \no finalized testware. \nG N Crmpogs Lowming. A3 Kighs Bt vt My et b o, s, o4 e i, 0 s 10 . o, U 3 i, g, s sty comtr ey b gyt B e el snbs At \nConpaet e e b ety oo s et P s e (e 4 e ol o o o € g 30 (F s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 39,
            "page_label": "40"
        }
    },
    {
        "page_content": "Section 5 The Psychology of Testing 27 \nTest completion work products give closure to the whole of the test process and \nshould provide ongoing ideas for increasing the effectiveness and efficiency of testing \nwithin the organization in the future, \n1.4.4 Traceability between the test basis and test \nwork products \nWe have mentioned bi-directional traceability for all test work products covered in -~ Traceability The \nSection 1.4.3. Bi-directional means that we can trace, for example, a given require-  degree to which a \nment through test conditions and test cases (o the test execution results, but we can  felationship can be \nalso trace the test execution results back 10 test cases, test cases back 10 test condi-  ©stablished between \ntions, and test conditions back to requirements. No matter what the work products \nare called, this cross-linking gives many benefits to the test process. We saw how \ntraceability can aid in the measurement and reporting of test coverage in order to \nreport coverage and defects related to requirements, which is more meaningful and \nof more value to stakeholders. \nGood traceability also supports the following: \n® analyzing the impact of changes, whether to requirements or to the component \nor system \n® muking testing auditable. and being able 10 measure coverage \no meeting IT governance criteria (where applicable) \no improving the coherence of test progress reports and test summary reports to \nstakeholders, as described above \n@ relating the technical aspects of testing to stakeholders in terms that they can \nunderstand \no providing information to assess product quality, process capability and pro- \nJject progress against business goals. \nYou may find that your test management or requirements management tool pro- \nvides support for traceability of work products; if so, make use of that feature. Some \norganizations find that they have 1o build their own management systems in order to \norganize test work products in the way that they want and 1o ensure that they have \nbi-directional traceability. However the support is implemented, it is important to \nhave automated support for traceability ~ it is not something that can be sustained \nwithout tool support. \n1.5 THE PSYCHOLOGY OF TESTING \nSYLLABUS LEARNING OBJECTIVES FOR 1.5 \nTHE PSYCH GY OF TESTING (K2) \nFL-15.1 Identify the psychological factors that influence the success of \ntesting (K1) \nFL-1.5.2  Explain the difference between the mindset required for \ntest activities and the mindset required for development \nactivities (K2) \nG N Compogs Lossming. A3 Kghs Bt vt My et b o, s, o4 B, 0 s 18 . o, U 3 s, e, s s sty comti ey b gy B e el smbs At s e e dmd ety Je——— s & cven barwny Conpuge Lonmay muries e 1y 10 ey akbacnd o o 10} o € shacgacm A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 40,
            "page_label": "41"
        }
    },
    {
        "page_content": "28 Chapter 1 Fundamentals of testing \n1.5.1 Human psychology and testing \nIn this section, we'll discuss the various psychological factors that influence testing \nand its success. We'll also contrast the mindset of a tester and a developer, For a \ngood introduction to the psychology of testing see Weinberg [2008]. \nAs mentioned earlier, the ISTQB definition of software testing includes both \ndynamic testing and static testing. Let's review the distinction again. Dynamic soft- \nware testing involves actually executing the software or some part of it, such as \nchecking an application-produced report for accuracy or checking response time to \nuser input. Static software testing does not execute the software but uses two possible \napproaches: automated static analysis on the code (e.g. evaluating its complexity) or \non a document (e.g. to evaluate the readability of a use case); or reviews of code or \ndocuments (e.g. to evaluate a requirement specification for consistency, ambiguity \nand completeness). \nFinding defects \nWhile dynamic and static testing are very different types of activities, they have \nin common their ability to find defects. Static testing finds defects directly, while \ndynamic testing finds evidence of a defect through a failure of the software to behave \nas expected. Either way, people carrying out static or dynamic tests must be focused on \nthe possibility — indeed. the high likelihood in many cases — of finding defects. Indeed, \nfinding defects is often a primary objective of static and dynamic testing activities. \nIdentifying defects may unfortunately be perceived by developers as a criticism, \nnot only of the product but also of its author — and in a sense. it is, But finding defects \nin testing should be constructive criticism, where testers have the best interest of the \ndeveloper in mind. One meaning of the word ‘criticism’ is ‘an examination, interpre- \ntation, analysis or judgement about something” - this is an objective assessment. But \nother meanings include disapproval by pointing out faults or shortcomings and even \nan attack on someone or something. Testing does not want to be the latter sense of \ncriticism, but even when intended in the first sense, it can be perceived in the other \nways. Testers need to be diplomats, along with everything else. \nBias \nHowever, there is another factor at work when we are reporting defects; the author \n(developer) believes that their code is correct ~ they obviously did not write it to be \nintentionally wrong. This confidence in their understanding is in some sense nec- \nessary for developers: they cannot proceed without it. But at the same time, this \nconfidence creates confirmation bias, Confirmation bias makes it difficult 1o accept \ninformation that disagrees with your currently held beliefs. Simply put, the author of \na work product has confidence that they have solved the requirements, design, meta- \ndata or code problem, at least in an acceptable fashion; however, strictly speaking, \nthat is false confidence. Other biases may also be at work, and it is also human nature \n10 blame the bearer of bad news (which defects are perceived to be). \nTesters are not always aware of their biases either, and they do have biases of their \nown. Since those biases are different from the developers, that is a benefit, but a lack \nof awareness of those biases sets up potential conflict. \nThis reluctance to accept that their work is not perfect is why some people regard \ntesting as a destructive activity (trying 10 destroy their work) rather than the construc- \ntive activity it is (trying to construct better quality software). Good testing contributes \ngreatly to product quality and project quality, as we saw in Sections 1.1 and 1.2 \nG N Crmpogs Lowming. A3 K Bt vt My et b o, s, o4 A, 0 s 10 . e, U 3 i, e, s sty comto ey b gy B e el smbs At s e e b et oo s et e Conpage Linmay murses e 1y 10wy akbacnd o o 10} W € hocacs YA (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 41,
            "page_label": "42"
        }
    },
    {
        "page_content": "Section 5 The Psychology of Testing 29 \nWhile some developers are aware of their biases when they participate in reviews \nand perform unit testing of their own work products, those biases act to impede their \neffectiveness at finding their own defects. The mental mistakes that caused them to \ncreate the defects remain in their minds in most cases. When proofreading our own \nwork, for example, we see what we meant, not what we wrote. \nReview and test your own work? \nShould software work product developers ~ business analysts, system designers, \narchitects, database administrators and developers — review and test their own \nwork? They certainly should: they have a decp understanding about the system, and \nquality is everyone's responsibility. \nHowever. many business analysts, system designers, architects, database admin- \nistrators and developers do not know the review, static analysis and dynamic testing \ntechniques discussed in the Foundation Syllabus and this book. While that situation \nis gradually changing. much of the self-testing by software work product developers \nis either not done or is not done as effectively as it could be. The principles and tech- \nniques in the Foundation Syllabus and this book are intended to help cither testers \nor others 1o be more effective at finding defects, both their own and those of others. \nAttitudes \nIt is a particular problem when a tester revels in being the bearer of bad news, For \nexample, one tester made a revealing — and not very flattering — remark during \nan interview with one of the authors. When asked what he liked about testing, he \nresponded, ‘I like to catch the developers’. He went on to explain that, when he found \na defect in someone’s work, he would go and demonstrate the failure on the pro- \ngrammer's workstation. He said that he made sure that he found at least one defect in \neveryone's work on a project, and went through this process of ritually humiliating \nthe programmer with each and every one of his colleagues. When asked why, he \nsaid, ‘I want to prove to everyone that I am their intellectual equal™. This person, \nwhile possessing many of the skills and traits one would want in a tester, had exactly \nthe wrong personality to be a truly professional tester. \nInstead of seeing themselves as their colleagues” adversaries or social inferiors out \n1o prove their equality, testers should see themselves as teammates, In their special \nrole, testers provide essential services in the development organization. They should \nask themselves, *Who are the stakeholders in the work that 1 do as a tester?’ Having \nidentified these stakeholders, they should ask each stakeholder group, “What services \ndo you want from testing, and how well are we providing them” \nWhile the specific services are not always defined, it is common that mature and \nwise developers know that studying their mistakes and the defects they have intro- \nduced is the key to learning how to get better. Further, smart software development \nmanagers understand that finding and fixing defects during testing not only reduces \nthe level of risk to the quality of the product, it also saves time and money when \ncompared to finding defects in production, \nCommunication \nClearly defined objectives and goals for testing, combined with constructive styles \nof communication on the part of test professionals, will help to avoid most negative \npersonal or group dynamics between testers and their colleagues in the development \nteam. Whenever defects are found, true testing professionals distinguish themselves \nby demonstrating good interpersonal skills. True testing professionals communicate \nG N Crmpogs Lowming. A3 K Bt vt My et b o, s, o4 A, 0 s 10 . e, U 3 i, e, s sty comto ey b gy B e el smbs At s e e b et oo s et e Congage Lisming mares e (i 1 vy adibmcnd o o sy o € shocgaes (430 (0 Sms s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 42,
            "page_label": "43"
        }
    },
    {
        "page_content": "30 Chapter 1 Fundamentals of testing \nfacts about defects, progress and risks in an objective and constructive way that \ncounteracts these misperceptions as much as possible. This helps to reduce tensions \nand build positive relationships with colleagues, supporting the view of testing as a \nconstructive and helpful activity, While this is not necessary, we have noticed that \nmany consummate testing professionals have business analysts, system designers, \narchitects, developers and other specialists with whom they work as close personal \nfriends. \nThis applies not only 1o testers but also 1o test managers, and not just to defects \nand failures but to all communication about testing, such as test results, test progress. \nand risks, \nHaving good communication skills is a complex topic, well beyond the scope of a \nbook on fundamental testing techniques. However, we can give you some basics for \ngood communication with your development colleagues: \n® Remember to think of your colleagues as teammates, not as opponents or adver- \nsaries. The way you regard people has a profound effect on the way you treat \nthem. You do not have o think in terms of Kinship or achieving world peace, \nbut you should keep in mind that everyone on the development team has the \ncommon goal of delivering a quality system, and everyone must work together \nto accomplish that. Start with collaboration, not battles. \n® Make sure that you focus on and emphasize the value and benefits of testing. \nRemind your developer colleagues that defect information provided by testing \ncan help them to improve their own skills and future work products. Remind \nmanagers that defects found carly by testing and fixed as soon as possible will \nsave time and money and reduce overall product quality risk. Also, be sure to \nrespond well when developers find problems in your own test work products. \nAsk them to review them and thank them for their findings (just as you would \nlike to be thanked for finding problems in their work). \n® Recognize that your colleagues have pride in their work, just as you do, and as \nsuch you owe them a tactful communication about defects you have found. It is \nnot really any harder to communicate your findings, especially the potentially \nembarrassing findings, in a neutral, fact-focused way. In fact, you will find that \nif you avoid criticizing people and their work products, but instead keep your \nwritten and verbal communications objective and factual, you will also avoid a \nlot of unnecessary conflict and drama with your colleagues. \ne Before you communicate these potentially embarrassing findings, mentally put \nyourself in the position of the person who created the work product. How are \nthey going to feel about this information? How might they react? What can you \ndo 10 help them get the essential message that they need 1o receive without pro- \nvoking a negative emotional reaction from them? \n@ Keep in mind the psychological element of cognitive dissonance. Cogni- \ntive dissonance is a defect ~ or perhaps a feature ~ in the human brain that \nmakes it difficult to process unexpected information, especially bad news. \nSo. while you might have been clear in what you said or wrote, the person on \nthe receiving end might not have clearly understood. Cognitive dissonance is \na two-way street, 100, and it is quite possible that you are misunderstanding \nsomeone’s reaction 10 your findings. So, before assuming the worst about \nsomeone and their motivations, confirm that the other person has understood \nwhat you have said and vice versa. \nG N Crmpogs Loswming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At s P v e e s oo st P Conpge Lewmay mures e 13 10 ey akoacd o o sy Vo € whacacs A (7O s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 43,
            "page_label": "44"
        }
    },
    {
        "page_content": "Section 5 The Psychology of Testing 31 \n1.5.2 Tester's and developer's mindsets \nTesters and developers actually have different thought processes and different objec- \ntives for their work. A mindset reflects an individual’s assumptions and the way that \nthey like to solve problems and make decisions. \nA tester’s mindset should include the following: \no Curiosity. Good testers are curious about why systems behave the way they do \nand how systems are built. When they see unexpected behaviour, they have a \nnatural urge to explore further, to isolate the failure, to look for more general- \nized problems and to gain deeper understanding. \n® Professional pessimism. Good testers expect to find defects and failures. They \nunderstand human fallibility and its implications for software development. \n(However, this is not to say that they are negative or adversarial, as we'll discuss \nin a moment.) \n® A critical eye, Good testers couple this professional pessimism with a natural \ninclination to doubt the correctness of software work products and their behav- \niours as they look at them. A good tester has, as a personal slogan, *If in doubt, \nitis a bug”. \n@ Attention to detail. Good testers notice everything, even the smallest details. \nSometimes these details are cosmetic problems like font-size mismatches, but \nsometimes these details are subtle clues that a serious failure is about to happen. \nThis trait is both a blessing and a curse. Some testers find that they cannot turn \nthis trait off, so they are constantly finding defects in the real world — even when \nnot being paid to find them. \n® Experience. Good testers not only know a defect when they see one, they also \nknow where to Jook for defects. Experienced testers have seen a veritable parade \nof bugs in their time, and they leverage this experience during all types of test- \ning, especially experience-based testing such as error guessing (see Chapter 4), \n® Good communication skills. All of these traits are essential, but, without the \nability to effectively communicate their findings, testers will produce useful \ninformation that will, alas, be put to no use. Good communicators know how \nto explain the test results, even negative results such as serious defects and \nquality risks, without coming across as preachy, scolding or defeatist. \nA good tester has the skills, the training, the certification and the mindset of \nprofessional tester, and of these four skills, the most important — and perhaps the \nmost elusive ~ is the mindset, \nThe tester’s mindset is to think about what could go wrong and what is missing. \nThe tester looks at a statement in a requirement or user story and asks, “What if it \nisn't? What haven't they thought of here? What could go wrong?” That mindset is \nquite different from the mindset that a business analyst, system designer, architect, \ndatabase administrator or developer must bring 1o creating the work products involved \nin developing software. While the testers (or reviewers) must assume that the work \nproduct under review or test is defective in some way ~and it is their job 1o find those \ndefects — the people developing that work product must have confidence that they \nunderstand how to do so properly. Looking at a statement in a requirement or user \nstory, the developer thinks, ‘How can I implement this? What technical challenges \ndo | need to solve?” \nv Aoy L, 4 s Srire My v e e o7 A1 ek o D ey s Bk s v s ppmed b i e gl e e b et e Conpage Linmay murves e 1y 10 ey kbacnd o o 01 S € whacacs YA (74T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 44,
            "page_label": "45"
        }
    },
    {
        "page_content": "32 Chapter 1 Fundamentals of testing \nHaving a tester or group of testers who are organizationally separate from devel- \nopment, either as individuals or as an independent test team, can provide significant \nbenefits, such as increased defect-detection percentage. A tester’s mindset is a ‘dif- \nferent pair of eyes” and independent testers can see things that developers do not see \n(because of confirmation bias discussed above). This is especially important for large, \ncomplex or safety-critical systems. \nHowever, independence from the developers does not mean an adversarial relation- \nship with them. In fact, such a relationship is toxic, often fatally so, to a test team’s \neffectiveness. \nThe softer side of software testing is often the harder side to master. A tester may \nhave adequate or even excellent technique skills and certifications, but if they do not \nhave adequate interpersonal and communication skills, they will not be an effective \ntester. Such soft skills can be improved with training and practice. The best testers \ncontinuously strive to attain a more professional mindset. and it is a lifelong journey. \no N4 Compogs Loy N3 B Bt vl Moy et b o, . o4 B 0 s 18 . ot L 3 i, e, ot sty ontsss ey b vt b e ol snbis A s bl i M e s gyt oo s e Sy S B Tl g pericncs Cengage L BTG e (W 14 Y adiBicnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 45,
            "page_label": "46"
        }
    },
    {
        "page_content": "Chapter Rewew 33 \nCHAPTER REVIEW \nLet's review what you have learned in this chapter. \nFrom Section 1.1, you should now know what testing is. You should be able 1o \nremember the typical objectives of testing. You should know the difference between \ntesting and debugging. You should know the Glossary keyword terms debugging, \ntest object, test objective. testing, validation and verification. \nFrom Section 1.2, you should now be able to explain why testing is necessary and \nsupport that explanation with examples. You should be able to explain the difference \nbetween testing and quality assurance and how they work together to improve quality. \nYou should be able to distinguish between an error (made by a person), a defect (in \na work product) and a failure (where the component or system does not perform as \nexpected). You should know the difference between the root cause of a defect and \nthe effects of a defect or failure, You should know the Glossary terms defect, error, \nfailure, quality, quality assurance and root cause. \nYou should be able to explain the seven principles of testing, discussed in \nSection 1.3. \nFrom Section 1.4, you should now recognize a test process, You should be able \nto recall the main testing activities of test planning, test monitoring and control, \ntest analysis, test design, test implementation, test execution and test completion. \nYou shoukd be familiar with the work products produced by cach test activity. You \nshould know the Glossary terms coverage, test analysis, test basis, test case. test \ncompletion, test condition, test control. test data, test design, test execution, test \nexecution schedule, test implementation. test monitoring, test oracle, test plan- \nning, test procedure. test suite, testware and traceability. \nFrom Section 1.5, you now should be able 10 explain the psychological factors \nthat influence the success of testing. You should be able to explain and contrast the \nmindsets of testers and developers, and why these differences can lead 10 problems. \ne 328Gy Lo 4 g St U ot . . et 0 10 et B vt i e ot s AP e et e et [Py pa pe——— o Lamang et e gl b s bbb et o 5 b § bt 4l b gt &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 46,
            "page_label": "47"
        }
    },
    {
        "page_content": "34 Chapter | Fundamentals of testing \nSAMPLE EXAM QUESTIONS \nQuestion 1 What is NOT a reason for testing? \na. To enable developers 1o code as quickly as \npossible. \nb. To reduce the risk of failures in operation, \n¢ To contribute to the quality of the components or \nsystems, \nd. To meet any applicable contractual or legal \nrequirements. \nQuestion 2 Consider the following definitions and \nmatch the term with the definition. \n1. A reason or purpose for designing and exceuting \natest, \n2. The component or system 1o be tested. \n3. Confirmation by examination and through \nprovision of objective evidence that the \nrequirements for a specific intended use or \napplication have been fulfilled. \na. 1) test object, 2) test objective, 3) validation. \nb. 1) test objective, 2) test object, 3) validation. \ne 1) validation, 2) test basis, 3) verification. \nd 1) test objective, 2) test object, 3) verification. \nQuestion 3 Which statement about quality \nassurance (QA) is true? \na. QA and testing are the same. \nb. QA includes both testing and root cause analysis. \n. Testing is quality control, not QA. \n. QA does not apply to testing. \nQuestion 4 1t is important to ensure that test design \nstarts during the requirements definition. Which of \nthe following test objectives supports this? \na. Preventing defects in the system. \nb. Finding defects through dynamic testing. \n¢ Gaining confidence in the system. \n. Finishing the project on time. \nQuestion 5 A test team consistently finds a lange \nnumber of defects during development, including \nsystem testing. Although the test manager understands \nthat this is good defect finding within their badget \nfor ber test team and industry, senior management \nand executives remain disappointed in the test group, \nsaying that the test team misses some bugs that the \nusers find after release. Given that the users are \ngenerally happy with the system and that the failures \nthat have occurred have generally been low-impact. \nwhich of the following testing principles is most likely \n10 help the test manager explain to these managens and \nexecutives why some defects are likely to be missed? \na. Exhaustive testing is impossible. \nb. Defect clustering. \n¢. Pesticide paradox. \nd. Absence-of-errors fallacy. \nQuestion 6 What are the benefits of traceability \nbetween the test basis and test work products? \na. Traceability means that test basis documents and \ntest work products do not need to be reviewed. \nb. Traceability ensures that test work products are \nlimited in number to save time in producing them. \n¢, Traceability enables test progress and defects to \nbe reported with reference 1o requirements, which \nis more understandable to stakeholders. \nd. Traceability enables developers to produce code \nthat is easier (0 test. \nQuestion 7 Which of the following is most \nimportant 1o promote and maintain good \nrelationships between testers and developers? \na. Understanding what managers value about testing, \nb. Explaining test results in a neutral fashion. \n. Identifying potential customer work-arounds \nfor bugs. \nd. Promoting better quality software whenever \npossible. \nVa0 g v, AR o M st ¥ i Ayt e 7 g o i i o i o S P e A [Pay pa Iy r— g Lramtng et e gl b s b et o 7 b § bt o4l b g &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 47,
            "page_label": "48"
        }
    },
    {
        "page_content": "Sample Exam Questons 35 \nQuestion 8 Given the following test work products,  a. 1)~ Test planning, 2) - Test design 3) - Test \nidentify the major activity in a test process that execution, 4) - Test implementation. \nproduces it. b. 1) = Test execution, 2) - Test analysis 3) ~ Test \n1. Test execution schedule. completion, 4) — Test execution, \n2 Test X o l)-'lftil‘ control, 2)—Tm lmlysis‘, 3) - Test \nmonitoring, 4) - Test implementation, \n3. Test progress reports. d. 1) Test implementation, 2) - Test design, \n4. Defect reports, 3) — Test monitoring, 4) — Test execution. \no S0 Compoge Loy N3 Rt Bt vl My et b o, s o4 B i, 0 s 18 ot (o 3 i, e, ot e paty ontoss ey b spqpened b e ol b g s el s M e syt oot s e Sy S el g pericncs Cengage L BTG e (W 14 W ol o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 48,
            "page_label": "49"
        }
    },
    {
        "page_content": "CHAPTER TWO \nTesting throughout the software \ndevelopment life cycle \nesting is not a stand-alone activity. It has its place within a software development \nlife cycle model and therefore the life cycle applied will largely determine how \ntesting is organized. There are many different forms of testing, Because several \ndisciplines, often with different interests, are involved in the development life cycle. \nitis important to clearly understand and define the various test levels and types. This \nchapter discusses the most commonly applied software development models, test \nlevels and test types. Maintenance can be seen as a specific instance of a development \nprocess. The way maintenance influences the test process, levels and types and how \ntesting can be organized is described in the last section of this chapter. \n2.1 SOFTWAREDEVELOPMENT LIFE CYCLE MODELS \nSYLLABUS LEARNING OBJECTIVES FOR \nDEVELOPMENT LIFE CYCLE MODELS (K2) \nSOFTWARE \nFL-2.1.1 Explain the relationships between software development \nactivities and test activities in the software development \nlife cycle (K2) \nFL-2.12 Identify reasons why software development life cycle models \nmust be adapted to the context of project and product \ncharacteristics (K1) \nIn this section, we'll discuss software development models and how testing fits into \nthem, We'll discuss sequential models, focusing on the V-model approach rather \nthan the waterfall. We'll discuss iterative and incremental models such as Rational \nUnified Process (RUP), Scrum, Kanban and Spiral (or prototyping). \nAs we go through this section, watch for the Syllabus terms commercial off-the- \nshelfl (COTS), sequential development model. and test level. You will find these \nkeywords defined in the Glossary (and ISTQB website). \nThe development process adopted for a project will depend on the project aims and \ngoals, There are numerous development life cycles that have been developed in order \nto achieve different required objectives, These life cycles range from lightweight and \nfast methodologies, where time to market is of the essence, through to fully controlled \nand documented methodologies. Each of these methodologies has its place in mod- \nern software development. and the most appropriate development process should be \noy N Crmpogs Loswming. NS Rt Roerrnd Moy o b copi, wamnd, o0 et o s 10 . e, U 3 vt g, acoms sty comms sy b sppvwsad B e ol snbhe gt Ebrorsl e A devmed 0 @1 sappresacd o st puarmaly afict S eenil Barmag ey Conpge Liwimag BuTSes (40 14 e akioned o 101 o € b YA (T s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 49,
            "page_label": "50"
        }
    },
    {
        "page_content": "Section 1 Software Development Life Cycle Models 37 \napplied o each project. The models specify the various stages of the process and the \norder in which they are carried out. \nThe life cycle model that is adopted for a project will have a big impact on the \ntesting that is carried out. Testing does not exist in isolation; test activities are highly \nrelated to software development activities, It will define the what, where and when \nof our planned testing, influence regression testing and largely determine which test \ntechniques to use. The way testing is organized must fit the development life cycle \nor it will fail to deliver its benefit, If time 10 market is the key driver, then the testing \nmust be fast and efficient. If a fully documented software development life cycle, with \nan audit trail of evidence, is required, the testing must be fully documented. \n‘Whichever life cycle model is being used, there are several characteristics of good \ntesting: \n® For every development activity there is a corresponding test activity. \n® Each test level has test objectives specific 1o that level, \n® The analysis and design of tests for a given test level should begin during the \ncorresponding software development activity. \no Testers should participate in discussions to help define and refine requirements \nand design. They should also be involved in reviewing work products as soon as \ndrafts are available in the software development cycle. \nRecall from Chapter |, testing Principle 3: ‘Early testing saves time and money”. \nBy starting testing activities as carly as possible in the software development life \ncyche, we find defects while they are still small green shoots (in requirements, for \nexample) before they have had a chance to grow into trees (in production). We also \nprevent defects from occurring at all by being more aware of what should be tested \nfrom the earliest point in whatever software development life cycle we are using. \n‘We will look at two categories of software development life cycle models: sequen- \ntial and termtive/incremental, \nSequential development models \nA sequential development model is one where the development activities happen  Sequential \nin a prescribed sequence — at least that is the idea. The models assume a lincar  development \nsequential flow of activities; the next phase is only supposed to start when the pre-  model A type of \nvious phase is complete. Berard [1993] said that developing software from require-  development ife cycle \nments is like walking on water — it is casier if it is frozen. But practice does not M&Ml \nconform to theory: the activities will overlap, and things will be discovered in a mwm:' \nlater phase that may invalidate assumptions made in previous phases (which were of » way of several discrete supposed 10 be finished). and successive phases \nThe waterfall model (in Figure 2.1) was one of the earliest models to be designed. it no overlap \nIt has a natural timeline where tasks are executed in a sequential fashion. We start — between them. \nat the top of the waterfall with a feasibility study and flow down through the various \nproject tasks, finishing with implementation into the live environment. Design flows \nthrough into development, which in turn flows into build, and finally on into test. \nDifferent models have different levels; the figure shows one possible model. With all \nwaterfall models, however, testing tends to happen towards the end of the life cycle, \ns0 defects are detected close to the live implementation date. With this model, it is \ndifficult to get feedback passed backwards up the waterfall and there are difficultics \nif we need to carry out numerous iterations for a particular phase. \nGt N Crmpuge Lowming. A3 K Bt vt My et b o, s, o4 e, 0 s 18 . o, U 3 i e, s sty conto ey b gy B e el snbs At s Je—p—— e e b et P Conpuge Linmay murses e 1y 10 ey koo o o 10} o € whacacm IS (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 50,
            "page_label": "51"
        }
    },
    {
        "page_content": "38 Chapter 2 Testing throughout the software development Ife cycle \n=3 \nGlobal \ndesign \nImpiementation \nEE \nThe V-model was developed to address some of the problems experienced using \nthe traditional waterfall approach. Defects were being found too late in the life cycle, \nas testing was not involved until the end of the project. Testing also added lead \ntime due to its late involvement. The V-model provides guidance on how testing \nbegins as early as possible in the life cycle. It also shows that testing is not only an \nexccution-based activity. There are a variety of activities that need to be performed \nbefore the end of the coding phase. These activities should be carried out in parallel \nwith development activities. and testers need to work with developers and business \nanalysts so they can perform these activities and tasks, producing a set of test deliv- \nerables. The work products produced by the developers and business analysts during \ndevelopment arc the basis of testing in one or more levels. By starting test design \nearly, defects are often found in the test basis documents. A good practice is 10 have \ntesters involved even carlier, during the review of the (draft) test basis documents. The \nV-model is a model that illustrates how testing activities (verification and validation) \ncan be integrated into each phase of the life cycle. Within the V-model, validation \ntesting takes place especially during the carly stages, for example reviewing the user \nrequirements, and late in the life cycle, for example during user acceptance testing. \nAlthough variants of the V-model exist. a common type of V-model uses four \nTest level (test stage) test levels. (See Section 2.2 for more on test levels.) The four test levels used, each \nA specific instantiation with their own objectives, are: \no Component testing: searches for defects in and verifies the functioning of soft- \nware components (for example modules, programs, objects, classes, etc.) that are \nseparately testable. \n@ Integration testing: tests interfaces between components, interactions to differ- \nent parts of a system such as an operating system, file system and hardware or \ninterfaces between systems. \nFIGURE 2.1 Waterfall model \not S04 Crmpoge Loty N3 Bt Bt vl My et b oo s o4 bl 0 b 18 ot U 3 o, . ot ety st ey b smpppesmed B e ol snbis gt s el s M e gyt oot s et sy e el g pericnce CEngage Ly BTG e (W 14 Y adlbicnd o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 51,
            "page_label": "52"
        }
    },
    {
        "page_content": "Section 1 Software Development Life Cycle Models 39 \n® System testing: concerned with the behaviour of the whole system/product as \ndefined by the scope of a development project or product. The main focus of \nsystem testing is verification against specified requirements. \n® Acceptance testing: validation testing with respect to user needs, requirements \nand business processes conducted to determine whether or not to accept the \nsystem. \nIn practice, a V-model may have more, fewer or different levels of development \nand testing, depending on the project and the software product. For example, there \nmay be component integration testing after component testing and system integration \ntesting after system testing. Test levels can be combined or reorganized depending on \nthe nature of the project or the system architecture. In the V-model, there may also \nbe overlapping of activities. \nSequential models aim to deliver all of the software at once, that is, the com- \nplete set of features required by stakeholders or users, or the software may be \ndelivered in releases containing significant chunks of new functionality. However, \ntypically this may take months or even years of development even for a single \nrelease. \nNote that the types of work products mentioned in Figure 2.2 on the left side of the \nV-model are just an illustration. In practice, they come under many different names. \nFIGURE 2.2 Vmodel \nIterative and incremental development models \nNot all life cycles are sequential. There are also iterative and incremental life \ncycles where, instead of one large development timeline from beginning to end, \nwe cycle through a number of smaller self-contained life cycle phases for the same \nproject. As with the V-model. there are many variants of iterative and incremental \nlife cycles, \not S04 Crmpoge Loy N3 B Bt vl My et b oo s o4 Bl 0 b 18 ot U 3 ot . ot ety st ey b smpqpesmed B e ol sndis gt s el i M v gyt oo s et Sy R G Tl g perience Cengage Ly BTG e (W 14 Y adibicnd o o o € g 4 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 52,
            "page_label": "53"
        }
    },
    {
        "page_content": "40 Chapter 2 Testing throughout the software development Ife cycle \nTo better understand the meaning of these two terms, consider the following two \nsequences of producing a painting. \n® Incremental: complete one piece at a time (scheduling or staging strategy). \nEach increment may be delivered to the customer. \nImages provided by: Mark Fewster, Grove Software Testing Ltd \no [terative: start with a rough product and refine it, iteratively (rework strategy). \nFinal version only delivered to the customer (although in practice, intermediate \nversions may be delivered to selected customers 1o get feedback). \nImages provided by: Mark Fewster, Grove Software Testing Ltd \nIn terms of developing software, a purely iterative model does not produce a \nworking system until the final iteration. The incremental approach produces work- \ning versions of parts of the system carly on and each of these can be released 1o the \ncustomer. The advantage of this is that the customer can gain carly benefit from using \nthe deliveries and, perhaps most importantly, the customers can give valuable feed- \nback. This feedback will influence what is done in future increments. Most iterative \napproaches also incorporate this feedback loop by delivering some (if not all) of the \n(intermediate) products created by the iterations. \nThe painting analogy shown above is not a perfect representation for the iterative \napproach, If the final product were to comprise 1,000 source code modules, you could \nbe forgiven for thinking that an iterative approach would have people starting the \nfirst iteration by writing one line of code in each module and then have the second \nand subsequent iterations each adding another line of code to each module until they \nwere completed. This is not the case. \nIn both iterative and incremental models, the features to be implemented are \ngrouped together (for example according to business priority or risk). In this way, \nthe focus is always on the most important of the outstanding features. The various \nproject phases, including their work products and activities, then occur for each group \nof features. The phases may be done either sequentially or overlapping. and the iter- \nations or increments themselves may be sequential or overlapping. \nAn iterative development model is shown in Figure 2.3. \noot N Crmpoge Lonwming A3 Bt Bt vt My et b o, o bl . 0 b 18 o U 3 o, g, s sty combta ey b syt B e el b et v s deemed sy Je—p—— s & cvent barwny Conpage Linmay murses e 1y 10 ey sk o o a1 Vo € whargacs S 17073 ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 53,
            "page_label": "54"
        }
    },
    {
        "page_content": "Section 1 Software Development Life Cycle Models \nPhase 1 Phase 2 \nHHE \nDefine \nDevelop \nBuild \nImplement \nImplement \n\\ Live Implementation / \nFIGURE 2.3 tteratwe development model \nTesting in incremental and iterative development \nDuring project initiation, high-level test planning and test analysis occurs in paral- \nlel with the project planning and business/requirements analysis. Any detailed test \nplanning, test analysis, test design and test implementation occurs at the beginning \nof each iteration, \nTest execution often involves overlapping test levels. Each test level begins as \ncarly as possible and may continue after subsequent, higher test levels have started. \nIn an iterative or incremental life cycle, many of the same tasks will be performed \nbut their timing and extent may vary. For example, rather than being able to imple- \nment the entire test environment at the beginning of the project, it may be more \nefficient to implement only the pant needed for the current iteration. The testing tasks \nmay be undertaken in a different order and not necessarily sequentially, There are \nlikely to be fewer entry and exit criteria between activities compared with sequential \nmodels. Also, much of the test planning and completion reporting are more likely \nto occur at the start and end of the project, respectively, rather than at the start and \nend of each iteration. \nWith any of the iterative or incremental life cycle models, the farther ahead the \nplanning occurs, the farther ahead the scope of the test process can extend. \nCommon issues with ferative and incremental models include: \n® More regression testing. \n® Defects outside the scope of the iteration or increment. \n® Less thorough testing. \nBecause the system is being produced a bit at a time, at any given point there will \nsome part which is completed in some sense, either an increment or the work that was \ndone iteratively. This part will be tested and may be used by the customer or user o give \nfeedback. When the nextincrement or iteration is developed. this will also be tested, but it \nis also important to do regression testing of the parts which have already been developed. \nThe more iterations or increments there are, the more regression testing will be needed \nthroughout development. (This type of testing is a good candidate for sutomation.) \nDefects that are found in the part that you are currently testing are dealt with in \nthe usual way, but what about defects found either by regression testing of previously \ndeveloped parts, or discovered by accident when testing a new part? These defects do \nneed to be logged and dealt with, but because they are outside the scope of the current \niterationfincrement, they can sometimes fall between the cracks and be forgotten, \nneglected or argued about. \nG N4 Crmpoge Lonwming A3 K Bt vt My et b o, s, o4 B . 0 s 18 . o, L 3 s, e, ot sty scnbons ey b gy B e el smbis At s \nP e e b s sy oo s et s 8 cven earwny Conpge! s e g b s P A— \n41 \n.",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 54,
            "page_label": "55"
        }
    },
    {
        "page_content": "42 Chapter 2 Testing throughout the software development Ife cycle \nThere is a danger that the testing may be less thorough in incremental and iter- \native life cycles, particularly regression testing of previously developed parts, and \nespecially if the regression testing is manual rather than automated. There is also a \ndanger that the testing is less formal, because we are dealing with smaller parts of \nthe system, and formality of the testing may seem like overkill for such a small thing. \nExamples of iterative and incremental development models are Rational Unified \nProcess (RUP), Scrum, Kanban and Spiral (or prototyping). \nRational Unified Process (RUP) \nRUP is a software development process framework from Rational Software Devel- \nopment, a division of IBM. It consists of four steps: \no Inception: the initial idea, planning (for example what resources would be \nneeded) and go/no-go decision for development. done with stakeholders. \no Elaboration: further detailed investigation into resources, architecture and costs, \no Construction: the software product is developed, including testing. \no Transition: the software product is released to customers, with modifications \nbased on user feedback. \nThis development process is tailorable for different contexts, and there are tools \n(and services) to support it. One of the basic principles is that development is iterative, \nwith risk being the primary driver for decisions about the development, Another prin- \nciple (relevant to us) is that the evaluation of quality (including testing) is continuous \nthroughout development. \nIn RUP. the increments that are produced, although significantly smaller than what \nis produced by sequential models, are larger than the increments produced by Agile \ndevelopment (see Scrum below), and would typically take months rather than days \nor weeks 1o complete, They might contain groups of related features, for example. \nScrum \nScrum is an iterative and incremental framework for effective team collabora- \ntion, which is typically used in Agile development, the most well-known iterative \nmethod. It (and Agile) is based on recognizing that change is inevitable and taking \na practical empirical approach to changing priorities. Work is broken down into \nsmall units that can be completed in a fairly short time (days, a week or two or \neven a month). The delivery of a unit of work is called a sprint. For software devel- \nopment, a sprint includes all aspects of development for a particular feature or set \nof small features, everything from requirements (typically user stories) through to \ntesting and (ideally) test automation, \nThe development teams are small (3 to 9 people) and cross-functional, that is, they \ninclude people who perform various roles, and often individuals take on different \ntasks (such as testing) within the team, The key roles are: \n® The Product Owner represents the business, that is, stakeholders and end users. \n@ The Development team (which includes testers) makes its own decisions about \ndevelopment, that is, they are self-organizing with a high level of autonomy. \n® The Scrum Master helps the development team 10 do their work as efficiently \nas possible, by interacting with other parts of the organization and dealing with \nproblems. The Scrum Master is not a manager, but a facilitator for the team, \noyt S Crmpogs Lo N3 B Bt vl Moy et b o, s o4 A 0 b 18 ot (b 3 o, . ot st ey b smppresmed B e ol snbis gt s el e A e gyt oot s e Sy o el ey eTicncs CEngag L BTG e (W 14 Y ol o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 55,
            "page_label": "56"
        }
    },
    {
        "page_content": "Section 1 Software Development Life Cycle Models 43 \nA stand-up meeting of typically around 15 minutes is held each day, for example \nfirst thing in the morning. to update everyone with progress from the previous day and \nplan the work ahead. (This is where the term “scrum’ came from, as in the gathering \nof a rugby team.) \nAt the start of a sprint, in the sprint planning meeting, some features are selected \n10 be implemented, with other features being put on a backlog. Acceptance criteria \napply to user stories, and are similar to test conditions, saying what needs to work for \nthe user story to be considered working. A definition of done can apply to a user story \n{(which includes but goes beyond satisfaction of the acceptance criteria), but also to unit \ntesting, system testing, iterations and releases, After a sprint completes, a retrospective \nshould be held to assess what went well and what could be improved for the next sprint. \nBecause development is limited to the sprint duration which is time-boxed, flex- \nibility is in choosing what can be developed in the time. Compare that to sequential \nmodels, where all the features are sclected first and the time taken to develop all of \nthem is based on that, Thus Scrum (and Agile) enable us to deliver the greatest value \nsoonest, an approach first proposed in the 1980s. \nBecause the iterations are short, the increments are small, such as a few small \nfeatures or even a few enhancements or bug fixes. \nKanban \nKanban came from an approach to work in manufacturing at Toyota. It is a way of \nvisualizing work and workflow. A Kanban board has columns for different stages of \nwork, from initial idea through development and testing stages to final delivery to \nusers. The tasks are put on sticky notes which are moved from left to right through \nthe columns (like an assembly line for cars), \nA key principle of Kanban is to have a limit for work-in-progress activities. If we \nconcentrate on one task, we are much more efficient at doing it, so this approach \nis less wasteful than trying to do little bits of lots of different tasks. This focus on \ncliminating waste makes this a lean approach. \nThere is also a strong focus on user and customer needs. lterations can be a fixed \nlength to deliver a single feature or enhancement, or features can be grouped together \nfor delivery, Kanban can span more than one team's work (as opposed to Scrum). \nIf user stories are grouped by feature, work may span more than one column on the \nKanban board, sometimes referred 1o as swim lanes. \nSpiral (or prototyping) \nThe Spiral model, initially proposed by Bochm [1996] is based on risk. There are \nfour steps: determine objectives, identify risks and alternatives, develop and test, \nand plan the next iteration. Prototypes may be developed as a way of addressing \nrisks: these prototypes may be kept and incorporated into later cycles (an incremen- \ntal approach), they might be discarded (a throw-away prototype), or they may be \nre-worked as part of the next cycle. \nThe diagram of the Spiral model shows development starting small from a centre, \nand moving in a circular way clockwise through the four stages. Each succeeding \ncycle builds outwards in a spiral through the phases, developing more functionality \ncach time. The key driver for the Spiral model is that it is risk-driven. \nAgile development \nIn this section, we will describe what Agile development is and then cover the \nchanges that this way of working brings to testing. This is additional to what you \nom0 300 g Lo 8 s Rt Ay o b i e et 1 o 1 . s s e Gy s o e ey b e ot b et e e b et P Conpage Lonmay murses e 1y 10 ey kbacnd o o 0} S € whacgacs YIS (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 56,
            "page_label": "57"
        }
    },
    {
        "page_content": "44 Chapter 2 Testing throughout the software development Ife cycle \nneed 1o know for the exam, as the Syllabus does not specifically cover Agile devel- \nopment (the ISTQB Foundation Level Agile Tester Extension Syllabus covers this), \nbut we hope this will give you useful background, especially if you are not familiar \nwith it. \nAgile software development is a group of software development methodologies \nbased on iterative and incremental development, where requirements and solutions \nevolve through collaboration between self-organizing cross-functional teams. Most \nAgile teams use Scrum, as described above. Typical Agile teams are 5 to 9 people, \nand the Agile manifesto describes ways of working that are ideal for small teams, and \nthat counteract problems prevalent in the late 19905, with its emphasis on process and \ndocumentation. The Agile manifesto consists of four statements describing what is \nvalued in this way of working: \no individuals and interactions over processes and tools \no working software over comprehensive documentation \n® customer collaboration over contract negotiation \n@ responding to change over following a plan. \nWhile there are several Agile methodologies in practice, the industry seems to \nhave settled on the use of Scrum as an Agile management approach, and Extreme \nProgramming (XP) as the main source of Agile development ideas, Some character- \nistics of project teams using Scrum and XP are: \n® The generation of business stories (a form of lightweight use cases) to define the \nfunctionality, rather than highly detailed requirements specifications. \n® The incorporation of business representatives into the development process, as \npart of each iteration (called a sprint and typically lasting 2 to 4 weeks), provid- \ning continual feedback and to define and carry out functional acceptance testing. \n@ The recognition that we cannot know the future, so changes to requirements \nare welcomed throughout the development process, as this approach can pro- \nduce a product that better meets the stakeholders’ needs as their knowledge \nRrows over time, \n® The concept of shared code ownership among the developers, and the close \ninclusion of testers in the sprint teams. \n® The writing of tests as the first step in the development of a component, and \nthe automation of those tests before any code is written. The component is \ncomplete when it then passes the automated tests, This is known as test-driven \ndevelopment. \no Simplicity: building only what is necessary, not everything you can think of. \n® The continwous integration and testing of the code throughout the sprint, at Jeast \nonce a day. \nProponents of the Scrum and XP approaches emphasize testing throughout the \nprocess. Each iteration (sprint) culminates in a short period of testing, often with an \nindependent tester as well as a business representative. Developers are to write and \nrun test cases for their code, and leading practitioners use tools to automate those \ntests and to measure structural coverage of the tests (see Chapters 4 and 6). Every \ntime a change is made in the code, the component is tested and then integrated with \nthe existing code, which is then tested using the full set of awtomated component \nG N Crmpogs Loswming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At s v e e s oo st P Conpge Lewmay mures e 13 10 ey akoacd o o sy Vo € whacacs A (7O s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 57,
            "page_label": "58"
        }
    },
    {
        "page_content": "Section 1 Software Development Life Cycle Models 45 \ntest cases. This gives continuous integration, by which we mean that changes are \nincorporated continuously into the software build. \nAgile development provides both benefits and challenges for testers. Some of the \nbenefits are: \n® The focus on working software and good quality code. \n® The inclusion of testing as part of and the starting point of software develop- \nment (test-driven development). \n® Accessibility of business stakeholders to help testers resolve questions about \nexpected behaviour of the system. \n® Self-organizing teams, where the whole team is responsible for quality and \ngives testers more autonomy in their work. \no Simplicity of design that should be easier to test, \nThere are also some significant challenges for testers when moving 1o an Agile \ndevelopment approach: \n® Testers who are used to working with well-documented requirements will be \ndesigning tests from a different kind of test basis: less formal and subject 1o \nchange. The manifesto does not say that documentation is no longer necessary \nor that it has no value, but it is often interpreted that way. \n® Because developers are doing more component testing, there may be a percep- \ntion that testers are not needed. But component testing and confirmation-based \nacceptance testing by only business representatives may miss major problems. \nSystem testing. with its wider perspective and emphasis on non-functional \ntesting as well as end-to-end functional testing is needed, even if it does not fit \ncomfortably into a sprint. \n® The tester’s role is different: since there is less documentation and more per- \nsonal interaction within an Agile team, testers need to adapt to this style of \nworking, and this can be difficult for some testers, Testers may be acting more \nas coaches in testing to both stakeholders and developers, who may not have a \nlot of testing knowledge. \n® Although there is Jess to test in one iteration than a whole system, there is also \na constant time pressure and less time to think about the testing for the new \nfeatures, \n® Because each increment is adding 10 an existing working system, regression \ntesting becomes extremely important, and automation becomes more beneficial. \nHowever, simply taking existing awtomated component or component integra- \ntion tests may not make an adequate regression suite, \nSoftware engineering teams are still learning how to apply Agile approaches. \nAgile approaches cannot be applied to all projects or products, and some testing \nchallenges remain to be surmounted with respect to Agile development. However, \nAgile methodologies are showing promising results in terms of both development \nefficiency and quality of the delivered code. \nMore information about testing in Agile development and iterative incremental \nmodels can be found in books by Black [2017], Crispin and Gregory [2008] and \nGregory and Crispin [2015). There is also an ISTQB certificate for the Foundation \nLevel Agile Tester Extension, \not S04 Crmpogs Loy N3 Bt Bt vl My et b oo s o4 bl 0 b 18 ot (o 3 o, . ot ety st ey b smpqpemed B e ol snbis gt s el e A e gyt oot s e Sy R G Tl g pericnce Cengage Ly BTG e (W 14 Y adibicnd o o o € g 4 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 58,
            "page_label": "59"
        }
    },
    {
        "page_content": "46 Chapter 2 Testing throughout the software development Ife cycle \n2.1.2 Software development life cycle models in context \nAs with many aspects of development and testing, there is no one correct or best life \ncycle model for every situation. Every project and product is different from others, \n50 it is important to choose a development model that is suitable for your own situ- \nation or context. The most suitable development model for you may be based on the \nfollowing: \n@ the project goal \no the type of product being developed \n® business priorities (for example time to market) \no identified product and project risks. \nThe development of an internal admin system is not the same as a safety- \ncritical system such as flight control software for aircraft or braking systems \nfor cars. These types of development need different life cycle models in order \nto succeed. The internal admin system may be developed very informally, with \ndifferent features delivered incrementally. The development (and testing) of \nsafety-critical systems needs to be far more rigorous and may be subject to legal \ncontracts and regulatory requirements, so a sequential life cycle model may be \nmore appropriate. \nIt is also important to consider organizational and cultural context. If you \nwant o use Scrum for example, good communication between team members \nis critical. \nThe context also determines the test levels and/or test activities that are appro- \npriate for a project, and they may be combined or reorganized. For the integration \nCommercial off-the- of a commercial off-the-shelf (COTS) software product into a system, for exam- \nshelf (COTS) (off- ple, a purchaser may perform acceptance testing focused on functionality and other \nthe-shelf software) A attributes (for example integration to the infrastructure and other systems). followed \nsoftware product that by a system integration test, The acceptance testing can include testing of system \n& developed foy \". functions, but also testing of quality attributes such as performance and other non- \nm:r;‘“m‘#: o functional tests. The testing may be done from the perspective of the end user and \nt and that may also be done from an operations point of view. \ns delivered to many The context within an organization also determines the most suitable life cycle \ncustomers in identical model. For example, the V-model may be used to develop back office systems, so that \nformat. all new features are integrated and tested before everyone updates to the new system. \nAt the same time, Agile development may be used for the Ul (User Interface) to the \nwebsite, and a Spiral model may be used to develop a new app. \nInternet of Things (loT) systems present special challenges for testing (as well \nas for development). Many different objects need to be integrated together and \ntested in realistic conditions, but each object or device may be developed in a dif- \nferent way with a different life cycle model. There is also more emphasis on the \nlater stages of the development life cycle, after objects are actually in use. There \nmay be extensive updates needed to different devices and supporting software \nsystems once users begin using the systems for real. There may also be unfore- \nseen security issues that might require updates, There may even be issues when \ntrying to decommission devices or software for loT systems. Changing contexts \nalways have an influence on testing, and in our technological world, constant \nchange is definitely the norm, so testing (and development) always needs to adapt \n1o its context. \nG N Crmpoge Lowming. A3 Kighs Bt vt My et b o, s, o4 M, 0 s 10 . o, (b 3 s e, s sty comto ey b gy B e el b AChagati s el e M e s syt oot s et Sy o ol ey pericnce Cengag Liwing BTN e (W 14 Y ol o o e e € g 430 (s s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 59,
            "page_label": "60"
        }
    },
    {
        "page_content": "2.2 TEST LEVELS \nSYLLABUS LEARNING OBJECTIVES FOR 2.2 TEST \nLEVELS (K2) \nFL-2.2.1  Compare the different test levels from the perspective of \nobjectives, test basis, test objects, typical defects and failures, \nand approaches and responsibilities (K2) \nWe have mentioned (and given the definition for) test levels in Section 2.1. The defi- \nnition of “test level” is ‘an instance of the test process’, which is not necessarily the \nmost helpful. The Syllabus here describes test levels as: \ngroups of test activities that are organized and managed together \nThe test activities were described in Chapter 1. Section 1.4 (test planning through \nto test completion). When we talk about test levels, we are looking at those activi- \nties performed with reference to development levels (such as those described in the \nVemodel) from components to systems, or even systems of systems. \nIn this section, we'll look in more detail at the various test levels and show how \nthey are related to other activities within the software development life cycle. The \nkey characteristics for each test level are discussed and defined, 1o be able to more \nclearly scparate the various test levels. A thorough understanding and definition of \nthe various test levels will identify missing areas and prevent overlap and repetition. \nSometimes we may wish to introduce deliberate overlap to address specific risks. \nUnderstanding whether we want overlaps and removing the gaps will make the test \nlevels more complementary, leading to more effective and efficient testing. We will \nlook at four test levels in this section. \nAs we go through this section, watch for the Syllabus terms acceptance testing, \nalpha testing, beta testing. component integration testing, component testing, \ncontractual acceptance testing, integration testing, operational acceptance test- \ning, regulatory acceptance testing. system integration testing. system testing, test \nbasis, test case, test environment, test object. test objective and user acceptance \ntesting. These terms are also defined in the Glossary. \nWhile the specific test levels required for - and planned for - a particular project \ncan vary, good practice in testing suggests that each test level has the following \nclearly identified: \n® Specific test objectives for the test level. \n® The test basis, the work product(s) used to derive the test conditions and test cases. \n® The test object (that is, what is being tested such as an item, build, feature or \nsystem under test). \n® The typical defects and failures that we are looking for at this test level. \n® Specific approaches and responsibilities for this test level. \nOne additional aspect is that each test level needs a test environment. Sometimes \nan environment can be shared by more than one test level: in other situations, a \nparticular environment is needed. For example, acceptance testing should have \n4 test environment that is as similar to production as is possible or feasible. \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 bl i, 0 s 18 . o, U 3 i, e, st sty comtos ey b gy B e el smbs AChagit s - \nSection 2 TestLevels 47 \ncomponent or system \n1o be tested. See also: \ntest item. \nTest environment \n(test bed, test rig) Pt \ncontaining hardware, \ninstrumentation, \nsimulators, software \ntools and other support \nelements needed to \nconduct a test. \ne e dmd sy oo s et s 8 cven earwny Conpe! s e e o e .",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 60,
            "page_label": "61"
        }
    },
    {
        "page_content": "48 Chapter 2 Testing throughout the software development Ife cycle \nIn component testing, developers often just use their development environment. In \nsystem testing, an environment may be needed with particular external connections, \nfor example. \n‘When these topics are clearly understood and defined for the entire project team, \nthis contributes to the success of the project. In addition, during test planning, the \nmanagers responsible for the test levels should consider how they intend to test a \nsystem’s configuration, if such data is part of a system. \n2.2.1 Component testing \nComponent testing Component testing, also known as unit or module testing, searches for defects in, \n(module testing, unit and verifies the functioning of, software items (for example modules, programs, \ntesting) The testingof  objects, classes, etc.) that are separately testable, \nindwidual hardware o Component tests are typically based on the requirements and detailed design \nsoftware components. specifications applicable to the component under test, as well as the code itself (which \nwe'll discuss in Chapter 4 when we talk about white-box testing). \nThe component under test, the test object, includes the individual components, the \ndata conversion and migration programs used to enable the new release, and database \ntables, joins, views, modules, procedures, referential integrity and field constraints, \nand even whole databases. \nComponent testing: objectives \nThe different test levels have different objectives. The objectives of component test- \ning include: \n® Reducing risk (for example by testing high-risk components more extensively). \n® Verifying whether or not functional and non-functional behaviours of the com- \nponent are as they should be (as designed and specified). \no Building confidence in the quality of the component: this may include measur- \ning structural coverage of the tests, giving confidence that the component has \nbeen tested as thoroughly as was planned. \no Finding defects in the component, \no Preventing defects from escaping 1o later testing. \nIn incremental and iterative development (for example Agile). automated com- \nponent regression tests are run frequently, to give confidence that new additions or \nchanges to a component have not caused existing components or links to break. \nComponent testing may be done in isolation from the rest of the system depend- \ning on the context of the development life cycle and the system. Most often, mock \nobjects or stubs and drivers are used to replace the missing software and simulate \nthe interface between the software components in a simple manner. A stub or mock \nobject is called from the software component to be tested; a driver calls a compo- \nnent 1o be tested (see Figure 2.4). Test harnesses may also be used to provide similar \nfunctionality, and service virtualization can give cloud-based functionality to test \ncomponents in realistic environments. \nComponent testing may include testing of functionality (for example are the \ncalculations correct) and specific non-functional characteristics such as resource- \nbehaviour (for example memory leaks), performance testing (for example do \ncalculations complete quickly enough), as well as structural testing. Test cases are \nderived from work products such as the software design or the data model. \nGt N Crmpogs Lowming. A3 g Bt vt My et b o, s, o4 B, s 10 . o, (b 3 s, g, s sty cntr ey b gy B e el snbs AChapit s el e A e gyt oot s e Sy o el Wy ericncs Cengage Lewing TGS e (W 14 Y adlbmcnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 61,
            "page_label": "62"
        }
    },
    {
        "page_content": "Section 2 TestLevels 49 \nComponent: A A Driver \nComponent: 8 Stub B \nFIGURE 2.4 Stubs and drivers \nComponent testing: test basis \nWhat is this particular component supposed to do? Examples of work products that \ncan be used as a test basis for component testing include: \no detailed design \n® code \no data model \n® component specifications (if available). \nComponent testing: test objects \nWhat are we actually testing at this level? We could say the smallest thing that can \nbe sensibly tested on its own. Typical test objects for component testing include: \n® components themselves, units or modules \n@ code and data structures \n® classes \no database models, \nComponent testing: typical defects and failures \nExamples of defects and failures that can typically be revealed by component testing \ninclude: \n® incorrect functionality (for example not as described in a design specification) \no data flow problems \no incorrect code or logic. \nAt the end of the description of all the test levels, see Table 2.1 which summarizes \nthe characteristics of each test level. \nComponent testing: specific approaches and responsibilities \nTypically, component testing occurs with access to the code being tested and with the \nsupport of the development environment, such as a unit test framework or debugging \ntool, In practice it usually involves the developer who wrote the code. The developer \nmay change between writing code and testing it. Sometimes, depending on the \napplicable level of risk, component testing is carried out by a different developer, \nintroducing independence. Defects are typically fixed as soon as they are found, \nwithout formally recording them in a defect management tool. Of course, if such \ndefects are recorded, this can provide useful information for root cause analysis. \no N Crmpoge Lossming. A3 K Bt vt My et b o, s, o4 b . 0 s 18 . o, (b 3 i, e, s sty comto ey b gy B e el smbs At e e b s oo st s 8 cvent barwny Conprge Lowmag maries e 1y 10 ey akbmcnd o o e} o € whacgacm A (7 0Tv s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 62,
            "page_label": "63"
        }
    },
    {
        "page_content": "50 Chapter 2 Testing throughout the software development Ife cycle \nOne approach in component testing. initially developed in Extreme Programming \n(XP), is to prepare and automate test cases before coding. This is called a test-first \napproach or test-driven development (TDD). This approach is highly iterative and is \nbased on cycles of developing automated tests, then building and integrating small \npicces of code, and executing the component tests until they pass, and is typically \ndone in Agile development. The idea is that the first thing the developer does is o \nwrite some automated tests for the component. Of course if these are run now, they \nwill fail because no code is there! Then just enough code is written until those tests \npass. This may involve fixing defects now found by the tests and re-factoring the \ncode. (This approach also helps to build only what is needed rather than a lot of \nfunctionality that is not really wanted.) \n2.2.2 Integration testing \nIntegration testing tests interfaces between components and inteructions of dif- \nferent parts of a system such as an operating system, file system and hardware or \ninterfaces between systems. Integration tests are typically based on the software and \nsystem design (both high-level and low-level), the system architecture (especially \nthe relationships between components or objects) and the workflows or use cases by \nwhich the stakeholders will employ the system. \nThere may be more than one level of integration testing and it may be carried out \non test objects of varying size, For example: \no Component integration testing tests the interactions between software compo- \nnents and is done after component testing. It is a good candidate for automation, \nIn iterative and incremental development, both component tests and integration \ntests are usually part of continuous integration, which may involve automated \nbuild. test and release to end users or to a next level. At least, this is the theory. \nIn practice, component integration testing may not be done at all, or misunder- \nstood and as a consequence not done well. \n® System integration testing tests the interactions between different systems, \npackages and microservices, and may be done after system testing, System \nintegration testing may also test interfaces to and provided by external organiza- \ntions (such as web services). In this case, the developing organization may con- \ntrol only one side of the interface, resulting in a number of problems: changes \nmay be destabilizing, defects in the external organization's software may block \nprogress in the testing, or special test environments may be needed. Business \nprocesses implemented as workflows may involve a series of systems that can \neven run on different platforms. System integration testing may be done in par- \nallel with other testing activities. \nIntegration testing: objectives \nThe objectives of integration testing include: \n® Reducing risk, for example by testing high-risk integrations first. \n® Verifying whether or not functional and non-functional behaviours of the inter- \nfaces are as they should be, as designed and specified. \no Building confidence in the quality of the interfaces. \nG N Compogs Lowming. A3 Kighs Bt vt My et b o, s, o4 e, 0 s 10 . o, (b 3 i, g, s sty comto ey b gy B e el snbis At s el e A e gyt oot s e Sy o el ey ericncs CEngage L BTG e (W 14 Y adlbnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 63,
            "page_label": "64"
        }
    },
    {
        "page_content": "Section 2 TestLevels 51 \no Finding defects in the interfaces themselves or in the components or systems \nbeing tested together. \n® Preventing defects from escaping to later testing, \nAutomated integration regression tests (such as in continuous integration) pro- \nvide confidence that changes have not broken existing interfaces, components or \nsystems. \nIntegration testing: test basis \nHow are these components or systems supposed 1o work together and communicate” \nExamples of work products that can be used as a test basis for integration testing \ninclude: \nsoftware and system design \nsequence diagrams \ninterface and communication protocol specifications \nuse cases \narchitecture at component or system level \nworkflows \nexternal interface definitions. \nIntegration testing: test objects \nWhat are we actually testing at this level? The emphasis here is in testing things \nwith others which have already been tested individually. We are interested in how \nthings work together and how they interact. Typical test objects for integration test- \ning include: \n® subsystems \ndatabases \ninfrastructure \ninterfaces \nAPIs (Application Programming Interfaces) \nmicroservices. \nIntegration testing: typical defects and failures \nExamples of defects and failures that can typically be revealed by component inte- \ngration testing include: \n® Incorrect data, missing data or incorrect data encoding. \n® Incorrect sequencing or timing of interface calls, \n® Interface mismatch, for example where one side sends a parameter where the \nvalue exceeds 1,000, but the other side only expects values up to 1.000. \no Failures in communication between components. \n® Unhandled or improperly handled communication failures between \ncomponents. \no Incorrect assumptions about the meaning, units or boundaries of the data being \npassed between components. \no N Crmpoge Lossming. A3 g Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i, g, s sty comto ey b gy B e el smbs At Je——— e e b s s & cvent barwny Conpage Lowmag marses e 1y 10 ey akbmcnd o o 41w € shargacm IS (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 64,
            "page_label": "65"
        }
    },
    {
        "page_content": "52 Chapter 2 Testing throughout the software development Ife cycle \nExamples of defects and failures that can typically be revealed by system integra- \ntion testing include: \ninconsistent message structures between systems \nincorrect data, missing data or incorrect data encoding \ninterface mismatch \nfailures in communication between systems \nunhandled or improperly handled communication failures between systems \nincorrect assumptions about the meaning, units or boundaries of the data being \npassed between systems \no failure to comply with mandatory security regulations. \nAt the end of the description of all the test levels, see Table 2.1 which summarizes \nthe characteristics of each test level, \nIntegration testing: specific approaches and responsibilities \nThe greater the scope of integration, the more difficult it becomes to isolate failures \n1o a specific interface, which may lead to an increased risk. This leads to varying \napproaches 10 integration testing. One extreme is that all components or systems \nare integrated simultaneously, after which everything is tested as a whole. This is \ncalled big-bang integration. Big-bang integration has the advantage that everything \nis finished before integration testing starts. There is no need to simulate (as yet \nunfinished) parts. The major disadvantage is that in general it is time-consuming \nand difficult to trace the cause of failures with this late integration. So big-bang \nintegration may seem like a good idea when planning the project. being optimistic \nand expecting to find no problems. If one thinks integration testing will find defects, \nit is a good practice to consider whether time might be saved by breaking down the \nintegration test process. \nAnother extreme is that all programs are integrated one by one, and tests are \ncarried out after each step (incremental testing). Between these two extremes, there \nis a range of variants, The incremental approach has the advantage that the defects \nare found carly in a smaller assembly when it is relatively easy to detect the cause. A \ndisadvantage is that it can be time-consuming, since mock objects or stubs and drivers \nmay have to be developed and used in the test. Within incremental integration testing \na range of possibilities exist, partly depending on the system architecture: \no Top-down: testing starts from the top and works 1o the bottom, following the \ncontrol flow or architectural structure (for example starting from the GUI or \nmain menu). Components or systems are substituted by stubs, \no Bottom-up: testing reverses this approach, starting from the bottom of the con- \ntrol flow upwards. Components or systems are substituted by drivers. \no Functional incremental: integration and testing takes place on the basis of the \nfunctions or functionality, as documented in the functional specification. \nThe preferred integration sequence and the number of integration steps required \ndepend on the location in the architecture of the high-risk interfaces, The best \nchoice is to stast integration with those interfaces that are expected to cause the most \nproblems. Doing so prevents major defects at the end of the integration test stage. \nIn onder to reduce the risk of late defect discovery, integration should normally be \nincremental rather than big bang. Ideally, testers should understand the architecture \no N Compoge Lossming. A3 K Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 st e, s s paty comto ey b sy B e el smbs At el s M e gyt oo s e Sy e el g ericnce Cengage Lty BTN e (W 14 Y adibmcnd o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 65,
            "page_label": "66"
        }
    },
    {
        "page_content": "and influence integration planning. If integration tests are planned before components \nor systems are built, they can be developed in the order required for most efficient \ntesting. A risk analysis of the most complex interfaces can help to focus integration \ntesting. In iterative and incremental development, integration is also incremental. \nExisting integration tests should be part of the regression tests used in continuous \nintegration. Continuous integration has major benefits because of its iterative nature. \nAt cach stage of integration, testers concentrate solely on the integration itself. For \nexample, if they are integrating component A with component B they are interested \nin testing the communication between the components, not the functionality of either \none. In integrating system X with system Y, again the focus is on the communication \nbetween the systems and what can be done by both systems together, rather than \ndefects in the individual systems. Both functional and structural approaches may be \nused. Testing of specific non-functional characteristics (for example performance) \nmay also be included in integration testing. \nComponent integration testing is often carried out by developers; system integra- \ntion testing is generally the responsibility of the testers. Either type of integration \ntesting could be done by a separate team of specialist integration testers, or by a \nspecialist group of developersfintegrators, including non-functional specialists. The \ntesters performing the system integration testing need to understand the system archi- \ntecture. Ideally, they should have had an influence on the development, integration \nplanning and integration testing. \n2.2.3 System testing \nSystem testing is concerned with the behaviour of the whole system/product as defined \nby the scope of a development project or product. It may include tests based on risk \nanalysis reports, system, functional or software requirements specifications, business \nprocesses, use cases or other high-level descriptions of system behaviour, interactions \nwith the operating system and system resources, The focus is on end-to-end tasks that \nthe system should perform, including non-functional aspects, such as performance. \nIn some systems, the quality of the data may be of critical importance, so there would \nbe a focus on data quality, System level tests may be automated 10 provide a regression \nsuite to ensure that changes have not adversely affected existing system functionality. \nStakeholders may use the information from system testing to decide whether the system \nis ready for user acceptance testing, for example. System testing is also where conform- \nance 1o kegal or regulatory requirements or to external standards is tested. \nThe test environment is important for system testing: it should correspond 1o the \nfinal production environment as much as possible. \nSystem testing: objectives \nThe objectives of system testing include: \n® reducing risk \nverifying whether or not functional and non-functional behaviours of the system \nare as they should be (as specified) \nvalidating that the system is complete and will work as it should and as expected \nbuilding confidence in the quality of the system as a whole \nfinding defects \npreventing defects from escaping to later testing or to production. \nSection 2 TestLevels 53 \nSystem testing Testing \nan integrated system \nto verify that it meets \nspecified requirements. \n(Note that the ISTQB \ndefinition implies that \nsystem testing Is only \nabout verification of \nspecified requirements. \nIn practice, system \ntesting is often also \nabout valdation that \nthe system is suitable \nfor its intended users, \nas well as verifying \nagainst any type of \nrequirement.) \nG N Crmpugs Lowming. A3 Kighs Bt vt My et b o, s, o4 e, 0 s 18 . o, U 3 i e, s sty commo ey b gyt B e el smbs At s \nJe—p—— Conpaet e e b ety s 8 cvent oy s e (e 4 e sl o o o € g 0 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 66,
            "page_label": "67"
        }
    },
    {
        "page_content": "54 Chapter 2 Testing throughout the software development Ife cycle \nSystem testing: test basis \nWhat should the system as a whole be able to do? Examples of work products that \ncan be used as a test basis for system testing inchude: \n® software and system requirement specifications (functional and non-functional) \no risk analysis reports \n® use cases \n® epics and user stories \no models of system behaviour \no state diagrams \n® system and user manuals. \nSystem testing: test objects \nWhat are we actually testing at this level? The emphasis here is in testing the whole \nsystem, from end to end, encompassing everything that the system needs to do (and \nhow well it should do it, so non-functional aspects are also tested here). Typical test \nobjects for integration testing include: \no applications \no hardware/software systems \n® operating systems \n@ system under test (SUT) \n® system configuration and configuration data. \nSystem testing: typical defects and failures \nExamples of defects and failures that can typically be revealed by system testing \ninclude: \nincorrect calculations \nincorrect or unexpected system functional or non-functional behaviour \nincorrect control and/or data flows within the system \nfailure to properly and completely carry out end-to-end functional tasks \nfailure of the system to work properly in the production environment(s) \nfailure of the system 1o work as described in system and user manuals, \nAt the end of the description of all the test levels, see Table 2.1 which summarizes \nthe characteristics of each test level. \nSystem testing: specific approaches and responsibilities \nSystem testing is most often the final test on behalf of development to verify that \nthe system 10 be delivered meets the specification and 1o validate that it meets \nexpectations; one of its purposes is to find as many defects as possible. Most often it \nis carried out by specialist testers that form a dedicated, and sometimes independ- \nent, test team within development, reporting to the development manager or project \nmanager. In some organizations system testing is carried out by a third-party team \nor by business analysts, Again, the required level of independence is based on the \napplicable risk level and this will have a high influence on the way system testing \nis organized. \nG N Compoge Lossming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 ot e, st sty conto ey b sy B e el smbs At e e b s oo st s & cvent barwny Conpuge Lonmag maries e 1 10 ey akbacnd o o a1 o € whacgacs A (7 0TA s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 67,
            "page_label": "68"
        }
    },
    {
        "page_content": "Section 2 TestLevels 55 \nSystem testing should investigate end-to-end behaviour of both functional \nand non-functional aspects of the system. An end-to-end test may include all of \nthe steps in a typical transaction, from logging on, accessing data, placing an \norder, etc. through to logging off and checking order status in a database. Typ- \nical non-functional tests include performance, security and reliability. Testers \nmay also need to deal with incomplete or undocumented requirements. System \ntesting of functional requirements starts by using the most appropriate black-box \ntechniques for the aspect of the system to be tested. For example, a decision table \nmay be created for combinations of effects described in business rules. White- \nbox techniques may also be used 1o assess the thoroughness of testing elements \nsuch as menu dialogue structure or web page navigation (see Chapter 4 for more \non test techniques). \nSystem testing requires a controlled test environment with regard to, among \nother things, control of the software versions, testware and the test data (see Chap- \nter § for more on configuration management). A system test is executed by the \ndevelopment organization in a (properly controlled) environment. The test envi- \nronment should correspond 1o the final target or production environment as much \nas possible in order to minimize the risk of environment-specific failures not being \nfound by testing. \nSystem testing is often carried out by independent testers, for example an internal \ntest team or external testing specialists. However, if testers are only brought in when \nsystem test execution is about to start, you will miss a lot of opportunities to save \ntime and money, as well as aggravation. If there are defects in specifications, such \nas missing functions or incorrect descriptions of business processes, these may not \nbe picked up before the system is built. Because many defects result from misunder- \nstandings, the discussions (indeed arguments) about them tend to be worse the later \nthey are discovered, The developers will defend their understanding because that is \nwhat they have built. The independent testers or end-users may realize that what was \nbuilt was not what was wanted. This situation can lead to defects being missed in \ntesting (if they are based on wrong specifications) or things being reported as defects \nthat actually are not (due to misunderstandings). These are known as false negative \nand false positive results respectively. Referring back to testing Principle 3, carly test \ninvolvement saves time and money, so have testers involved in user story refinement \nand static testing such as reviews, \n2.2.4 Acceptance testing Formal testing with \n‘When the development organization has performed its system test (and possibly also requirements, and \nsystem integration tests) and has corrected all or most defects, the system may be  pysiness processes \ndelivered for acceptance testing, Acceptance tests typically produce information  conducted to determine \nto assess the system’s readiness for release or deployment to end-users or custom-  whether of not a \ners, Although defects are found at this level, that is not the main aim of acceptance  system satisfies the \ntesting. (If lots of defects are found at this late stage. there are serious problems  acceptance citeria \nwith the whole system, and major project risks.) The focus is on validation, the use ~ and 10 enable the user, \nof the system for real, how suitable the system is to be put into production or actual customers or other \nuse by its intended users. Regulatory and legal requirements, and conformance to Juthorized entiy s \nstandards may also be checked in acceptance testing, although they should also have AT the \nbeen addressed in an eardier level of testing. so that the acceptance test is confirming system, \ncompliance to the standards, \nG N Crmpogs Lowming. A3 Kt Bt vt My et b o, s, o4 e, 0 s 18 . o, (b 3 i, g, s sty cmto ey b gy B e el b At el e M e gyt oot s o Sy o el g pericnce Cengage Lewing BTG e (W 14 Y adlbmnd o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 68,
            "page_label": "69"
        }
    },
    {
        "page_content": "56 Chapter 2 Testing throughout the software development Ife cycle \nm_etheusysm \nAcceptance testing: objectives \nThe objectives of acceptance testing include: \n® establishing confidence in the quality of the system as a whole \n@ validating that the system is complete and will work as expected \no verifying that functional and non-functional behaviours of the system are as \nspecified. \nDifferent forms of acceptance testing \nAcceptance testing is quite a broad category, and it comes in several different fla- \nvours or forms. We will look at four of these. \nUser acceptance testing (UAT) \nUser acceptance testing is exactly what it says, It is acceptance testing done by (or on \nbehalf of) users, that is, end-users. The focus is on making sure that the system is really \nfit for purpose and ready to be used by real intended users of the system. The UAT \ncan be done in the real environment or in a simulated operational environment (but as \nrealistic as possible). The aim of testing here s to build confidence that the system will \nindeed enable the users 1o do what they need 1o do in an efficient way. The system needs \nto fulfil the requirements and meet their needs. The users focus on their business pro- \ncesses, which they shoukl be able to perform with a minimum of difficulty, cost and risk, \nOperational acceptance testing (OAT) \nOperational acceptance testing focuses on operations and may be performed by \nsystem administrators. The main purpose is to give confidence to the system admin- \nistrators or operators that they will be able 1o keep the system running. and recover \nfrom adverse events quickly and without additional risk. It is normally performed in \na simulated production environment and is looking at operational aspects, such as: \n® testing of backups and restoration of backups \no installing, uninstalling and upgrading \n® disaster recovery \n® user management \n® maintenance tasks \no data loading and migration tasks \n@ checking for security vulnerabilities (for example ethical hacking) \no performance and load testing. \nContractual and regulatory acceptance testing \nIf asystem has been custom-developed for another company, there is normally a legal \ncontract describing the responsibilities, schedule and costs of the project. The con- \ntract should also include or refer 10 acceplance criteria for the system, which should \nhave been defined and agreed when the contract was first taken out. Having agreed \nthe acceptance criteria in advance, contractual acceptance testing is focused on \nwhether or not the system meets those criteria. This form of testing is often per- \nformed by users or independent testers, \nRegulatory acceptance testing is focused on ensuring that the system conforms to \ngovernment, legal or safety regulations. This type of testing is also often performed by \nG N Crmpoge Lowming. A3 g Bt vt My et b o, s, o4 Al i, 0 s 10 . o, U 3 i e, s sty comtr ey b gy B e el b At Conpaet e e b et oo p—— s 8 cmeni oy s e (e 4 e ol o o o € g 4 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 69,
            "page_label": "70"
        }
    },
    {
        "page_content": "Section 2 TestLevels 57 \nindependent testers. It may be a requirement 1o have a representative of the regulatory \nbady present to witness or to audit the tests. \nFor both of these forms of acceptance testing, the aim is to build confidence that \nthe system is in conformance with the contract or regulations. \nAlpha and beta testing \nAlpha and beta testing are typically used for COTS software, such as software pack- \nages that can be bought or downloaded by consumers. Feedback is needed from \npotential or existing users in their market before the software product is put out for \nsale commercially. The testing here is looking for feedback (and defects) from real \nusers and customers or potential customers. Sometimes free software is offered to \nthose who volunteer to do beta testing. \n“The difference between alpha and beta testing is only in where the testing takes place. \nAlpha testing is at the company that developed the software, and beta testing isdone in-~ Alpha testing \nthe users” own offices or homes. In alpha testing, a cross-section of potential users are  Simulated or actual \ninvited to use the system. Developers observe the users and note problems. Alphatesting ~ Operational testing \nmay also be carried out by an independent test team. Alpha testing is normally mentioned  €onducted in the \nfirst, but these two forms can be done in any order, or only one could be done (or none).  déveloper's test \nBeta testing sends the system or software package out 10 a cross-section of users \nwho install it and use it under real-world working conditions. The users send records W \nof defects with the system to the development organization, where the defects are \nrepaired. Beta testing is more visible, and is increasingly popular to be done remotely.  Beta testing (field \nFor example, crowd testing, where people or potential users from all over the world testing) m \nremotely test an application, can be a form of beta testing. One of the advantages of  Of actual operational \nbeta testing is that different users will have a great variety of different environments fisting conducted \n(browsers. other software, hardware configurations, etc.), so the testing can cover ;:;5 l’\"’ \nmany more combinations of factors. the development \norganization. Acceptance testing: test basis \nHow do we know that the system is ready to be used for real? Examples of work \nproducts that can be used as a test basis for the various forms of acceptance testing \ninclude: \nbusiness processes \nuser or business requirements \nregulations, legal contracts and standards \nuse cases. \nsystem requirements \nsystem or user documentation \ninstallation procedures \nrisk analysis reports. \nFor operational acceptance testing (OAT), there are some additional aspects with \nspecific work products that can be a test basis: \n® backup and restore/recovery procedures \no disaster recovery procedures \no non-functional requirements \n® operations documentation \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 b . 0 s 18 . o, U 3 i, e, s sty comto ey b gy B e el snbs At s el i M v gyt oot s e sy S8 ool g pericnce Cengag L BTG e (W 14 Y oS o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 70,
            "page_label": "71"
        }
    },
    {
        "page_content": "58 Chapter 2 Testing throughout the software development Ife cycle \no deployment and installation instructions \no performance targets \no database packages \n® security standards or regulations, \nNote that it is particularly important to have very clear, well-tested and frequently \nrehearsed procedures for disaster recovery and restoring backups. If you are in the \nsituation of having to perform these procedures, then you may be in a state of panic, \nsince something serious will have already gone wrong. In that psychological state, \nit is very easy to make mistakes, and here mistakes could be disastrous. There are \nstories of organizations who compounded one disaster by accidentally deleting their \nbackups. or who find that their backups are unusable or incomplete! This is why \nrestoring from backups is an important test to do regularly. \nAcceptance testing: test objects \nWhat are we actually testing ot this level? The emphasis here is in gaining confi- \ndence, based on the particular form of acceptance testing: user confidence, confi- \ndence in operations, confidence that we have met legal or regulatory requirements, \nand confidence that real users will like and be happy with the software we are sell- \ning. Some of the things we are testing are similar to the test objects of system testing. \nTypical test objects for acceptance testing include: \n® system under test (SUT) \n® system configuration and configuration data \n@ business processes for a fully integrated system \n® recovery systems and hot sites (for business continuity and disaster recovery \ntesting) \n operational and maintenance processes \no forms \n® reports \n® existing and converted production data. \nAcceptance testing: typical defects and failures \nExamples of defects and failures that can typically be revealed by acceptance testing \ninclude: \no system workflows do not meet business or user requirements \n® business rules are not implemented correctly \n® system does not satisfy contractual or regulatory requirements \no non-functional failures such as security vulnerabilitics, inadequate performance \nefficiency under high load, or improper operation on a supported platform. \nAt the end of the description of all the test levels, see Table 2.1 which summarizes \nthe characteristics of each test level. \nAcceptance testing: specific approaches and responsibilities \nThe acceptance test should answer questions such as: ‘Can the system be released?’, \n“What, if any, are the outstanding (business) risks?\" and ‘Has development met \ntheir obligations?\" Acceptance testing is most often the responsibility of the user or \nG N4 Compoge Lossming. A3 g Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i, g, s sty comto ey b sy B e el smbs At Je——— e e b s s & cvent barwny Conpage Lowmay muries e 1y 10wy akbmcnd o o a1 V€ shacacm IS (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 71,
            "page_label": "72"
        }
    },
    {
        "page_content": "Section 2 TestLevels 59 \ncustomer, although other stakeholders may be involved as well. The execution of the \nacceptance test requires a test environment that is, for most aspects, representative \nof the production environment (‘as-f production’). \nThe goal of acceptance testing is to establish confidence in the system, part of the \nsystem or specific non-functional charactenstics, for example usability of the system. \nAcceptance testing is most often focused on a validation type of testing, where we are \ntrying to determine whether the system is fit for purpose. Finding defects should not \nbe the main focus in acceptance testing. Although it assesses the system’s readiness \nfor deployment and use, it is not necessarily the final level of testing. For example, a \nlarge-scale system integration test may come after the acceptance of a system. \nAcceptance testing may occur at more than just a single level, for example: \n® A COTS software product may be acceptance tested when it is installed or \nintegrated. \n® Acceptance testing of the usability of a component may be done during compo- \nnent testing, \n® Acceptance testing of a new functional enhancement may come before system \ntesting. \nUser acceptance testing focuses mainly on the functionality, thereby validating \nthe fitness for use of the system by the business user, while the operational acceptance \ntest (also called production acceptance test) validates whether the system meets the \nrequirements for operation. The user acceptance test is performed by the users and \napplication managers. In terms of planning, the user acceptance test usually links \ntightly to the system test, and will, in many cases, be organized partly overlapping \nin time, If the system 1o be tested consists of @ number of more or less independent \nsubsystems, the acceptance test for a subsystem that meets its exit criteria from the \nsystem test can start while another subsystem may still be in the system test phase. \nIn most organizations, system administration will perform the operational accept- \nance test shortly before the system is released. The operational acceptance test may \ninclude testing of backup/restore, data load and migration tasks, disaster recovery, \nuser management, maintenance tasks and periodic check of security vulnerabilities. \nNote that organizations may use other terms, such as factory acceptance testing \nand site acceptance testing for systems that are tested before and after being moved \n10 a customer’s site. \nIn iterative development, different forms of acceptance testing may be done at \nvarious times, and often in parallel, At the end of an iteration, a new feature may \nbe tested to validate that it meets stakeholder and user needs. This is user accept- \nance testing. If software for general release (COTS) is being developed, alpha \nand beta testing may be used at or near the end of an iteration or set of iterations. \nOperational and regulatory acceptance testing may also occur at the end of an \niteration or set of iterations. \nTest level characteristics: summary \nTable 2.1 summarizes the characteristics of the different test levels: the test basis, \ntest objects and typical defects and failures. We have covered these in the various \nsections, but it is useful to contrast them in order to distinguish them from cach \nother. We have omitted some of the detail to make the table easier 1o take in at a \nglance. Note that some of the typical defects for integration testing are oaly for \nsystem integration testing (SIT). \nG N Crmpogs Lowming A3 gt Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 s e, ot sty st ey b gy B e el snbs At v e e s oo st P Conpge Lewmay murses e 1y 10 ey akbacnd comwm o iy Vow € whacacs A (7 OTA s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 72,
            "page_label": "73"
        }
    },
    {
        "page_content": "um \nguises \n2 \no \n—— \ni \no \ng \nvy \na \n1otesy \n11 \not \n1 1y \ng \noy \ne \npomabbn \nG \ns \ni \no \ne \ng \n| \ne \n% \n] \n1 \n0 \ng \n1 o \ng \n0 P \ne \no \nSy \nP \ng \nT \nY \ne \no \n) T \nPk \ns s oy b 3 3 1 > PRy A\\ o R \n@ TABLE 2.1 Testlevel characteristics \n? \nTest basis \nComponent testing \nreduce risk \nverify functional and non- \nfunctional behaviour \nbuild confidence in \ncomponents \nIntegration testing \nreduce risk \nverify functional and non- \nfunctional behaviour \nbuild confidence in interfaces \nfind defects \nprevent defects 10 higher \nprevent defects to higher \nlevels \nrequirement specs (functional \nand non-functional) \nrisk analysss reports \nAcceptance testing \nestablsh confidence in whole \nsystem and its use \nvalidate completeness, works \nas expected \nverify functional and non- \nfunctional behaviour",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 73,
            "page_label": "74"
        }
    },
    {
        "page_content": "11 \nvl \n1 1y \ng \no \ne \npomrbb \n(e \ns \ni \no \ne \ng \ne \n% \n] \n1 \n00 \ng \n1 o \ng \nP \ne \no o \nSy \nP \ng \nY \nB \no \n) T \nPk \n19 \n4 b s oy b 3 1 > PR A o1 R g < B 3bebes) S5k B IR A3 7 LR TG b ey ) gz 4 o s 7Y 88 ] \nTypical defects \nand failures \nwrong functionality \ndata flow problems. \nincorrect codefogic \nIntegration testing \ndata problems \nInconsistent message \nstructure (SIT) \ntiming problems \nInterface mismatch \ncommunecation failures \nincorrect assumptions \nnot complying with \nregulations (SIT) \nSystem testing \napplications. \nhardware/software \noperating systems \nsystem under test \nsystem configuration and data \nrecovery systems \noperation and maintenance \nprocesses \nforms \nreports \nexisting and converted \nproduction data \nsystem workflows do not \nmeet business or user \nneeds \nbusiness rules not correct \ncontractual or regulatory \nproblems \nnon-functional failures \n(performance, security)",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 74,
            "page_label": "75"
        }
    },
    {
        "page_content": "62 Chapter 2 Testing throughout the software development Ife cycle \n2.3 TEST TYPES \nLLABUS LEARNING OBJECTIVES FOR 2.3 TEST \nTYPES (K2) \nFL-23.1  Compare functional, non-functional, and white-box testing (K2) \nFL-232 Recognize that functional, non-functional and white-box tests \noccur at any test level (K1) \nFL-233 Compare the purposes of confirmation testing and regression \ntesting (K2) \nIn this section, we'll look at different test types, We'll discuss tests that focus on the \nfunctionality of a system, which informally is testing whar the system does. We'll \nalso discuss tests that focus on non-functional attributes of a system, which infor- \nmally is testing how well the system does what it does, We'll introduce testing based \non the system’s structure. Finally, we'll look at testing of changes to the system, \nboth confirmation testing (testing that the changes succeeded) and regression testing \n(testing that the changes did not affect anything unintentionally). \nThe test types discussed here can involve the development and use of a model of \nthe software or its behaviours. Such models can occur in structural testing when we \nuse control flow models or menu structure models. Such models in non-functional \ntesting can involve performance models, usability models and security threat models. \nThey can also arise in functional testing, such as the use of process flow models, state \ntransition models or plain language specifications. Examples of such models will be \nfound in Chapter 4. \nAs we go through this section, watch for the Syllabus terms functional testing. \nnon-functional testing, test type and white-box testing. You will find these terms \ndefined in the Glossary as well. \nTest types are introduced as a means of clearly defining the objective of a certain \ntest level for a program or project. We need to think about different types of testing \nbecause testing the functionality of the component or system may not be sufficient at \neach level 1o meet the overall test objectives. Focusing the testing on a specific test \nobjective and, therefore, selecting the appropriate type of test, helps make it easier to \nmake and communicate decisions about test objectives, Typical objectives may include: \no Evaluating functional quality, for example whether a function or feature is com- \nplete, correct and appropriate. \no Evaluating non-functional quality characteristics, for example reliability, perfor- \nmance efficiency, security, compatibility and usability. \no Evaluating whether the structure or architecture of the component or system is \ncorrect, complete and as specified. \no Evaluating the effects of changes, looking at both the changes themselves (for \nexample defect fixes) and also the remaining system to check for any unintended \nside-effects of the change. These are confirmation testing and regression testing, \nrespectively, and are discussed in Section 2.3.4. \nG N Crmpoge Lossming. A3 Kighs Bt vt My et b o, s, o4 B . 0 s 10 . o, U 3 i, e, s sty st ey b gy B e el smbs AChagiti s e e dmd ety oo st s & cvent barwny Conpage Liwmay muries e 141 10 ey akbmcnd o o 141 Vo € whacgaes IS (70T s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 75,
            "page_label": "76"
        }
    },
    {
        "page_content": "Section 3 Test Types 63 \nA test type is focused on a particular test objective, which could be the testing Test type A group of \nof a function to be performed by the component or system: a non-functional quality  yace actwities based on \ncharacteristic, such as reliability or usability: the structure or architecture of the  spacific test objectives \ncomponent or system; or related to changes, that is, confirming that defects have  aimed at specific \nbeen fixed (confirmation testing. or re-testing) and looking for unintended changes  characterstics of a \n(regression testing). Depending on its objectives, testing will be organized differently.  component or system. \nFor example, component testing aimed at performance would be quite different from \ncomponent testing aimed at achieving decision coverage. \n2.3.1 Functional testing \nThe function of a system (or component) is what it does. This is typically \ndescribed in work products such as business requirements specifications, func- \ntional specifications, use cases, epics or user stories. There may be some func- \ntions that are assumed 1o be provided that are not documented, They are also \npart of the requirements for a system, though it is difficult to test against undocu- \nmented and implicit requirements. Functional tests are based on these functions, \ndescribed in documents or understood by the testers, and may be performed at \nall test levels (for example tests for components may be based on a component \nspecification). \nFunctional testing considers the specified behaviour and is often also referred  Functional testing \n1o as black-box testing (specification-based testing). This is not entirely true, since  Testing conducted to \nblack-box testing also includes non-functional testing (see Section 2.3.2). evaluate the compliance \nFunctional testing can also be done focusing on suitability, interoperability testing. of a component o \nsecurity, accuracy and compliance. Security testing, for example, investigates the \nfunctions (for example a firewall) relating to detection of threats, such as viruses, s \nfrom malicious outsiders, \n“Testing of functionality could be done from different perspectives, the two main \nones being requirements-based or business-process-based. \nRequirements-based testing uses a specification of the functional requirements \nfor the system as the basis for designing tests. A good way to start is to use the table \nof contents of the requirements specification as an initial test inventory or list of \nitems 1o test (or not to test). We should also prioritize the requirements based on risk \ncriteria (if this is not already done in the specification) and use this to prioritize the \ntests. This will ensure that the most important and most critical tests are included in \nthe testing effort, \nBusiness-process-based testing uses knowledge of the business processes. Busi- \nness processes describe the scenarios involved in the day-to-day business use of the \nsystem. For example, a personnel and payroll system may have a business process \nalong the lines of: someone joins the company, he or she is paid on a regular basis, and \nhe or she finally leaves the company. User scenanios originate from object-oriented \ndevelopment but are nowadays popular in many development life cycles, They also \ntake the business processes as a starting point, although they start from tasks to be \nperformed by users, Use cases are a very useful basis for test cases from a business \nperspective. \nThe techniques used for functional testing are often specification-based, but \nexperience-based techniques can also be used (see Chapter 4 for more on test \ntechniques), Test conditions and test cases are derived from the functionality of the \nG N Crmpogs Lowming. A3 Kighs Bt vt My et b o, s, o4 A, 0 s 18 . o, U 3 ottt g, s sty comto ey b gy B e el snbs At s v e e e oo s et P Conpage Linmay muries e 1y 10 ey koo o o 1} wow € whacacs YA (70T ns g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 76,
            "page_label": "77"
        }
    },
    {
        "page_content": "64 Chapter 2 Testing throughout the software development Ife cycle \ncomponent or system. As part of test design, a model may be developed, such as a \nprocess model. state transition model or a plain-language specification. \nThe thoroughness of functional testing can be measured by a coverage meas- \nure based on elements of the function that we can list. For example, we can list \nall of the options available from every pull-down menu. If our set of tests has \nat least one test for each option, then we have 100% coverage of these menu \noptions. Of course, that does not mean that the system or component is 100% \ntested, but it does mean that we have at least touched every one of the things \nwe identified. When we have traceability between our tests and functional \nrequirements, we can identify which requirements we have not yet covered, \nthat is, have not yet tested (coverage gaps). For example, if we covered only \n90% of the menu options, we could add tests so that the untested 10% are then \ncovered, \nSpecial skills or knowledge may be needed for functional testing, particularly for \nspecialized application domains. For example, medical device software may need \nmedical knowledge both for the design and testing of such systems. The worst thing \nthat a heart pacemaker can do is not to stop giving the electrical stimulant to the heart \n(the heart may still limp along less efficiently). The worst thing is to speed up, giving \nthe signal much too frequently: this can be fatal. Other specialized application arcas \ninclude gaming or interactive entertainment systems, geological modelling for oil \nand gas exploration, or automotive systems. \n2.3.2 Non-functional testing \nThis test type is the testing of the quality characteristics, or non-functional attributes \nof the system (or component or integration group). Here we are interested in how \nwell or how fast something is done. We are testing something that we need 1o meas- \nure on a scale of measurement. for example time to respond. \nNon-functional Non-functional testing, as functional testing, is performed at all test levels, \ntesting Testing Non-functional testing includes, but is not limited to, performance testing. load test- \nconducted to evaluate ing, stress testing, usability testing, maintainability testing. reliability testing. port- \nthe compliance of a ability testing and security testing. It is the testing of how well the system works. \nMany have tried to capture software quality in a collection of characteristics and \nMm‘mn-fu\\cﬁoml related sub-characteristics. In these models, some elementary characteristics keep \nfoquremeTis: on reappearing, although their place in the hicrarchy can differ. The International \nOrganization for Standardization (1SO) has defined a set of quality charactenstics \nin ISE/IEC 25010 [2011). \nA common misconception is that non-functional testing occurs only during \nhigher levels of testing such as system test, system integration test and acceptance \ntest. In fact, non-functional testing may be performed at all test levels: the higher \nthe level of risk associated with each type of non-functional testing, the earlier \nin the life cycle it should occur. Ideally, non-functional testing involves tests that \nquantifiably measure characteristics of the systems and software. For example, in \nperformance testing we can measure transaction throughput, resource utilization \nand response times, Generally, non-functional testing defines expected results in \nterms of the external behaviour of the software. This means that we typically use \nblack-box test techniques. For example, we could use boundary value analysis to \nGt N Compoge Lowming. A3 K Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 3 st e, ot sty contos ey b gy B e el smbs AChagit s e e b ety oo s et T Conprge Lonmay muries e 1y 10 ey kbmcnd o o 1} o € whbacacm A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 77,
            "page_label": "78"
        }
    },
    {
        "page_content": "Section 3 Test Types 65 \ndefine the stress conditions for performance tests, and equivalence partitioning to \nidentify types of devices for compatibility testing, or 1o identify user groups for \nusability testing (novice, experienced, age range, geographical location, educational \nbackground). \nThe thoroughness of non-functional testing can be measured by the coverage of \nnon-functional elements. If we had at least one test for each major group of users, then \nwe would have 100% coverage of those user groups that we had identified. Of course, \nwe may have forgotten an important user group, such as those with disabilities, so we \nhave only covered the groups we have identified. \nIf we have traceability between non-functional tests and non-functional require- \nments, we may be able to identify coverage gaps. For example, an implicit require- \nment is for accessibility for disabled users. \nSpecial skills or knowledge may be needed for non-functional testing, such as for \nperformance testing, usability testing or security testing (for example for specific \ndevelopment languages). \nMore about non-functional testing is found in other ISTQB qualification Sylla- \nbuses, including the Advanced Test Analyst, the Advanced Technical Test Analyst, \nand the Advanced Security Tester, the Foundation Performance Testing, and the \nFoundation Usability Testing Syllabus. \n2.3.3 White-box testing \nThe third test type looks at the internal structure or implementation of the system \nor component. If we are talking about the structure of a system, we may call it \nthe system architecture. Structural elements also include the code itself, control \nflows, business processes and data flows. White-box testing is also referred 1o White-box testing \nas structural testing or glass-box because we are interested in what is happening  (clear-box testing, \ninside the box. code-based testing, \nWhite-box testing is most often used as a way of measuring the thoroughness of ~ 91ss-box testing, logic- \ntesting through the coverage of a set of structural clements or coverage items. ltcan ~ COVErag¢ testing, logic- \noccur at any test Jevel, although it is true to say that it tends to be mostly applied at \ncomponent testing and component integration testing, and generally is less likely at testing) Testing based \nhigher test levels, except for business process testing. At component integration fevel o0 ‘analysss of the \nit may be based on the architecture of the system, such as a calling hierarchy or the  jrarna) structure-based \ninterfaces between components (the interfaces themselves can be listed as coverage  of the component or \nitems), The test basis for system, system integration or acceptance testing could bea  system. \nbusiness model, for example business rules. \nAt component level, and to a lesser extent at component integration testing, there \nis good tool support to measure code coverage. Coverage measurement tools assess \nthe percentage of executable elements (for example statements or decision outcomes) \nthat have been exercised (that is, they have been covered) by a test suite. If coverage \nis not 100%, then additional tests may need to be written and run to cover those parts \nthat have not yet been exercised, This of course depends on the exit criteria. (Coverage \nand white-box test techniques are covered in Chapter 4.) \nSpecial skills or knowledge may be needed for white-box testing. such as knowl- \nedge of the code (to interpret coverage tool results) or how data is stored (for database \nqueries). \not S04 Crmpoge Loy N3 K Bt sl My et b o s o4 A 0 s 18 ot (b 3 s, . ot oty st ey b smpqresmnd B e ol sndis Ak 11 rview e e s JR—p—— T Conguge Lismng mares e (i 1 vy adibacnd commm o ey o € shocgaes 30 (6T Sts g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 78,
            "page_label": "79"
        }
    },
    {
        "page_content": "66 Chapter 2 Testing throughout the software development Ife cycle \nConfirmation testing \n(re-testing) Dynamic \ntesting conducted after \nfixing defects with the \nabjective to confirm \nthat fadures caused by \nthose defects do not \noccur anymore. \nRegression testing \nTesting of a previously \ntested component \nor system following \nmodification to ensure \nthat defects have not \nbeen introduced or \nhave been uncovered in \nunchanged areas of the \nsoftware as a result of \nthe changes made. \n2.3.4 Change-related testing \nThe final test type is the testing of changes. This category is slightly different to the \nothers because if you have made a change to the software, you will have changed the \nway it functions, how well it functions (or both) and its structure. However, we are \nlooking here at the specific types of tests relating to changes, even though they may \ninclude all of the other test types. There are two things to be particularly aware of \nwhen changes are made: the change itself and any other effects of the change. \nConfirmation testing (re-testing) \nWhen a test fails and we determine that the cause of the failure is a software defect, \nthe defect is reported and we can expect a new version of the software that has had \nthe defect fixed. In this case we will need to execute the test again to confirm that the \ndefect has indeed been fixed. This is known as confirmation testing (also known \nas re-testing). \nWhen doing confirmation testing, it is important to ensure that steps leading up to \nthe failure are carried out in exactly the same way as described in the defect report, \nusing the same inputs, data and environment, and possibly extending beyond the \ntest 1o ensure that the change has indeed fixed all of the problems due to the defect. \nIf the test now passes. does this mean that the software is now correct? Well, we \nnow know that at least one part of the software is correct ~ where the defect was, \nBut this is not enough. The fix may have introduced or uncovered a different defect \nelsewhere in the software. The way to detect these unexpected side-effects of fixes \nis to do regression testing. \nRegression testing \nLike confirmation testing, regression testing involves executing test cases that have \nbeen executed before. The difference is that, for regression testing, the test cases \nprobably passed the last time they were executed (compare this with the test cases \nexecuted in confirmation testing — they failed the last time). \nThe term regression testing is something of a misnomer. It would be better if it \nwere called anti-regression testing because we are executing tests with the intent \nof checking that the system has not regressed (that is, it does not now have more \ndefects in it as a result of some change). More specifically, the purpose of regression \ntesting is to make sure (as far as is practical) that modifications in the software or \nthe environment have not caused unintended adverse side effects and that the system \nstill meets its requirements. \nIt is common for organizations to have what is usually called a regression test \nsuite or regression test pack. This is a set of test cases that is specifically used for \nregression testing. They are designed to collectively exercise most functions (certainly \nthe most important ones) in a system, but not test any one in detail. It is appropriate \n10 have a regression test suite at every level of testing (component testing, integration \ntesting, system testing, etc.). In some cases, all of the test cases in a regression test \nsuite would be executed every time a new version of software is produced: this makes \nthem ideal candidates for automation. However, it is much better to be able to select \nsubsets for execution, especially if the regression test suite is very large. In Agile \ndevelopment, a selection of regression tests would be run to meet the objectives of a \nparticular iteration. Automation of regression tests should start as early as possible \nin the project. See Chapter 6 for more on test automation. \nG N Compoge Lowming. A3 K Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 3 i e, s sty comto ey b gy B e el b At \nJe—p—— Conpae e e b sy s 8 cven oy s e (e 4 e ol o o o € g 4 (F s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 79,
            "page_label": "80"
        }
    },
    {
        "page_content": "Section 3 Test Types 67 \nRegression tests are executed whenever the software changes, either as a result \nof fixes or new or changed functionality. It is also a good idea to execute them when \nsome aspect of the environment changes, for example when a new version of the host \noperating system is introduced or the production environment has a new version of \nthe Java Virtual Machine or anti-malware software. \nMaintenance of a regression test suite should be carried out so it evolves over time \nin line with the software. As new functionality is added to a system, new regression \ntests should be added. As old functionality is changed or removed, so too should \nregression tests be changed or removed. As new tests are added, a regression test \nsuite may become very large. If all the tests have to be executed manually it may not \nbe possible to execute them all every time the regression suite is used. In this case, a \nsubset of the test cases has to be chosen. This selection should be made considering \nthe latest changes that have been made 1o the software. Sometimes a regression test \nsuite of automated tests can become so large that it is not always possible to execute \nthem all, It may be possible and desirable to eliminate some test cases from a large \nregression test suite, for example if they are repetitive (tests which exercise the same \nconditions) or can be combined (if they are always run together). Another approach \nis to eliminate test cases when the risk associated with that test is so low that it is not \nworth running it anymore. \nBoth confirmation testing and regression testing are done at all test levels. \nIn iterative and incremental development, changes are more frequent, even con- \ntinuous and the software is refactored frequently. This makes confirmation testing \nand regression testing even more important. But iterative development such as Agile \nshould also include continuous testing, and this testing is mainly regression testing. \nFor 16T systems, change-related testing covers not only software systems but the \nchanges made to individual objects or devices. which may be frequently updated or \nreplaced. \n2.3.5 Test types and test levels \n‘We mentioned as we went through the test types that each test type is applicable at \nevery test level. The testing is different, depending on the test level and test type, of \ncourse, but the Syllabus gives examples of cach test type at each test level to illustrate \nthe point. \nFunctional tests at each test level \nLet's use a banking example to look at the different levels of testing. There are many \nfeatures in a financial application. Some of them are visible to users and others are \nbehind the scenes but equally important for the whole application to work well. \nThe more technical and more detailed aspects should be tested at the lower levels, \nand the customer-facing aspects at higher levels. We will also look at examples of \nthe different test types showing the different testing for functional, non-functional, \nwhite-box and change-related testing. \no Component testing: how the component should calculate compound interest. \no Component integration testing: how account information from the user interface \nis passed to the business logic. \n® System testing: how account holders can apply for a line of credit. \nG N Compuge Lowming. A3 K Bt vt My et b o, s, o4 e s, 0 s 10 . o, U 3 st g, s e paty comto ey b gy B e el smbs At Je—p—— e e b et P Conpage Lonmay murves e 1y 10 ey akbacnd o o 0} wow € hacacs YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 80,
            "page_label": "81"
        }
    },
    {
        "page_content": "68 Chapter 2 Testing throughout the software development Ife cycle \n® System integration testing: how the system uses an external microservice 1o \ncheck an account holder’s credit score. \n® Acceptance testing: how the banker handles approving or declining a credit \napplication. \nNon-functional tests at each test level \no Component testing: the time or number of CPU cycles to perform a complex \ninterest calculation. \no Component integration testing: checking for buffer overflow (a security flaw) \nfrom data passed from the user interface to the business logic. \n® System testing: portability tests on whether the presentation layer works on sup- \nported browsers and mobile devices, \n® System integration testing: reliability tests to evaluate robustness if the credit \nscore microservice does not respond. \n® Acceptance testing: usability tests for accessibility of the banker's credit pro- \ncessing interface for people with disabilities. \nWhite-box tests at each test level \no Component testing: achieve 100% statement and decision coverage for all com- \nponents performing financial calculations. \no Component integration testing: coverage of how each screen in the browser \ninterface passes data to the next screen and to the business logic, \n® System testing: coverage of sequences of web pages that can occur during a \ncredit line application, \n® System integration testing: coverage of all possible inquiry types sent to the \ncredit score microservice. \n® Acceptance testing: coverage of all supported financial data file structures and \nvalue ranges for bank-to-bank transfers. \nChange-related tests at each test level \no Component testing: automated regression tests for each component are included \nin the continuous integration framework and pipeline. \ne Component integration testing: confirmation tests for interface-related defects \nare activated as the fixes are checked into the code repository. \n@ System testing: all tests for a given workflow are re-executed if any screen changes. \n® System integration testing: as part of continuous deployment of the credit scor- \ning microservice, automated tests of the interactions of the application with the \nmicroservice are re-executed. \n® Acceptance testing: all previously failed tests are re-exccuted after defects \nfound in acceptance testing are fixed. \nNote that not every test type will occur at every test level in every system! However, \nit is a good idea to think about how every test type might apply at cach test level, \nand try to implement those tests at the earliest opportunity within the development \nlife cycle. \no N Crmpoge Lossming. A3 Kt Bt vt My et b o, s, o b, b 10 . o, U 3 st g, s sty comto ey b gy B e el smbs At Je——— e e b s s & cvent barwny Conpuge Lonmay marses e 1y 10 ey akbacnd o o 10y o € hacgacs A (7 0T3 s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 81,
            "page_label": "82"
        }
    },
    {
        "page_content": "Section 4 Maintenance Testing 69 \n2.4 MAINTENANCE TESTING \nSYLLABUS LEARNING OBJECTIVES FOR 2.4 MAINTENANCE \nTESTING (K2) \nFL-24.1 Summarize triggers for maintenance testing (K2) \nFL-24.2  Describe the role of impact analysis in maintenance testing (K2) \nOnce deployed. a system is often in service for years or even decades. During this \ntime, the system and its operational environment are often corrected, changed or \nextended. As we gothrough this section, watch for the Syllabus terms impact analysis \nand maintenance testing. You will find these terms also defined in the Glossary. \nTesting that is executed during this life cycle phase is called maintenance testing.  Maintenance testing \nMaintenance testing, along with the entire process of maintenance releases, should  Testing the changes to \nbe carefully planned. Not oaly must planned maintenance releases be considered, but 8N operational system \nthe process for developing and testing hot fixes must be as well. Maintenance testing ~ 0f the impact of a \nincludes any type of testing of changes to an existing. operational system, whether changed environment \nthe changes result from modifications, migration or retirement of the software \nor system. \nModifications can result from planned enhancement changes such as those \nreferred to as minor releases, that include new features and accumulated (non- \nemergency) bug fixes. Modifications can also result from corrective and more urgent \nemergency changes. Modifications can also involve changes of environment, such \nas planned operating system or database upgrades, planned upgrade of COTS soft- \nware, or patches to correct newly exposed or discovered vulnerabilities of the \noperating system. \nMigration involves moving from one platform to another. This can involve \nabandoning a platform no longer supported or adding a new supported platform. \nEither way, testing must include operational tests of the new environment as well \nas of the changed software. Migration testing can also include conversion test- \ning, where data from another application will be migrated into the system being \nmaintained. \nNote that maintenance testing is different from testing for maintainability (which \nis the degree to which & component or system can be modified by the intended main- \ntainers). In this section, we'll discuss maintenance testing. \nThe same test process steps will apply as for testing during development and, \ndepending on the size and risk of the changes made, several levels of testing are car- \nried out: a component test, an integration test, a system test and an acceptance test. \nIf testing is done more formally, an application for a change may be used to produce \na test plan for testing the change, with test cases changed or created as needed. In \nless formal testing. thought needs to be given 10 how the change should be tested, \neven if this planning, updating of test cases and execution of the tests is part of a \nCONLINUOUS Process, \nThe scope of maintenance testing depends on several factors, which influence the \ntest types and test levels. The factors are: \n® Degree of risk of the change, for example a self-contained change is a lower nisk \nthan a change to a part of the system that communicates with other systems. \nGt N Crmpoge Lowming. A3 Kighs Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 3 st e, s sty comtr ey b gy B e el smbs At e e dmd sy oo s et s 8 cven earwny Conpuge Linmay muries e 1y 10wy kbacnd o o 01w € whacgacm A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 82,
            "page_label": "83"
        }
    },
    {
        "page_content": "70 Chapter 2 Testing throughout the software development Ife cycle \n® The size of the existing system, for example a small system would need less \nregression testing than a larger system. \n® The size of the change, which affects the amount of testing of the changes that \nwould be needed. The amount of regression testing is more related to the size of \nthe system than the size of the change. \n2.4.1 Triggers for maintenance \nAs stated, maintenance testing is done on an existing operational system. There are \nthree possible triggers for maintenance testing: \no modifications \n® migration \n® retirement, \nMoedifications include planned enhancement changes (for example release-based), \ncorrective and emergency changes and changes of environment, such as planned \noperating system or database upgrades, or patches to newly exposed or discovered \nvulnerabilities of the operating system and upgrades of COTS software. \nMadifications may also be of hardware or devices, not just software components \nor systems. For example, in loT systems, new or significantly modified hardware \ndevices may be introduced 10 a working system. The emphasis in maintenance test- \ning would likely focus on different types of integration testing and security testing \nat all test levels, \nMaintenance testing for migration (for example from one platform to another) \nshould also include operational testing of the new environment, as well as the changed \nsoftware, It is important to know that the platform you will be transferring 1o is sound \nbefore you start migrating your own files and applications. \nMaintenance testing for the retirement of a system may include the testing of data \nmigration or archiving, if long data-retention periods are required. Testing of restore \nor retrieve procedures after archiving may also be needed. There is no point in try- \ning to save and preserve something that you can no longer access. These procedures \nshould be regularly tested and action taken to migrate away from technology that is \nreaching the end of its life. You may remember seeing magnetic tape on old movies, \nwhich was thought to be a good long-term archiving solution at the time. \n2.4.2 Impact analysis and regression testing \nAs mentioned earlier, maintenance testing usually consists of two parts: \no testing the changes \n® regression tests to show that the rest of the system has not been affected by the \nImpact analysis The maintenance work, \nidentification of al In addition to testing what has been changed, maintenance testing includes exten- \nMpmm sive regression testing to parts of the system that have not been changed. Some \nW‘m;;‘::‘“ﬂ systems will have extensive regression suites (automated or not) where the costs \n- ded 1o of executing all of the tests would be significant. A major and important activity \naccomplish the change. within maintenance testing is impact analysis. During impact analysis, together with \nstakeholders, a decision is made on what parts of the system may be unintentionally \nG N Compoge Lossming. A3 K Bt vt My et b o, s, o4 b, s 10 . o, U 3 st g, s sty comto ey b gy B e el smbs At sl el e A e gyt oot s e Sy S G el g pericnce (engage L BTG e (W 14 Y ol o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 83,
            "page_label": "84"
        }
    },
    {
        "page_content": "Section 4 Maintenance Testing 71 \naffected and therefore need more extensive regression testing. Risk analysis will help \nto decide where to focus regression testing. It is unlikely that the team will have time \n1o repeat all the existing tests, so this gives us the best value for the time and effort \nwe can spend in regression testing. \n1f the test specifications from the onginal development of the system are kept, one \nmay be able to reuse them for regression testing and to adapt them for changes to \nthe system. This may be as simple as changing the expected results for your existing \ntests, Sometimes additional tests may need to be built. Extension or enhancement to \nthe system may mean new areas have been specified and tests would be drawn up just \nas for the development. Do not forget that automated regression tests will also need \nto be updated in line with the changes: this can take significant effort, depending on \nthe architecture of your automation (see Chapter 6). \nImpact analysis can also be used 1o help make a decision about whether or not a \nparticular change should be made. If the change has the potential to cause high-risk \nvulnerabilities throughout the system, it may be a better decision not to make that \nchange. \nThere are a number of factors that make impact analysis more difficult: \n® Specifications are out of date or missing (for example business requirements, \nuser stories, architecture diagrams). \n® Test cases are not documented or are out of date. \n® Bi-directional traceability between tests and the test basis has not been \nmaintained. \n® ‘Tool support is weak or non-existent, \n® The people involved do not have domain andfor system knowledge, \n® The maintainability of the software has not been taken into enough considera- \ntion during development. \nImpact analysis can be very useful in making maintenance testing more efficient, \nbut if it is not, or cannot be, done well, then the risks of making the change are greatly \nincreased. \no S Compogs Lo N3 B Bt vl Moy et b o, . o4 Bl 0 s 18 . o, L 3 i, . ot sty contsss ey b smpppeneed B e ol snbis g s bl i M e s gyt oo s et Sy R B el W pericncs (engag L BTG e (W 14 Y adibicd o o s o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 84,
            "page_label": "85"
        }
    },
    {
        "page_content": "72 Chapter 2 Testing throughout the software development lée cycle \nCHAPTER REVIEW \nLet’s review what you have learned in this chapler. \nFrom Section 2.1, you should now understand the relationship between devel- \nopment activities and test activities within a development life cycke and be familiar \nwith sequential life cycle models (waterfall and V-model) and iterative/incremental \nlife cycle models (RUP, Scrum, Kanban and Spiral). You should be able to recall the \nreasons for different levels of testing and characteristics of good testing in any life \ncycle model. You should be able 1o give reasons why software development life cycle \nmodels need o be adapted 10 the context of the project and product being developed. \nYou shoukd know the Glossary terms commercial off-the-shelf (COTS), sequential \ndevelopment model and test level. \nFrom Section 2.2, you should know the typical levels of testing (component, \nintegration, system and acceptance testing). You should be able to compare the differ- \nent levels of testing with respect 10 their major objectives, the test basis, typical objects \nof testing, typical defects and failures, and approaches and responsibilities for each \ntest level. You should know the Glossary terms acceptance testing, alpha testing, \nbeta testing, component integration testing, component testing, contractual \nacceptance testing, integration testing. operational acceptance testing, regulatory \nacceptance testing, system integration testing. system testing. test basis, test case, \ntest environment, test object, test objective and user acceptance testing. \nFrom Section 2.3, you should know the four major types of test (functional, \nnon-functional, structural and change-related) and should be able 1 provide some \nconcrete examples for cach of these. You should understand that functional and \nstructural tests ocour at any test level and be able 1o explain how they are applied in \nthe various test levels. You should be able to identify and describe non-functional \ntest types based on pon-functional requirements and product quality characteristics. \nFinally, you should be able to explain the purpose of confirmation testing (re-testing) \nand regression testing in the context of change-related testing. You should know the \nGlossary terms functional testing, non-functional testing, test type and white-box \ntesting. \nFrom Section 2.4, you should be able 1o compare maintenance lesting to testing of \nnew applications. You should be able to identify triggers and reasons for maintenance \ntesting, such as modifications, migration and retirement. Finally, you should be able 1o \ndescribe the role of regression testing and impact analysis within maintenance testing. \nYou should know the Glossary terms impact analysis and maintenance testing, \ne 20 Copy L 48 St U ot . i Gt 0 10 et vt e B ot s AP e et e gt wpay [ESIPe - o Lramamg st e gl s bbb et o 7 b § bt S4i b gt &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 85,
            "page_label": "86"
        }
    },
    {
        "page_content": "[P ay pa \nSAMPLE EXAM QUESTIONS \nQuestion 1 Which of the following statements is \ntrue? \na. Overlapping test levels and test activities are more \ncommon in sequential life cycle models than in \niterative incremental models, \nb. The V-model is an iterative incremental life cycle \nmaodel because each development activity has a \ncorresponding lest activity, \n¢. When completed, iterative incremental \nlife cycle models are more likely to \ndeliver the full set of features originally \nenvisioned by stakeholders than sequential \nmaodels, \nd. In iterative and incremental life cycle \nmodels, delivery of usable software to \nend-users is much more frequent than in \nsequential models. \nQuestion 2 What level of testing is typically \nperformed by system administration staff”? \na. Regulatory acceptance testing. \nb. System testing. \n¢ System integration testing. \nd. Operational acceptance testing. \nQuestion 3 Which of the following is i test type? \na. Component testing. \nb. Functional testing. \nc. System testing. \nd. Acceptance testing. \nQuestion 4 Consider the three triggers for \nmaintenance, and match the event with the correct \ntrigger: \n1. Data conversion from one system to another. \n2. Upgrade of COTS software. \n3. Test of data archiving. \n4. System now runs on a different platform and \noperaling system. \n5. Testing restore o retrieve procedures. \n6, Patches for security vulnerabilities. \nSamgle Exam Questions 73 \na. Modification: 2 and 3, Migration: | and 5, \nRetirement: 4 and 6. \nb. Modification: 2 and 6, Migration: | and 4, \nRetirement: 3 and 5. \n¢, Modification: 2 and 4, Migration: | and 3, \nRetirement: 5 and 6. \nd. Modification: 1 and 5, Migration: 2 and 3, \nRetirement: 4 and 6. \nQuestion 5 Which of these is a functional test”? \na. Measuring response time on an online booking \nsystem, \nb. Checking the effect of high volumes of traffic in a \ncall centre system, \n¢. Checking the online bookings screen information \nand the database contents against the information \non the letter to the customers. \nd. Checking how easy the system is 1o use, particularly \nfor users with disabilities such as impaired vision. \nQuestion 6 Which of the following is true, \nregarding the process of testing emergency fixes” \na. There is no time to test the change before it goes. \nlive. so only the best developers should do this \nwork and shoukd pot involve testers as they slow \ndown the process. \nb. Just run the retest of the defect actually fixed. \n€. Always run a full regression test of the whole \nsystem in case other parts of the system have been \nadversely affected. \nd. Retest the changed area and then use risk \nassessment to decide on a reasonable subset of the \nwhole regression test to run in case other parts of \nthe system have been adversely affected. \nQuestion 7 A regression test: \na. Is only run once. \nb, Will always be awtomated. \n©. Will check unchanged areas of the software to see \nif they have been affected. \nd. Will check changed areas of the software 1o see if \nthey have been affected. \ne 328 Cogs Lo 4 St U ot . i Gt 0 10 et e et i e G s e AP e et e ot Iy p o Lramtng et gl b s bbb e o 7 S § bt o4l b gt &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 86,
            "page_label": "87"
        }
    },
    {
        "page_content": "74 Chapter 2 Testing throughout the software development Ife cycle \nQuestion 8 Non-functional testing includes: Question 9 Beta testing is: \na. Testing to see where the system does not function a. Performed by customers at their \ncorrectly. own site. \nb. Testing the quality attributes of the system b. Performed by customers at the software \nincluding reliability and usability. developer's site. \n¢. Gaining user approval for the system. <. Performed by an independent test team. \nd. Testing a system feature using only the software d. Useful to test software developed for a specific \nrequired for that function. CUSIOmEr O USEr. \not S04 Crmpoge Loy A3 R Rt vl My et o o s o4 Bl 0 b 18 o (b 3 i, e s ety ontrs ey b spppeeed b e ol b gt 11 el s M v syl oot s e Sy e el g eTiencs (g Lewing BTG e (W 4 ¢ ol o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 87,
            "page_label": "88"
        }
    },
    {
        "page_content": "CHAPTER THREE \nStatic techniques \nsmic test technigues provide a powerful way to improve the quality and productivity \nlof software development. This chapter describes static test techniques, including \nreviews, and provides an overview of how they are conducted. The fundamental \nobjective of static testing is to improve the quality of software work products \nby assisting engineers to recognize and fix their own defects early in the software \ndevelopment process. While static testing techniques will not solve all the problems, \nthey are enormously effective. Static techniques can improve both quality and \nproductivity by impressive factors. Static testing is not magic and it should not be \nconsidered a replacement for dynamic testing, but all software organizations should \nconsider using reviews in all major aspects of their work, including requirements, \ndesign, implementation, testing and maintenance. Static analysis tools implement \nautomated checks, for example on code, \n3.1 STATIC TECHNIQUES AND THE TEST \nPROCESS \nSYLLABUS LEARNING OBJECTIVES FOR 3.1 § \nTECHNIQUES AND THE TEST PROCESS (K2) \nIc \nFL-3.1.1 Recognize types of software work product that can be \nexamined by the different static testing techniques (K1) \nFL-3.1.2  Use examples to describe the value of static testing (K2) \nFL-3.1.3 Explain the difference between static and dynamic \ntechniques, considering objectives, types of defects to be \nidentified, and the role of these techniques within the software \nlife cycle (K2) \nIn this section, we consider how static testing techniques fit into the overall test \nprocess. Dynamic testing requires that we run the item or system under test. but \nstatic testing techniques allow us to find defects directly in work products, without \nthe execution of the code and without the need to isolate the failure to locate the \nunderlying defect. Static techniques include both reviews and static analysis, each of \nwhich we'll discuss in this chapter. Static techniques are efficient ways to find and \nremove defects, and can find certain defects that are hard to find with dynamic test- \ning. As we go through this section, watch for the Syllabus terms dynamic testing, \n75 \noy N Crmpogs Lowming. NS Rt Roerrnd Moy ot b copi, wamnd, o0 o, o s 10 e, U 3 vt g, acmms ety commvs sy b sppoesed B e Bk snbhe oot Eibrorsl e A devmed 0 ) spprevacd o st puarmaly afect S cenl Barmag gy Conpge Liwmmag BuTSes (40 14 s akiomed o 101 o € bt YA (T s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 88,
            "page_label": "89"
        }
    },
    {
        "page_content": "76 Chapter 3 Static techniques \nstatic analysis and static testing. You will find these terms also defined in the \nGlossary. \nIn Chapter 1, we saw that testing is defined as all life cycle activities, both static \nand dynamic, to do with planning. preparing and evaluating software and related work \nproducts. As indicated in that definition, two approaches can be used to achieve these \nobjectives, static testing and dynamic testing. Static analysis is a form of automated \nstatic testing. \nThe definitions of static analysis and static testing are very similar, and to be \nhonest, are somewhat confusing! The definition of static analysis would apply equally \nwell to reviews, which are a form of static testing but are not part of static analysis, \nGenerally, static analysis involves the use of tools to do the analysis; in fact, the \nSyllabus describes them as “tool-driven evaluation” of code or work products, The \nkey difference is: considering the code we want 10 evaluate, dynamic testing actually \nexecutes that code; static testing (including static analysis) does NOT execute the \ncode we are evaluating, \nOne area where static analysis is often used (and is critical for) is in safety-critical \nsystems such as flight control software, medical devices or nuclear power control \nsoftware. However static analysis is also very important in security testing, as it can \nidentify malicious code (which does not make itself visible in execution). In continu- \nous delivery and continuous deployment, the automated build systems also frequently \nmake use of static analysis as part of the build process. \n3.1.1 Work products that can be examined by static testing \nStatic analysis is most often used to evaluate code against various criteria, such as \nadherence to coding standards, thresholds for complexity, spelling and grammar \ncorrectness and reading difficulty (the last few for work products other than code), \nHowever, static testing is broader than automated evaluations: reviews can be applied \n10 any type of work product, including: \n® Any type of specification: business requirements, functional requirements, secu- \nrity requirements. \no Epics, user stories and acceptance criteria. \no Code. \no Testware, that is, any type of work product to do with testing, for example test \nplans, test conditions, test cases, test procedures and automated test scripts, \no User guides, help text, wizards and other things designed to help the user to \nmore effectively use the system. \n® Web pages (there are also static analysis tools to analyze whether any links are \nbroken for example). \no Coatracts (a particularly important work product 1o review, as a lot of money \nmay be riding on the specific wording), project plans, schedules and budgets, \n® Models such as activity diagrams or other models used in model-based \ntesting (MBT), \n(Note that there is an ISTQB Foundation Level Model-Based Tester Extension \nSyllabus.) \nReviews apply to any work product; the reviewers need to be able to read and under- \nstand it, but can then provide feedback about it. See Section 3.2 for more on reviews. \nG N Compuge Lowming. A3 K Bt vt My et b o, s, o4 e s, 0 s 10 . o, U 3 st g, s e paty comto ey b gy B e el smbs At \nJe—p—— Conpaet e e b et s 8 even oy s e (e 4 e sl o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 89,
            "page_label": "90"
        }
    },
    {
        "page_content": "Section 1 Static Techniques and the Test Process 77 \nStatic analysis of software code is done using 2 tool 10 analyze the code with \nrespect to the criteria of interest. For example, static analysis tools can identify dead \ncode, a section of code that can never be reached from anywhere else in the code, \nThis dead code can never be executed. so should be removed, as it could be confusing \n1o leave it in. Static analysis can also identify a variable whose value is used before \nit has been defined. This type of defect can cause failures which are difficult to find \nby dynamic testing. \nThere are also tools that can analyze natural language text, for example in require- \nments, to catch some typos, assess readability level (ease of understandability), etc., \ns0 these are also a form of static analysis. \n3.1.2 Benefits of static testing \nStudies have shown that as a result of reviews, a significant increase in productivity \nand product quality can be achieved Gilb and Graham [1993] and van Veenendaal \n[1999]. Reducing the number of defects early in the product life cycle also means \nthat less time would be spent on testing and maintenance. The use of static testing, \nsuch as reviews on software work products, has two major advantages: \n® Since static testing can start carly in the life cycle, early feedback on quality \nissues can be established, for example an carly validation of user requirements \nrather than late in the life cycle during acceptance testing. Feedback during \ndesign review or backlog refinement is more useful than after a feature has been \nbuilt, \n® By detecting defects at an early stage, rework costs are most often relatively \nlow, and thus relatively cheap improvements to the quality of software \nproducts can be achieved, as many of the follow-on costs of late updates are \navoided, for example additional regression tests, confirmation tests, etc. \nAdditional benefits of static testing may include: \n® Defects are more efficiently detected and corrected, particularly since this is \ndone before dynamic test execution, \n® Defects that are not easily found by dynamic testing, such as security vulner- \nabilities, are identified. \n® Defects in future design and code are prevented by uncovering inconsisten- \ncies, ambiguities, contradictions, omissions, inaccuracies and redundancies in \nrequirements, \n® Since rework effort is substantially reduced, development productivity figures \nare likely to increase. \n® Reduced development cost and time. \n® Reduced testing cost and time. If defects are found and fixed before test \nexecution starts, there are fewer to find in testing, so more tests pass, and there \nare fewer defect reports to write and fewer confirmation tests 1o run after fixes, \nThis saves both time and money. \n® Reduced total cost of quality over the software’s lifetime. If defects are found \nand fixed early, then there should be fewer that get through to later testing or \noperation. The defects that are not there do not need to be investigated or fixed, \nsaving time and money. \no N Crmpoge Lossming. A3 K Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i, g, st s sty comtos ey b sy B e el smbs At el e A v gyt oot s e sy S G el g eTicncs CEngage L BTG e (W 14 Y adiBmcnd o o S € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 90,
            "page_label": "91"
        }
    },
    {
        "page_content": "78 Chapter 3 Static techniques \no Improved communication within the team, since there is an exchange of \ninformation between the participants during reviews, which can lead to an \nincreased awareness of quality issues, \nIn conclusion, static testing is a very suitable method for improving the quality of \nsoftware work products. This applies primarily to the assessed work products them- \nselves. It is also important that the quality improvement is not achieved just once, \nbut has a more permanent nature. The feedback from the static testing process to the \ndevelopment process allows for process improvement, which supports the avoidance \nof similar errors being made in the future. This is particularly useful for sprint or \nproject retrospectives. \n3.1.3 Differences between static and dynamic testing \nStatic and dynamic testing have the same objectives: 1o assess the quality of work \nproducts and identify defects as early as possible. But static and dynamic testing are \nnot the same. They find different types of defect, so they are complementary and are \nbest used together. It does not make sense to ask if one is better than the other; both \nare useful and needed. \nWith dynamic testing methods, software is executed using a set of input values \nand its output is then examined and compared to what is expected. During static \ntesting, software work products are examined manually, or with a set of tools, but not \nexecuted. Dynamic testing can be started early by identifying test conditions and test \ncases as carly as possible in the life cycle (as we discussed in Chapter 1 Section 1.4), \nbut dynamic test execution can only be applied to software code. Dynamic execution \nis applied as a technique to detect defects and to determine quality attributes of the \ncode. This dynamic testing option is not applicable for the majority of the software \nwork products. Among the questions that arise are: \no How can we evaluate or analyze a work product, such as a requirement \nspecification, a user story, a design document, a test plan or a user manual? \n® How can we effectively examine the source code before execution? \nAs discussed above, one powerful technique that can be used is static testing, for \nexample reviews. In principle, all human-readable software work products can be \ntested using review techniques. \nTypes of defects that are easier to find during static testing include: \n® Requirements defects, such as inconsistencies, ambiguities, contradictions. \nomissions, inaccuracies, redundancies. \n® Design defects, such as inefficient algorithms or database structures, high \ncoupling or low cohesion. \no Coding defects, such as variables with undefined values, variables that are \ndeclared but never used, unreachable code, duplicate code. All of these can be \nfound by static analysis tools. \n® Deviations from standards, for example lack of adherence 1o coding standards, \n@ Incorrect interface specifications, such as different units of measurement used \nby the calling system than by the called system. In 1999, a Mars Orbiter burned \nup in the atmosphere due to one team using metric units of measurement and \nthe other using imperial units of measurement, as described in Nasa [1999]. \nGt N Crmpoge Lowming. A3 Kigh Bt vt My et b o, s, o4 A, 0 s 10 . o, (b 3 st e, s s paty comtr ey b gy B e el b At s el e A e gyt oot s et Sy o el ey pericncs Cengge L BTG e (W 14 Y ol o o e o € g 430 (s s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 91,
            "page_label": "92"
        }
    },
    {
        "page_content": "Section 2 Rewview Process 79 \n® Security vulnerabilities, for example buffer overflow susceptibility. \n@ Traceability problems, such as gaps or inaccuracies or lack of coverage (for example \nmissing tests for an acceplance criterion). \n® Maintainability defects, such as improper modularization, poor reusability, code \nthat is difficult to analyze and modify (often referred to as code smells). These \ndefects do not show up in dynamic testing but can be critical for long-term costs \nof the system. \nCompared to dynamic testing, static testing finds defects rather than failures. \nYou may recall that in Chapter | Section 1.2.3 we made a distinction between errors, \ndefects and failures. An error is a mistake made by a human being (for example in \nwriting code). a defect is something that is wrong (for example in the code itself) and \na failure is when the system or component does not perform as it should (for example \nreturns the wrong balance). Static testing does not cause the system or component to \ndo anything, so it cannot find failures; only dynamic testing can do that, However, \nstatic testing does find defects directly. Dynamic testing has to investigate the failure \nto find a defect. \nIn addition to finding defects, the objectives of reviews are often also informa- \ntional, communicational and educational. Participants learn about the content of \nsoftware work products to help them understand the role of their own work and 1o \nplan for future stages of development. Reviews often represent project milestones \nand support the establishment of a baseline for a software product. The type and \nquantity of defects found during reviews can also help testers focus their testing \nand select effective classes of tests. In some cases, customers/users or product \nowners attend the review meeting and provide feedback 1o the development team, \n50 reviews are also a means of customer/user communication. \n3.2 REVIEW PROCESS \nSYLLAB \nPROCESS \nLEARNING OBJECTIVES FOR 3.2 REVIEW \n) \nFL-3.2.1 Summarize the activities of the work product review \nprocess (K2) \nFL-3.2.2 Recognize the different roles and responsibilities in a formal \nreview (K1) \nFL-3.2.3  Explain the differences between different review types: \nFL-3.24  Apply a review technique to a work product to find \ndefects (K3) \nFL-3.2.5 Explain the factors that contribute to a successful \nreview (K2) \nG N Compoge Lossming. A3 g Bt vt My et b o, s, o4 B s, 0 s 18 . o, U 3 s, e, s sty comto ey b gy B e el smbs At e T Je——— s & cven sy Conpuge Lowmag marses e 1 10 ey akbmcnd o o 10} S € shacgacm A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 92,
            "page_label": "93"
        }
    },
    {
        "page_content": "80 Chapter 3 Statc techniques \nIn this section, we will focus on reviews as a distinct ~ and distinctly useful ~ form \nof static testing. We'll discuss the process for carrying out reviews. We'll talk about \nwho does what in a review meeting and as part of the review process. We'll cover \ntypes of reviews that you can use. We'll look at different variations for reviews based \non different ways to prepare for and perform reviews, and finally we will look at \nsuccess factors to enable the most effective and efficient reviews possible. As we \no through this section, watch for the Syllabus terms ad hoc reviewing, checklist- \nbased reviewing, formal review, informal review, inspection, perspective-based \nreading. review, role-based reviewing. scenario-based reviewing. technical \nreview and walkthrough. You will find these terms also defined in the Glossary. \nOne reason why reviews are so useful is that having a different person look at \na work product is a way to overcome cognitive bias, the tendency to see what we \nintended rather than what we actually wrote. \nReviews vary from very informal to formal (that is, well-structured and regulated). \nAlthough inspection is perhaps the most documented and formal review technique, it is \ncertainly not the only one. The formality of a review process is related by factors such as \nthe maturity of the development process, any legal or regulatory requirements or the need \nfor an audit trail. In practice, the informal review is perhaps the most commeon type of \nreview, Informal reviews are applied at various times during the carly stages in the life \ncyele of o work product. A two-person team can conduct an informal review, as the author \ncan ask a colleague to review a work product or code. Pair working (pair programming. \npair testing or a tester and devedoper pairing) is also an informal way to review the work \nproducts that both are working on. In later stages, reviews often involve more people and \nameeting. This normally involves peers of the author, who try to find defects in the work \nproduct under review and discuss these defects ina review meeting. The goal is 1o help the \nauthor and to improve the quality of the work product. Informal reviews come in various \nshapes and forms, but all have one characteristic in common: they are nol documented. \nFormal reviews generally have team participation, documented results of the \nreview and specified procedures to follow in carrying out the review, \nDifferent reviews may have a different focus or objective for the review. For example, \none objective may be to find defects. This is often at keast one of the objectives of most \ntypes of review, formal or informal. Sometimes the objective is for all participants to \ngain knowledge and understanding: although a walkthrough is normally used for this \npurpose, an informal review could also meet this goal, and it is often a by-product (if not \nan explicit objective) for technical reviews or inspections. Another objective may be to \nhold discussions and come to a consensus about technical issues; this is normally the \nfocus of a technical review. \nThe standard that covers review processes is ISO/IEC 20246 [2017). \n3.2.1 Work product review process \nIn contrast to informal reviews, formal reviews follow a formal process. Informal \nreviews may perform at least some of the same activities 1o some extent. The review \nprocess consists of the following main activities: \nPlanning \nThe Foundation Syllabus specifies the following elements of the planning activity: \n® Defining the scope of the review: the purpose of the review, what work products \n(for example documents) or parts of work products to review and the quality \ncharacteristics to be evaluated in the review. \nG N Crmpogs Lowming. A3 Kt Bt vt My et b o, s, o4 e, 0 s 18 . o, (b 3 i, g, s sty cmto ey b gy B e el b At el e M e gyt oot s o Sy o el g pericnce Cengage Lewing BTG e (W 14 Y adlbmnd o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 93,
            "page_label": "94"
        }
    },
    {
        "page_content": "Section 2 Review Process 81 \no Estimating effort and the timeframe for the review. \n® ldentifying review characteristics such as the type of review with roles, \nactivities and checklists. \n® Selecting the people to participate in the review and allocating roles to each \nreviewer. \n@ Defining the entry and exit criteria for more formal review types (for example \ninspections). \n@ Checking that entry criteria are met before the review starts (for more formal \nreview types). \nLet’s examine these in more detail, \nThe review process for a particular review may begin with a request for review \nby the author to the review leader, who takes overall responsibility for the review, \nfor example scheduling (dates, time, place and invitation) of the review. On a project \nlevel, the project planning needs to allow time for review and rework activities, thus \nproviding engineers with time to thoroughly participate in reviews, \nFor more formal reviews, for example inspections, the facilitator performs an entry \ncheck and defines at this stage formal exit criteria, The entry check is carried out to \nensure that the reviewers™ time is not wasted on a work product that is not ready for \nreview. A work product containing too many obvious mistakes is cleardy not ready \nto enter a formal review process, and it could even be very harmful to the review \nprocess. It would possibly de-motivate both reviewers and the author. Also, the review \nis likely to be less effective because the numerous obvious and minor defects will \nconceal the major defects. \nThe following are possible entry criteria for a formal review: \n® A shont check of a work product sample by the review leader (or expert) does \nnot reveal many major defects. \n® The work product to be reviewed is available with line numbers (if relevant). \n® The work product has been cleaned up by running any applicable automated \nchecks, such as static analysis or spelling and grammar assessments. \n® References needed for the review are stable and available. \n® The work product author is prepared 10 join the review team and feels \nconfident with the quality of the product. \nIf the work product passes the entry check, the review leader and author decide \nwhich part(s) of it to review. Because the human mind can comprehend a limited set \nof pages at one time, the number should not be too high. The maximum number of \npages depends, among other things, on the objective, review type and work product \ntype. It should be derived from practical experiences within the organization. For a \nreview, the maximum size is usually between 10 and 20 pages. In formal inspection, \nonly a page or two may be looked at in depth in order 1o find the most serious defects \nthat are not obvious. \nAfter the work product size has been set and the pages to be checked have been \nselected, the review leader determines, in co-operation with the author, the com- \nposition of the review team. The team normally consists of four to six partici- \npants, including facilitator (moderator) and author. To improve the effectiveness of \nthe review, different roles may be assigned to cach of the participants. These roles \nhelp the reviewers focus on particular types of defects during individual review. \no N Crmpoge Lossming. A3 K Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i, g, st s sty comtos ey b sy B e el smbs At el e A v gyt oot s e sy S G el g eTicncs CEngage L BTG e (W 14 Y adiBmcnd o o S € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 94,
            "page_label": "95"
        }
    },
    {
        "page_content": "82 Chapter 3 Statc techniques \nThis reduces the chance of different reviewers finding the same defects. The review leader \nor facilitator assigns the roles to the reviewers. (See below on role-based reviewing.) \nQuality characteristics may also be evaluated and documented in a review, for \nexample, the testability of a design or the readability or understandability of user \nhelp or installation instructions. These may also be assigned roles. \nInitiate review \nThe Foundation Syllabus specifics the following activities for initiating a review: \no Distributing the work products (physically or electronically) and any other \nrelevant material such as logging forms, checklists or related work products. \no Explaining the scope, objectives, process, roles and work products to the \nparticipants. \n® Answering any questions that participants may have about the review. \nLet’s examine these in more detail. \nThe goal of this set of activities is to get everybody on the same wavelength regard- \ning the work product under review and to commit to the time that will be spent on \nchecking (that is, individual reviewing). The result of the entry check and defined exit \ncriteria are discussed in case of a more formal review, This stage of the review process \nis important to increase the motivation of reviewers and thus the effectiveness of the \nreview process. At customer sites, we have measured results of up to 70% more major \ndefects found per page as a result of performing a kick-off meeting, a form of review \ninitiation, as described in van Veenendaal and van der Zwan [2001]. \nThe review initiation may be done remotely, or it may involve a meeting (in person \nor using video conferencing). If a meeting is held, the reviewers receive a short \nintroduction to the objectives of the review and the work products. The relationships \nbetween the work product under review and the other work products (for example \nsources or predecessor work products) are explained, especially if the number of \nrelated work products is high, \nRole assignments, checking rate. the pages to be reviewed, the roles to be taken by \neach person, process changes and possible other questions are also discussed during \nthis meeting. \nWhether or not a meeting is held, the facilitator (moderator) ensures that cach \nreviewer is clear about their responsibilities, and answers any questions that they may \nhave, either about the work products, or the review process itself. \nIndividual review (that is, individual preparation) \nThe Foundation Syllabus specifies the following activities for the individual review: \n® Reviewing all or part of the work documents(s). \no Noting potential defects, recommendations and questions. \nLet us examine these in more detail, \nIn the individual review, the participants work alone on the work product under \nreview using the related work products, procedures, rules and cheeklists provided. The \nindividual participants identify defects, recommendations, questions and comments, \naccording to their understanding of the work product and the particular role they \nhave been given. All issues are recorded, preferably using a logging form. Spelling \nmistakes are recorded on the work product under review, but not mentioned during \nthe meeting. The annotated work product may be given to the author at the end of the \nG N Crmpoge Lowming. A3 Kighs Bt vt My et b o, s, o4 e, 0 s 18 . o, (b 3 st e, s sty comto ey b gy B e el b At el i M e s gyt oot s o Sy S el g pericncs CEngag Lewing BTN e (W 14 Y adlbicnd o o e o € g 430 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 95,
            "page_label": "96"
        }
    },
    {
        "page_content": "Section 2 Review Process 83 \nlogging meeting. Using checklists during individual reviewing can make reviews more \neffective and efficient, for example a specific checklist based on perspectives such as \nuser, maintainer, tester or operations, or a checklist for typical coding problems. This \nmay also be an assigned role. \nA critical success factor for a thorough preparation is the number of pages \nindividually reviewed per hour. This is called the checking rate. The optimum checking \nrate is the result of a mix of factors, including the type of work product, its complexity, \nthe number of related work products and the experience of the reviewer. Usually the \nchecking rate is in the range of five to ten pages per hour, but may be much less for \nformal inspection, for example one page per hour. During preparation, participants \nshould not exceed the checking rate they have been asked to use. By collecting data \nand measuring the review process, company-specific criteria for checking rate and \nwork product size (see Planning) can be set, preferably specific to a work product type. \nIssue communication and analysis \nThe Foundation Syllabus specifies the following activities for issue communication \nand analysis: \no Communicating identified potential defects, for example in a review meeting. \n® Analyzing potential defects, assigning ownership and status 1o them, \no Evaluating and documenting quality characteristics. \n@ Evaluating the review findings against the exit criteria to make a review decision \n(reject; major changes needed: accept, possibly with minor changes). \nLet’s examine these in more detail. \nIn a formal review, the things found by the individual reviewers are communicated \n10 the author of the work product (or the person who will fix defects), and may also \nbe communicated to the other reviewers. We may refer to these as issues at this point \nbecause we do not yet know if they are defects or not. The issues may be communi- \ncated electronically, or in a review meeting, which may consist of the following activ- \nities (partly depending on the review type): logging, discussion and decision making. \nLogging in a review meeting \nDuring logging. the issues, that is, potential defects, that have been identified dur- \ning the individual review are mentioned page by page, reviewer by reviewer, and are \nlogged either by the author or by a scribe. A separate person 1o do the logging (a scribe) \nis especially useful for formal review types such as an inspection. To ensure progress \nand efficiency, no real discussion is allowed during logging. If an issue needs discus- \nsion, the item is noted as a discussion item and then handled in the discussion part of \nthe meeting. A detailed discussion on whether or not an issue is a defect is not very \nmeaningful, as it is much more efficient to simply log it and proceed to the next one. \nFurthermore, in spite of the opinion of the team, a discussed and discarded defect may \nwell turn out to be a real one during rework. \nEvery defect and its severity should be logged. The participant who identifies the \ndefect may propose the severity, or the facilitator may assign a severity. If reviewers \nassign severity, it is important that the meaning of each category is understood by all \nreviewers in the same way. Otherwise, one reviewer may regard everything as critical, \nfor example, skewing the review results. Severity classes could be: \n® Critical: defects will cause downstream damage: the scope and impact of the \ndefect is beyond the work product under inspection. \nGt N Crmpogs Loswming. A3 K Bt vt My et b o, s, o4 e . 0 s 18 . o, U 3 i e, s sty conto ey b gy B e el b AChagat s v e e e oo s et P Conpuge Lonmay muries e 1y 10 ey kbacnd o o s} Vo € whacacm YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 96,
            "page_label": "97"
        }
    },
    {
        "page_content": "84 Chapter 3 Statc techniques \n® Major: defects could cause a downstream effect (for example a fault in a design \ncan result in an error in the implementation). \n® Minor: defects are not likely to cause downstream damage (for example \nnon-compliance with the standards and templates). \nIn order to keep the added value of reviews, spelling errors are not part of the \ndefect classification. Spelling defects are noted by the participants in the work product \nunder review and given to the author at the end of the meeting, or could be dealt with \nin a separate proofreading exercise. \nDuring logging. the focus is on logging as many defects as possible within a certain \ntimeframe. To ensure this, the facilitator tries to keep a good logging rate (number of \ndefects logged per minute). In a well-led and disciplined formal review meeting, the \nlogging rate should be between one and two defects logged per minute. \nDiscussion part of a review meeting \nFor a more formal review, the issues classified as discussion items will be handled \nduring a discussion part of the review meeting, which occurs after the logging has been \ncompleted. Less formal reviews will often not have separate logging and discussion \nparts and will start immediately with logging mixed with discussion (which may lead \n10 fewer defects being found). Participants can take part in the discussion by bringing \nforward their comments and reasoning. In the discussion part of the meeting. the \nfacilitator or moderator takes care of people issues. For example, they prevent discus- \nsions from getting too personal, rephrase remarks if necessary and call for a break to \ncool down heated discussions and/or participants. \nReviewers who do not need to be in the discussion may leave, or stay as a learning \nexercise. The facilitator also paces this part of the meeting and ensures that all \ndiscussed items cither have an outcome by the end of the meeting or are noted as \nan action point if the issue cannot be solved during the meeting. The outcome of \ndiscussions is documented for future reference. \nQuality charactenistics may also be evaluated and documented at this point, for \nexample, the testability of a design or the readability or understandability of user help \nor installation instructions. \nDecisionmaking part of a review meeting \nAt the end of the meeting, a decision on the work product under review has o \nbe made by the participants, sometimes based on formal exit criteria. The most \nimportant exit criterion may be the average number of critical and/or major defects \nfound per page (for example no more than three critical/major defects per page), If \nthe number of defects found per page exceeds a certain level, the work product may \nneed to be reviewed again, after it has been reworked. If the work product complies \nwith the exit criteria, it will be checked later by the review leader, facilitator or one \nor more participants. Subsequently, the work product can leave the review process. \nIn addition to the number of defects per page, other exit criteria are used that \nmeasure the thoroughness of the review process, such as ensuring that all pages have \nbeen checked at the right rate. The average number of defects per page is only a valid \nquality indicator if these process criteria are met. \n1f a project is under pressure, the review leader or facilitator will sometimes be \nforced to skip re-reviews and exit with a defect-prone work product. Setting (and \nagreeing to) quantified exit criteria helps the review leader or facilitator to make firm \ndecisions at all times. Even if a limited sample of a work product has been (formally) \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 A i, 0 s 18 . o, U 3 ot e, s sty comto ey b gy B e el b At v e e e oo s et P Conpuge Lonmay muries e 1y 10 ey koo o o 0} o € whacacm YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 97,
            "page_label": "98"
        }
    },
    {
        "page_content": "Section 2 Review Process 85 \nreviewed, an estimate of remaining defects per page can give an indication of likely \nproblems later on. \nFor informal reviews, some of these activities may be performed, but informally. \nFor example, potential defects may be emailed to the author of the work product with \no suggested severity classification. The author may then evaluate exit criteria, possibly \nchecking back with the reviewer(s). \nFixing and reporting \nThe Foundation Syllabus specifies the following activities for fixing and reporting: \no Creating defect reports for those findings that require changes. \n® Fixing defects found (typically done by the author) in the work product \nreviewed. \no Communicating defects to the appropriate person or team (when found in a \nwork product related to the work product reviewed). \n® Recording updated status of defects (in formal reviews), potentially including \nthe agreement of the comment originator. \no Gathering metrics (for more formal review types), for example of defects fixed, \ndeferred, etc. \n® Checking that exit criteria are met (for more formal review types). \n® Accepting the work product when the exit criteria are reached. \nLet’s examine these in more detail. \nDefect reports may have been recorded in a general defect logging tool (for example \nalso used by testers) or may have been recorded in a review log. \nDuring fixing. the author will improve the work product under review step by step, \nbased on the issues or defects detected by the individual reviewers and/or those found \nin the review meeting. Not every issue that is reported is a defect that leads to a fix. \nIt is often the author's responsibility to judge if an issue really is a defect which has \nto be fixed, though in some cases the review meeting participants may make those \ndecisions. If nothing is done about an issue for a certain reason, it should still be \nreported to show that the author has considered it. \nChanges that are made to the work product should be easy to identify by anyone \nwho will be confirming that the fixes are correct. For example, the author may turn \non ‘track changes’ in a document. \nSometimes an issue raised affects a work product that is not under the direct con- \ntrol of the author of the reviewed work product. For example, reviewing a feature 1o \nbe implemented may reveal an inconsistency or omission in the user story or require- \nments specification that it is based on. If the author cannot update the user story or \nrequirement specification directly, they need to communicate this to whoever can \nupdate the related work product. \nAs defects are fixed, the status of those defects should be updated wherever they \nare managed. In some reviews, the originator of the comment or defect would then \nconfirm that the defect has been fixed adequately, that the author has correctly under- \nstood their comment and fixed the right problem in the right way. \nIn order to control and optimize the review process, a number of measurements \nare collected by the facilitator at each step of the process. Examples of such measure- \nments include number of defects found, number of defects found per page, time spent \nchecking per page, total review effort, ete. It is the responsibility of the facilitator \nor moderator to ensure that the information is correct and stored for future analysis. \not S04 Crmpoge Loy N3 K Bt vl My et b o s o4 A 4 b 18 ot (b 3 s, . ot kot st ey b smpqpesmed B e ol snbis gt 11 v e e e Je—p—— e Congage Lisming mares e (i 1 vy adibmcnd o o sy o € shocgaes (430 (0 Sms s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 98,
            "page_label": "99"
        }
    },
    {
        "page_content": "86 Chapter 3 Statc techniques \nThe facilitator or moderator Ís responsible for ensuring that satisfactory actions \nhave been taken on all logged defects, process improvement suggestions and change \nrequests. Although the facilitator or moderator checks to make sure that the author \nhas taken action on all known defects, it is not necessary for them to check all the \ncorrections ín detail. If it is decided that all participants will check the updated work \nproduct, the facilitator or moderator takes care of the distribution and collects the \nfeedback. For more formal review types the review leader or facilitator checks for \ncompliance to the exit criteria, \nWhen all of the exit criteria have been met, including fixing. checking of fixes \nand confirming that the review process was properly carried out, then the work prod- \nuct can be formally accepted. This may be particularly important in safety-critical \nsystems. \n3.2.2 Roles and responsibilities in a formal review \nThe participants ín any type of formal review should have adequate knowledge of \nthe review process. The best, and most efficient, review situation occurs when the \nparticipants gain some kind of advantage for their own work during reviewing, In \nthe case of an inspection or technical review, participants should have been properly \ntrained, as both types Of review have proven 1o be far less successful without trained \nparticipants. This indeed 1s a critical success faCtOr. \nThe best formal reviews come from well-organized teams, guided by trained facil- \nitators (moderators). Within a review team, six types of roles can be distinguished: \nauthor. management, facilitator (or moderator), review leader, reviewers and scribe \n(or recorder), \nThe author \nThe author has two main responsibilities: \n® Creating the work product under review. \n® Fixing defects ín the work product (if necessary). \nThe author's basic goal should be to learn as much as possible with regard to \nimproving the quality of the work product, but also to improve his or her ability to \nwrite future work products. The author's task is to illuminate unclear areas and to \nunderstand the defects found. \nManagement \nManagement has a number of very important responsibilities in successful reviews, \nincluding: \n® Ensuring that reviews are planned. \n® Deciding on the execution of reviews. \n® Assigning staff, budget and time. \n® Monitoring ongoing cost effectiveness, \n® Executing control decisions in the event of inadequate outcomes. \nThe role of management in reviews is often underestimated, but without adequate \nsupport from managers, reviews are seldom successful. The manager needs to believe \nin reviews, and 1o ensure that they will be carried out as part of development. This \no N Compoge Lossming. A3 Kt Bt vt My et b o, s, o b s, 0 s 10 . o, U 3 ot g, st sty comto rế b sy B y el smbs At el s s e gyt oot s o sy S el g pericnce Cengage Ly x e (W 14 Y kh ee o o o € g 3 e , g n",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 99,
            "page_label": "100"
        }
    },
    {
        "page_content": "Section 2 Review Process 87 \nincludes allowing time in schedules for reviews to be done (and also any resulting \nrework!). Managers should also keep an eye on the effectiveness of reviews and \nencourage increasing effectiveness and efficiency, so that the greatest benefits are \ngained from reviews. The manager should have clear objectives for the review pro- \ncessfes) and should determine whether those objectives have been met. The manager \nwill ensure that any review training requested by the participants takes place. Of \ncourse, 4 manager can also be involved in the review itself, depending on his or her \nbackground, playing the role of a review leader or reviewer if this would be helpful. \nIn some review types, the manager should not be the moderator or facilitator of the \nreview meeting (for example inspection), \nFacilitator (often called moderator) \nThe responsibilities of the facilitator or moderator are: \n® Ensuring the effective running of review meetings (when held). \n® Mediating, if necessary, between the various points of view, \n® Being the person upon whom the success of the review often depends. \n“The facilitator or moderator leads each individual review process. The facilitator \nperforms the entry check and checks on the fixes. in order to control the quality of \nthe input and output of the review process. The moderator also schedules the meet- \ning. disseminates work products and other relevant materials, organizes the mecting, \ncoaches other team members, paces the meeting, leads possible discussions and stores \nthe data that is collected. \nReview leader \nThe responsibilities of the review leader are: \n® Taking overall responsibility for the revicw. \n® Deciding who will be involved. \nDepending on the size of the organization, the role of the review leader may be \ntaken by a manager, or by the facilitator (moderator). lf review leader is a separate \nrole, they will be working closely with both of these roles. \nIn some organizations, there is no distinction made between the review leader and \nthe facilitator in practice. The main difference is that the review leader is responsible \nfor the review happening, and organizes the people involved, but may not be involved \nin the review meeting (if this is how issues are communicated). The facilitators \nmain role is in dealing with the people while the review is happening, ensuring that \nthe review meetings are run well, and making sure that interpersonal issues do not \ndisrupt the review process. \nReviewers \nThe responsibilities of the reviewers are: \n® Being subject matter experts, persons working on the project, stakeholders with \nan interest in the work product, and/or individuals with specific technical or \nbusiness backgrounds, \n® Identifying potential defects in the work product under review. \n® Representing different perspectives as requested, for example tester, \ndeveloper, user, operator, business analyst, usability expert, etc, \nG N4 Compoge Lossming. A3 Kin Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i, g, s sty comto rế b sy B n el smbs At el s A e gyt oot s o Sy 4 G el g perience Cengage Ly x e (W 14 s n kh ee o o o € g nghn e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 100,
            "page_label": "101"
        }
    },
    {
        "page_content": "88 Chapter 3 Statc techniques \nThe role of the reviewers (also called checkers or inspectors) is to be another \nset Of eyes to look at the work products from a fresh point of view. The reviewers \ncheck any material for defects, mostly prior to the meeting. The level of thorough- \nness required depends on the type of review. The level of domain knowledge or \ntechnical expertise needed by the reviewers also depends on the type of review. \nReviewers should be chosen to represent different perspectives in the review pro- \ncess. In addition to the work product under review, reviewers also receive other \nmaterial, including source work products, standards, checklists, ete. In general, \nthe fewer source and reference work products provided, the more domain expertise \nis neeeded regarding the coment of the work product under review. \nScribe (or recorder) \nThe responsibilities of the scribe or recorder are: \n® Collating potential defects found during the individual review activity. \n® Recording new potential defects, open points and decisions from the review \nmeeting (when held), \nDuring the logging meeting, the scribe (or recorder) has to record each defect men- \ntioned and any suggestions for process improvement. In practice, itis often the author \nwho plays the role of scribe, ensuring that the log is readable and understandable. If \nauthors record their own defects. or at least make their own notes in their own words, \nit helps them to understand the log better when they look through it afterwards to \nfix the defects found. However, having someone other than the author take the role \nof the scribe (for example the facilitator) can have significant advantages, since the \nauthor is freed up to think about the work product rather than being tied down with \nlots of writing. \n“The roke of the scribe or recorder may be less relevant, or even irrelevant, if the defects, \ndiscussion points, decisions and other issues raised in the review are recorded electron- \nically, although the scribe may be the person doing that recording during a meeting. \nAlthough we have described the roles and responsibilities as though they are all \nseparate and distinct, this does not mean that they are done by different people. One \nperson may take on more than one role and perform the responsibilities for multiple \nroles. The determination of who does what role also depends on the type of review. \n3.2.3 Types of review \nThe different review types have different objectives and can be used for different \npurposes, but the most common objective of all review types is to uncover defects \nin the work product being reviewed. The type of review should be chosen, based on \na number of factors, including the needs of the project, available resources, product \ntype and risks, business domain and company culture, \nThe main review types, their main characteristics and attributes are described \nbelow, \nInformal review (for example buddy check, pairing, pair review) \nAn informal review is characterized by the following attributes: \n® Main purpose/objective: detecting potential defects. \n® Possible additional purposes: generating new ideas or solutions, quickly solving \nminor problems. \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 e, s 18 . o, (b 3 i e, rn sty st rế b gy B y el b At el e M e gyl oot s e Sy o el g pericncs Cengage Liwing TGS e (W 14 Y adlbmnd o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 101,
            "page_label": "102"
        }
    },
    {
        "page_content": "Section 2 Review Process 89 \n® Not based on a formal (documented) review process. \n® May not involve a review meeting. \n® May be performed by a colleague of the author (buddy check) or by more \npeople. \n® Results may be documented (but often are not). \n® Varies in usefulness depending on the reviewer(s). \n® Use of checklists is optional. \n® Very commonly used in Agile đevelopment. \nAn informal review may simply be one person saying 1o a colleague, \"Could you \nhave a quick look at what I've just done?' The colleague may spend less than an \nhour looking through and giving any comments back to the author, such as typos, \nsomething missing, o a 'But have you thought of this?” comment. With the right \nperson revicwing, this buddy check can be very effective (and at little cost ín time). \nOther forms of informal review include pair working, when one person works with \nanother to produce a work product, the second person continually evaluating (that is, \nreviewing) what the first person is typing, \nA walkthrough is characterized by the following attributes: Walkthrough (Structured \no Main L b : . walkthrough) A type purposes: find defects, improve the software product, consider alternative o e n which an \nimplementations, evaluate conformance to standards and specifications. author leads members. \n® Possible additional purposes: cxchanging ideas about techniques or style of the review through a \nvariations, training of participants, achieving consensus. Work product and the \n® Individual preparation before the review meeting is optional. and make comments \n® Review meeting is typically led by the author of the work product. about possible issues. \n® Use of a scribe is mandatory. \n® Use of checklists is optional, \n® May take the form of scenarios, dry runs or simulations, \n® Potential defect logs and review reports may be produced. \n® May vary ín practice from quite informal to very formal. \nWithin a walkthrough, the author does most of the preparation. The partic- \nipamts, who are selected from different departments and backgrounds, are not \ngenerally required to do a detailed study of the work products in advance (but \nit is an option). Because of the way the meeting is structured, a large number \nof people can participate and this larger audience can bring a great number of \ndiverse viewpoints regarding the contents of the work product being reviewed. If \nthe audience represents a broad cross-section of skills and disciplines, it can give \nassurance that no major defects are missed in the walkthrough. A walkthrough is \nespecially useful for higher-level work products, sụch as requirement specifications \nand architectural documents. \nA walkthrough is often used to transfer knowledge and cducate a wider audi- \nence about a particular work product, In some cases, the educational value of a \nwalkthrough is more important than finding defects (although defects should be \nwelcomed). \nG N Crmpoge Lossming. A3 K Bt vt My et b o, s, o4 b . 0 s 10 . o, U 3 s, e, s sty comto rế b gy B n el smbs At bl e i e gyt oo s et Sy 4 el g pericncs Cengage 12 BTG e (W 14 WY kh ee o o o € g ngh e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 102,
            "page_label": "103"
        }
    },
    {
        "page_content": "90 Chapter 3 Statc techniques \nTf a large number of people are present, a seribe (someone other than the author) \nmay be used to record the discussion, the questions raised and any decisions taken \nat the meeting. \nTechnical review \nTechnical review A technical review is characterized by the following attributes: \na team of technically- s \nqualified personnel that e \nMain purposes: gaining consensus, detecting potential defects. \nPossible further purposes: evaluating quality and building confidence \nin the work product, generating new ideas, motivating and enabling \nauthors to improve future work products, considering alternative \nimplememations. \nReviewers should be technical peers of the author, and technical experts in \nrelevant disciplines. \nIndividual preparation before the review meeting is required. \nReview meeting is optional, ideally led by a trained facilitator (typically not the \nauthor). \nScribe is mandatory, ideally not the author. \nUse of checklists ís optional. \nPotential defect logs and review reports are typically produced. \nA technical review is often a discussion meeting that focuses on achieving consen- \nsus about the technical content of a work product that all the participants have studied \nbefore the meeting. During technical reviews, defects are found by experts, who focus \non the content of the work product. The experts who participate in a technical review \nmay include, for example, architects, chief designers and key users, It is useful to \nhave an independent facilitator, especially if there are a number of strong opinions \nabout technical issues. Technical reviews are typically less formal than inspections, \nbut generally more formal than walkthroughs. In practice, technical reviews may \nvary from quite informal to very formal, \nInspection A type An inspection is characterized by the following attributes: \nof formal review to ĩ ĩ . N . _ identify issues in a ® Main purposes: detecting potential defects, evaluating quality and building \nwork product, which confidence in the work product, preventing future similar defects through author \nlearning and root cause analysis. \nPossible further purposes: motivating and enabling authors to improve \nfuture work products and the software development process, achieving \nconsensus, \nA defined process is followed, with formal documented outputs, based on rules \nand checklists, \nThere are clearly defined roles, such as those specified in Section 3.2.2 which \nare mandatory and may include a dedicated reader who reads/paraphrases the \nwork product aloud during the review meeting. \nIndividual preparation before the review meeting is required. \no N Crmpoge Lossming. A3 K Bt vt Myt b o, s, o4 B . 0 s 10 . o, U 3 st g, s NH sty comtos rế b sy B n rồnnà smbs At el e A e gyt oot s e sy s el g ericncs Cengage 12 ng K e (W 14 WY kh ee o o o € g nghn e , g n",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 103,
            "page_label": "104"
        }
    },
    {
        "page_content": "Section 2 Review Process 91 \nReviewers are either peers of the author or experts in other disciplines that are \nrelevant to the work product. \nSpecified entry and exit criteria are used. \nA scribe is mandatory, \nThe review meeting is led by a trained facilitator/moderator (not the author). \nThe author cannot act as the review leader, facilitator, reader or scribe. \nPotential defect logs and review reports are produced, \nMetrics are collected and used to improve the entire software development \nProcess. including the inspection process. \nInspection is the most formal review type. The work product under inspection ís \nprepared and checked thoroughly by the reviewers before the meeting, comparing \nthe work product with its sources and other referenced work products, and using \nrules and checklists, In the inspection meeting, the defects found are logged and \nany discussion is postponed until the discussion part of the meeting. This makes the \ninspection meeting a very efficient meeting. \nDepending on the organization and the objectives of a project, inspections can be \nbalanced to serve a number of goals. For example, if the time to market is extremely \nimportant, the emphasis in inspections will be on efficiency. In a safety-critical \nmarket, the focus will be on effectiveness. \n‘When inspections are done well, they not only help to identify defects ín the work \nproducts being reviewed, but the emphasis on process improvement and learning \nleads to better ways of producing work products, both for the author and for other \nreviewers. \nAll review types \nA single work product may be the subject of more than one review. lf more than one \ntype of review is used, the order may vary, For example, an informal review may be \ncarried out before a technical review to make sure the work product is ready for the \ntechnical review, or an inspection may be carried out on a requirements specifica- \ntion or epic before a walkthrough with customers. No one of the types of review is \nthe winner, but the different types serve different purposes at different stages in the \nlife cycle of a work product. \nA peer review is where all of the reviewers are at the same or similar organiza- \ntional level as the author, that is, the author’s equals are reviewing the work, A peer \nreview is not another type of review, as all of the types of review could be carried \nout by peers, from informal review to inspection. \nAll review types have as at least one of their purposes to find defects. Section 3.1.3 \ngave some cxamples of defects that can be found by static testing, including revicws. \nThe most important thing is to find the most severe defects, those that represent the \nhighest risk to the organization. Reviewing the most important work products is one \nway 1o get greater value from reviews. For example, a decimal point error may not \nbe very important in a report about sales of low-volume items, but a decimal point \nerror in a multi-million dollar or curo contract could be very significant. The types \nof defect found depend on the work product, as well as on the reviewers, and the way \nin which the reviews are carried out, \not S04 Crmpoge Loy N3 Kighn Bt vl My et x ot s o4 B 0 b 18 ot L e 3 o, . st st rế l n\"e ee B n ol snbis At s el e i v gyl oo s ot Sy S el g pericncs Cengg L BTN e (W 14 Y adlbicd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 104,
            "page_label": "105"
        }
    },
    {
        "page_content": "92 Chapter 3 Statc techniques \n3.2.4 Applying review techniques \nSuppose you have agreed to participate in a review of some type. You have been \ngiven the work product to review, with instructions about which parts to concentrate \non, and perhaps some related documentation. How do you actually do the individual \nreviewing, the 'individual preparation” of the review process? \nThere are a number of techniques that can be used, whatever type of review is \nbeing performed. The main aim is to find defects, but some of these techniques will \nbe more effective than others in some situations. \nThe Syllabus lists five different techniques (although you could argue that the first \none is more the absence of any technique). \nAd hoc reviewing \n1f you have been given no instructions or guidance for hơw to review, then you will \nAd hoc reviewing probably be doing an ad hoc review. You will likely read the work product from the \nA review technique beginning, and you may notice a few things that you note s possible defects. lf you \ncarried out by are a very good reviewer, this may be quite effective, but if you really are not sure \n' reviewers what you are supposed 1o be looking out for, you may not find much that is useful, \ninformaly, without a If everyone uses only ad hoc reviewing, the reviewers are likely to al| find the same \nStructured process. things. which is wasteful, \nChecklist-based reviewing \nChecklist-based Checklist-based reviewing is often far more effective, because there is helpful \nreviewing A review guidance given about what 1o look for in the supplied checklist. The checklist would \ntechnique guided by be one of the things distributed when the review is initiated. Different reviewers may \nalist of questions or have different checklists. This helps to cut down the number of duplicate defects \nfequred attributes. found by reviewers. A checklist may contain items to check or questions. \nThe checklist may contain questions relating to the work product, such as \"Have \nreferences been given for claims shown?, ‘Are all poimers valid?' or 'Has the \ncustomer's viewpoint been considered?' It is a good idea to organize a checklist \ns0 that if the answer to a question ís ‘No’, then there is a potential defect. The best \nchecklist questions are often derived from something that went wrong or was missed \nÍn a previous review. lf a major defect was missed, adding a specific question t a \nchecklist to look for that type of defect is very effective, \nHowever, checklists should not just be allowed to grow and grow. It is important to \nreview the checklists regularly, and to remove questions that are no longer useful, so \nthat the checklist is focused on the most important things. The longer a checklist is, the \nless likely it is to be fully used, so it is also a good idea to limit the size of the check- \nlist to one page, and to put the most important checklist questions at the beginning. \nChecklists are useful when reviewing any type of work product, from unit or com- \nponent code to user stories or Help text, The questions would, of course, be differemt \nfor each work product, as they are targeted ot potential defects for that type of work \nproduct, For example, code-specific questions (about pointers) would be appropriate \nfor a unit code checklist, and customer-related questions would be appropriate for a \nuser story or requirements specification checklist, \nWhen checklists are used well, this is a very effective review technique; if all \nof the checklist questions are evaluated, this is a systematic search for previous or \ntypical defect types. \nG N Crmpoge Loswming. A3 K Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 3 i, g, rn ety st rế b E ee B y el smbs At e e ee ety / s e T b sS GMNNSCUN GSU GIEGSD (74T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 105,
            "page_label": "106"
        }
    },
    {
        "page_content": "Section 2 Review Process 93 \nOne last word of warning about checklist-based reviews: do not be constrained \nto check only what is on the checklist. The checklist can help to guide you to look \nfor specific things, but also be aware of other things, and take inspiration from the \nchecklist to look for related potential defects outside of the checklist. \nScenario-based reviewing and dry runs \nA scenario-based review is one where the reviewers are given a structured  Scenario-based \nperspective on hơw to work through a work product from a particular point Of view.  reviewing A review \nFor example, if use cases are used, a use case can be a scenarioto work through how  technique where the \nthe system will work from each actor’s point of view (people and other systems). review Ís guided by \nWhen you are stepping through the scenario, this is also called a 'dry run’, a term determining the abikty \nused for a rehearsal before a big event or speech. In this sense, going through the sce- \nnario is a rehearsal for the system. We are looking at the way the system is expected \nto be used, which is an important aspect of validation. \nA scenario is a different way of reviewing from using a checklist. You are looking \nat how the system will be used from a specific perspective, and this can be more \nfocused than just a list of questions or points to check, When using a scenario, we are \nacting out realistic ways of using the system and validating whether or not it will meet \nuser needs and expectations, A checklist is more likely to be focused on verifying \nindividual aspects, such as types of defect, that have occurred previously. \nAs with checklists, do not be constrained by the scenario, but use it as inspiration \nfor finding other defects outside of the scenario. With user-focused scenarios, be \nparticularly aware of missing features. \nRole-based reviewing \nRole-based reviewing is similar to scenario-based revicwing, but the viewpoints  Role-based \nare different stakeholders rather than just users. Roles can include specific end-user  reviewing A review \ntypes, such as experienced versus inexperienced, age-related (children, teenagers,  !@Chique where \nadults, seniors), or accessibility roles such as vision impaired, hearing impaired, ete.  TeViewers evaluate a \nPersonas may also be used, which are typically based on a profile of a specific \nset Of characteristics, A persona is the characterizalion of a user who represents stakeholder roles. \nthe target audience for your system. A number of different personas are used to \nrepresent a range of user profiles, needs or desires. For example, one persona may \nbe a young adult who is single and likes skiing: another may be a married elderly \nperson with significant health problems. Each persona is described in detail (job, \nwhere they live, income level, etc.). Each persona would represent a role in role- \nbased reviewing. \nFigure 3.1 shows some different roles with respect to documents or work products \nused within a review. The roles represent views of the work product under review: \n® Focus on higher-level work products, for example does the design comply to the \nrequirements (Type 1 in Figure 3,1). \n® Focus on standards, for example internal consistency, clarity, naming conven- \ntions, templates (Type 2). \n® Focus on related work products at the same level, for example interfaces \nbetween software functions (Type 3). \n® Focus on usage of the work product, for example for testability or maintain- \nability (Type 4). \nyy XDN Crmpoge Lowming. A3 Khện Bt vt My et n c- s, o4 A, s 10 . o, U 3 st g, s sty comto rê b gy n n el snbs At s e e a et n s et — ì sS GMNNSUNSUN GG GG GEGSD ID DSIPHEHE",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 106,
            "page_label": "107"
        }
    },
    {
        "page_content": "94 Chapter 3 Statc techniques \nType 1 \nhogher-level \nwork product \nwork / Type 2 product Type 3 \ncompliance +——=œ wnder — related \n10 standards el work product \ny \nType 4 \nuser of the \nwork product \nFIGURE 3.1 Basic review roles for a work product under review \nThe author may raise additional specific roles and questions to be addressed. \nThe moderator has the option to also fulfil a role. alongside the task of being the \nfacilitator. Checking the work product improves the facilitator’s ability to lead the \nmeeting. because it ensures better understanding. Furthermore, it improves the review \nefficiency because the facilitator replaces an engineer who would otherwise have to \ncheck the work product and attend the meeting. It is recommended that the facilitator \ntake the role of checking compliance to standards, since this tends to be a highly \nobjective role, with less discussion about the defects found. \nRoles can also be organizational, such as from the perspective of a system admin- \nistrator or user administrator, They can also be from testing perspectives such as \nperformance testing or security testing. \nIn inspection. viewpoint roles can also include reviewing from the perspective \nof contractual issues, manufacturing (if relevant), legal issues, design or testing, \noperations or delivery or a third-party supplicr, In addition to these viewpoint roles, \nthere can be procedural roles, such as checking all financial caleulations, starting \nfrom the back. looking for the most important things or cross-checking within or \nbetween work products. There may also be document or work product roles where \neach reviewer is given special responsibility for using one particular related work \nproduct, For cxamplc, if there are three user stories that are related to the work prod- \nuct being reviewed, and there are three reviewers, each would pay special attention \nto one of the three user stories. \nPerspective-based reading \nPerspective-based Perspective-based reading is similar to role-based reviewing, but rather than play- \nreading (Perspective- ing a specific role. the reviewer typically tries to perform the tasks on a high level \nbased reviewing) A that they would be doing with the work product under review (taking that perspec- \nmw tive), for example, as a tester making some test designs. Stakeholder perspectives \nmm include end-users, marketing, designer, tester or operations. These are similar to \nproduct from different some of the inspection roles, and in fact this technique was devised for inspections. \nViewpoints. This technique goes beyond role-based and ís therefore typically more expensive, \nbut it also finds more defects. \nG N Crmpoge Lossming. A3 Kt Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 ot g, s sty conto rế b gy B y el smbis At el e s v gyt oo s o Sy s G el ey pericncs CEngage Ly BTG e (W 14 Y kh Gg o o o € g 3 e e g n",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 107,
            "page_label": "108"
        }
    },
    {
        "page_content": "Section 2 Review Process 95 \nThe benefits of this technique (and also role-based and scenario-based reviewing) \nare that cách individual reviewer has their own specialty within the review, and will \nbe looking in more depth within their own assigned area, This makes the review \nmore effective (since there is deeper checking) but also more efficient, since there is \nless duplicalion of issues found by different reviewers. Checklists can also be used \nfor the different perspectives. \nAnother approach to perspective-based reading is for the reviewers to use the \nwork product to generate other work products from it. When doing perspective- \nbased reading on a requirements specification, a tester-perspective reviewer would \nbe generating acceptance tests, This can help to identify missing information \nneeded for the tests. \nPerspective-based reading combines aspects of all of the review techniques (except \nad học) and so can be the most effective, There is no one right way to do reviews, but \nwhen reviewers look at the work product in different ways, using checklists, scenarios, \nroles or perspectives, then the best results are gained. More information is available \nin Shull, Rus and Basili [2000]. \n3.2.5 Success factors for reviews \nImplementing (formal) reviews is not easy. There is no one way to success and there \nare numerous ways to fail. The most common reasons for failure in reviews are due \nto either organizational factors or people-related factors. We will look at aspects of \nboth of these. \nOne aspcct (which is not emphasized in the Syllabus) is the importance of having \na champion, the person who will lead the process on a project or organizational \nlevel. They need expertise, enthusiasm and a practical mindset in order to guide \nmoderators and participants. The authority of this champion should be clear to the \nentire organization. \nOrganizational success factors for reviews \nTo be successful, a review culture should be part of the mindset of the organization, \nand there are a number of aspects which can make or break the e{fectiveness and \nefficiency of reviews within an organization. \nHave clear objectives \nEach review should have clear objectives. These are defined during the planning \nstage, communicated to all participants and may be used to measure the exit criteria \nor definition of done for the review, \nPick the right review type and technique \nIn the previous subsections, we discussed a number of review types and techniques, \neạch of which has its strengths and weaknesses, and advantages and disadvantages \ninuse. You should be careful to select and use review types and techniques that will \nbest enable the achievement of the objectives of the project and the review itself. \nBe sure to consider the type, importance and risk level of the work product to be \nreviewed, and the reviewers who will participate. For example, do not try to review \neverything by inspection; fit the review to the risk associated with specific parts of \nthe work product. Some work products may only warrant an informal review and \nothers will repay using inspection. Of course, it is also of utmost importance that \nG N Crmpoge Lowming. A3 Khện Bt vt My et b o, s, o4 e, 0 s 18 . o, U 3 i e, rn sty comtr rế b gy B n el snbs /Ch v H n a n — ì S Lonmay muries e 1y 10 ey akbacnd o o 0y o € whacacs YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 108,
            "page_label": "109"
        }
    },
    {
        "page_content": "96 Chapter3 Statc techniques \nthe right people are involved. Consider the work product to be reviewed, Would a \nchecklist-based review technique or a role-based technique be the most suitable for \nidentifying defects? \nReview materials need to be kept up to date \nWe use work products that support the review process to perform a review. These \nwork products need to be up to date and of good quality in order to support the \nreviews, Review materials, such as checklists, need reviewing too! When significant \ndefects are found in work products (either in reviews or in testing), add an item to the \nrelevant checklist so that this type of work product defect can be identified carlier \nnext time. At regular intervals, check to make sure that all of the items or questions \non a checklist are still relevant, and remove any that are not useful any more. \nLimit the scope of the review \nLarge documents or work products should be reviewed in small chunks, so that \nfeedback can be given to the authors while they are still working on the work prod- \nuct. Early and frequent feedback is more effective and more efficient, partly because \nif a particular type of defect is detected early, the author can be aware of this ín the \nrest of the work product; this prevents that type of defect from appearing in the parts \nof the work product written later, \nIt takes time \nReviews take time, and this time needs to be scheduled. But it is not just the time \nfor review meetings. It is more important to allow adequate time for reviewers to \ndo the preparation and individual studying of the work product before any meet- \ning, as this is when most defects are identified. Skimping on preparation time is a \nfalse economy. The review meetings must be scheduled with adequate notice, so that \nreviewers can plan the review time into their own work scbedules, It is not realistic \n1o expect reviewers to be able to drop everything for a review, they need to balance \ntheir own work with the review work. \nManagement support is critical \nManagement support is essential for success, Managers should, among other things, \nincorporate adequate time fog review activities in project schedules. They should \nvisibly support the review process and help to foster a culture where reviews are scen \nas valuable both to the organization and to the individual. Managers especially must \ncommit not to use metrics or other information from reviews for the evaluation of \nthe author or the participants, If people are blamed for making mistakes, they do not \nmake fewer mistakes, they hide them better! \nOne of the authors was called in to give training for reviews in an organization \nwhere the manager claimed to be very much in favour of using them. This sounded \ngood, but during the training. it became clear that something was not right; there was \ngreat resistance 1o the idea of reviews, When raised with the manager, be said, ‘But \nI am really supporting reviews here — that way I can find out who puts in the most \ndefects and fire them,' Needless to say, reviews did not work in this organization! \nTo ensure that reviews become part of the day-to-day activities, the hours to be \nspent should be made visible within cach project plan. The engineers involved should \nbe prompted to schedule time for preparation and, very importantly, rework. Tracking \nthese hours will improve planning of the next review. As stated earlier, management \nplays an important part in planning of review activities, \nG N Crmpoge Loswming. A3 K Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 30 i, g, s sty comto ey b gy B n el smbis /Ch v eview e e e n s et — ì SOIeHDẵRSNUGMNNUUN GG o € whacacs YA (7T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 109,
            "page_label": "110"
        }
    },
    {
        "page_content": "Section 2 Review Process 97 \nReport quantified and aggregated results and benefits at a high (not individual) \nlevel to all those involved as soon as possible, including discussion of the conse- \nquences of defects if they had not been found this carly, Costs should of course be \ntracked, but benefits, especially when problems do not occur in the future, should \nbe made visible by quantifying the benefits as well as the costs. \nPeople-related success factors for reviews \nReviews are about evaluating someone’s work product. Some reviewers tend to get \ntoo personal when they are not well managed by the facilitator (moderator). People \nissues and psychological aspects should be dealt with by the facilitalor and should \nbe part of the review training, thus making the review a positive experience for the \nauthor. During the review, defects should be welcomed. This ís much more likely \nwhen they are expressed objectively. It is important that all participants create and \noperate in an atmosphere of trust. \nPick the right reviewers \nThe people chosen to participate in a review have a significant impact on the suc- \ncess and oulcome of the review, Different review types or techniques may require \ndifferent skills from the reviewers, and different types of work product may require \ndifferent skills, For example, if code is being reviewed, the reviewers must at least \nbe able to read and understand the code, so developer skills are needed. If role-based \nreview techniques are used, people who have or can adopt those particular perspec- \ntives are needed as reviewers. If someone will be using a particular work product in \ntheir own work (for example a developer who will be working to implement a fea- \nture in a user story), that person would be a good candidate to include on the review \nteam, as they have a vested interest in understanding their source (the user story), \nclarifying any ambiguities and removing any defects before they use it in their own \ndevelopment work. \nUse testers \nAs discussed in Chapter 1, testers are professional pessimists. This focus on what \ncould go wrong makes them good contributors to reviews, provided they observe \nthe earlier points about keeping the review experience a positive one. In addition to \nproviding valuable input to the review itsell, testers who participate in reviews often \nlearn about the product. This supports earlier testing, one of the principles discussed \nin Chapter 1. Using testers as reviewers is also beneficial to the testers, as the review \ncan help them identify relevant test conditions and test cases, and to begin preparing \nthe tests carlier. \nEach participant does their review work well \nEach reviewer has responsibilities and possibly roles to play in the review. It is \nimportant that each of them takes the reviewing work seriously and does it to the \nbest of their ability. This includes spending adequate time on the review activities, \nand paying attention to detail as needed. for example by using a checklist. \nLimit the scope of the review and pick things that really count \nAs mentioned under the organizational factors, do not try to review too much at one \ntime. Even if a limited scope has not been formally defined, reviewers should be \nselective about what they spend their time on. Select the work products for review \nthat are most important in a project, Reviewing highly critical, upstream work \nG N Compoge Lowming. A3 Khện Bt vt My et b o, s, o4 A i, 0 s 18 . o, U 3 i e, rn sty comto rê b gyt B y el snbs At s e e a et n s et — ì Conpage Lismng K e (i 1 ey khn ee Gne o n o € shocgaes ngh e n g n",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 110,
            "page_label": "111"
        }
    },
    {
        "page_content": "98 Chapter 3 Statc techniques \nproducts like requirements and architecture will most certainly show the benefits of \nthe review process to the project. This investment in review hours will have a clear \nand high return on investment, \nAnother advantage of limiting the scope of a review is that it is easier to concen- \ntrate on a small piece of work rather than trying to take in a large amount, Review- \ners need to keep up their concentration on the important things, both in individual \npreparation and in any review meeting. \nDefects found should be welcomed \nThe attitude towards defects found in reviews is eritical to success. Any defect found \nis an opportunity to improve not only the quality of the work product that it was \nfound in but also to become aware of other similar defects in the same or in different \nwork products, When reviews are working well, many more defects are prevented \nthan are found. \nBut the attitude towards defects by the author of the work product is also critical, \nDefects should be acknowledged and appreciated (even if you are not sure at the time \nwhether it really is a defect or not, acknowledge the point made by the person reporting \nit). Defects should be reported and handled objectively; defect reporting should never \nbecome personal, as this creates il will and damages the review process and culture. \nReview meetings are well managed \nThe role of the facilitator or moderator in the review meeting is one of the major \nfactors in making the whole review experience useful and pleasant. The meeting \nshould be focused on the review objectives, and any discussion should be tightly \ncontrolled. It is all too casy for discussions in meetings to go on and on, but if \nthe purpose of the review is to identify defects as efficiently as possible, then most \ndiscussion is probably unnecessary. All review participants should feel that their \ntime in the review (including the meeting) has been a valuable use of their tìme, \nTrust is critical \nAs mentioned in the earlier section, it is very important that the defect information \nis considered sensitive data and handled accordingly. Even the rumour of a manager \nwanting to use review data to evaluate people is enough to destroy the effectiveness \nOf a review process. There needs to be an atmosphere of trust among the reviewers; \nthey all know that they are helping cách other get better. and they need to be confi- \ndent that the data will be used in the right way, \nHow you communicate is important \nParticularly in review meetings, subliminal factors such as body language and tone \nØf voice may damage the openness of a good review atmosphere. In addition, ¡f \ndefects raised are criticized, for example by the author, then reviewers are much less \nlikely to raise other defects, so something very important may be missed. Language \nis important as well. Contrast “There may be a problem here' (objective wording), \nwith “You did it wrong bere' (personal attack). \nFollow the rules but keep it simple \nFollow all the formal rules until you know why and how to modify them, but make \nthe process only as formal as the project culture or maturity level allows. Do not \nbecome too theoretical or too detailed. Checklists and roles are recommended to \nincrease the effectiveness of defect identification. \nom0 300 o v Lornng 8 s Bt Ay b i e et o 1 . s s e Gy s o e ryy o e ot b et e e a et — ì SGeHRẵRSNGMNNSUSUN GG S € whacgacs YIS (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 111,
            "page_label": "112"
        }
    },
    {
        "page_content": "Section 2 Review Process 99 \nTrain participants \nIt is important that training is provided in review techniques, especially the more \nformal techniques, such as inspection. Otherwise the process is likely to be impeded \nby those who do not understand the process and the reasoning behind it. Special \ntraining should be provided to the facilitators (moderators) to prepare them for their \ncritical role in the review process. \nContinuously improve process and tools \nContinuous improvement of process guidelines and supporting tools (for cxam- \nple checklists), based upon the ideas of participants, ensures the motivation of the \nengineers involved. Motivation is the key to a successful change process. There \nshould also be an emphasis, ín addition to defect finding, on learning and process \nimprovement which becomes part of the culture of the organization. \nJust do it! \nFinally, the review process is simple but not casy, Every step of the process is clear, \nbut experience is needed 1o execute them correctly. Try to get experienced people to \nobserve and help where possible. But most importantly, start doing reviews and start \nlearning from every review. \nMore information about successful reviews can be found in Weigers [2002], van \nVeenendaal [2004], Sauer [2000] and Kramer and Legeard [2016]. \not S04 Crmpogs Loty N3 Bt Bt vl My et x oo, s o4 Bl 0 s 18 ot (o 3 o, . ot ety st ey l ree s B n ol snbis gt s el e A e gyt oot s e sy SR Tl g eTicncs CEngage Ly BTG e (W 14 Y kh ee o o o € g ngh e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 112,
            "page_label": "113"
        }
    },
    {
        "page_content": "100 Chapter 3 Static techniques \nCHAPTER REVIEW \nLets review what you have learned in this chapter. \nFrom Section 3.1, you should be able to recognize the software work products \nthat can be examined by static testing techniques. You should be able to explain the \nvalue of static testing by using examples. You should be able to explain the differ- \nence between static testing and dynamic testing ín terms of their objectives, types of \ndefects to be found, and the role of these techniques within the software life cycle. You \nshould know the Glossary terms dynamic testing, static analysis and static testing, \nFrom Section 3.2, you should be able 1o summuarize the activities of the work \nproduct review process. You should recognize the different roles and responsibilities \nin a formal review. You should be able to cxplain the differences between the various \ntypes of review: informal review, walkthrough, technical review and inspection. You \nshould be able to apply a review technique to a work product to find defects, Finally, \nyou should be able to explain the factors for successful performance Of reviews, \nboth organizational and people-related. You should know the Glossary terms ad hoc \nreviewing, checklist-based reviewing, formal review, informal review, inspec- \ntion, perspective-based reading, review, role-based reviewing, scenario-based \nreviewing, technical review and walkthrough. Finally, you shookd be able 10 carry \nout a review, particularly the individual reviewing of a work product, as this is K3 \nlevel in the Syllabus. \ne 0002031772 4 g St U ot . i et 0 10 et vt i e G ot s AP e et e et —+. rs o Loamtng et e gl b s bbb e o 7 b § bt o4l b gt &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 113,
            "page_label": "114"
        }
    },
    {
        "page_content": "SAMPLE EXAM QUESTIONS \nQuestion 1 Which of the following artefacts can \nNOT be examined using review techniques? \na, Software code. \nb. User story. \n© Test designs. \nd. User’s intentions, \nQuestion 2 Which of the following are the main \nactivities of the work product review process? \n1. Planning. \n2. Initiate review. \nSelect reviewers. \nIndividual review. \nReview meeting. \nEvaluating review findings against exit criteria, \n1ssue communication and analysis. \nFixing and reporting. \n812478 \nb. 2,3,4,58 \n& L3352 \nd 14,567 \nR \n- \ntme \nm \nQuestion 3 Which statement about static and \ndynamic testing is True? \na. Static testing and dynamic testing have different \nobjectives. \nb. Static testing and dynamic testing find the same \ntypes of đefcct. \n¢ Static testing identifies đefects through failures; \ndynamic testing finds defects directly. \nd. Static testing can find some types of defect with \nless effort than dynamic testing. \nQuestion 4 What statement about reviews is True? \na. Inspections are led by a trained moderator, but \nthis is not necessary for technical reviews. \nb. Technical reviews are led by a trained leader, \ninspections are not. \n© In a walkthrough, the author does not attend. \nd. Participants for a walkihrough always need to be \nthoroughly trained. \nSamgle Exam Questions 101 \nQuestion 5 Which statement below ís True? \na. Management ensures effective running of review \nmectings; the review leader decides who will be \ninvolved. \nb. Management is responsible foe review planning: \nthe facilitator monitors ongoing cost effectiveness. \n© Management organizes when and where reviews \nwill take place: the review leader assigns staff, \nbudget and time. \nd Management decides on the execution of reviews: \nthe facilitator is often the person on whom the \nsuccess of the review depends. \nQuestion 6 Match the following characteristics \nwith the type of review, \n1. Led by the author, \n2. Undocumented. \n3. Reviewers are technical peers of the author. \n4. Led by a trained moderator or leader. \n5. Uses entry and exit criteria. \nINSP: Inspection \nTR: Technical review \n1R: Informal review \nW: Walkthrough \na. INSP: 4, TR: 3.1R: 2and 5, W: I \nb. INSP: dand 5, TR: 3,IR: 2, \n€. INSP: I and 5, TR: 3, IR: I.W'-l \nd INSP: S, TR: 4, IR: 3, W: 1 and 2 \nQuestion 7 Which of the following statements \nabout success factors in reviews are True? \n1. Reviewers shoukl try to review as much of the \nwork product us they can, \n2. The author acknowledges and appreciales defects \nfound in their work. \n3. Each review has clear objectives, which are \ncommunicated to the reviewers. \n4. Testers are not normally involved in reviews, as \ntheir work focuses on test design. \n5. Checklists should be standardized and used for all \ntypes of work product. \ne 00020122 4 St U ot . . Gt 0 10t B et i e G ot s AP e et e et .. —<. Iy o Lramtng et e gl b s adbd et o 7 S § bt o4l b gt &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 114,
            "page_label": "115"
        }
    },
    {
        "page_content": "102 Chapter 3 Statc techniques \na 2and 3. \nb. 1, 2and3. \nc 1,2and4. \nd. 2,3and 5. \nQuestion 8 Which review technique is this: \nreviewing a requirements specification or user story \nfrom the point of view of an end-user of the system, \nusing a checklist? \n3. Checklist-based. \nb. Role-based. \nc. Perspective-based. \nd. Scenario-based. \nQuestion 9 Consider the following specification for \nreview: \nWhen you sign up, you must give your first \nand last name, postal address, phone number, \nemail address and password. When you log in, \nyou must give your last name, phone number \nand password, You are logged in until you select \not S04 Crmpoge Loy N3 Bt Bt vl My et x oo s o4 bl 0 b 18 ot (o 3 i, . ot st ey l ee s B n ol snbis Akt s el e i e s gyt oot s et Sy s el g pericnce Cengage 12 BTN e (W 14 s n kh ee o o n o € g 30 e , g \nLog Out followed by answering 'Yes' to ‘Are you \nsure?` When you are logged in, you can update \nyour details, but you need to confirm the change \nby entering a secure code sent to your phone. You \nthen need to log in again. \nThe following are potential defects in the \nspecification: \n1. Incorrect timeout for the secure code sent to the \nphone. \n2. Buffer overflow for lengthy postal address. \n3. Cannot change your phone number, as the code is \nsent to the old number. \n4. Details are stored as an additional entry in the \ndatabase for every change. \nWhich of the potential detects above would be \nmost likely to be found by a review performed by \ndevelopers? \na. 2and 3, \nb. 2.3 and 4. \n© 1,2and4. \nđ. land4.",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 115,
            "page_label": "116"
        }
    },
    {
        "page_content": "Exercise 103 \nEXERCISE \nPerform an individual review of the functional specification shown in Document 3.1, using a checklist-based \nreview. The checklist is as follows: \n1. Do all requirements give sufficient detail needed for determining the expected results for tests? \n2. Are all dependencies and restrictions specified? \n3. Are alternatives specified for all options? \nMake a list of potential defects in the specification, noting the severity level of each (High, Medium or Low). \nNote which checklist question has found the defect. Solution ideas are given ín the next section. \no S0 Crmpoge Loy N3 Rt Bt vl My et x o s o4 Bl 0 s 18 ot (o 3 s, e, ot ety st re l ree s n n ol b gt i el e M e gyt oot s e Sy s n el n pericncs Cengage Lewing AT e (W 14 W ol o o e € g 30 e n g G",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 116,
            "page_label": "117"
        }
    },
    {
        "page_content": "104 Chapter 3 Statc techniques \nDOCUMENT 3.1  Functional requirements specification \nFunctional requirements \nThis specification describes the required functionality of the booking system for a sports centre, \nBrowsing the facilities \nCustomers will be able to view all the available facilities using the following options to filter the selection: \n® None - shows all facilities \n®. Outdoor/indoor \n® Sport - selected from a pull-down list \n® Date - using a standard calendar \n® Time - selected from a pull-down list \nEach facility shown will include colour-coded information on availability (not available, already booked. \nand available for booking) and the time slots. \nSelecting a facility \nAny facility available for booking can be selected by clicking on the required time peziod. A confirmation \nmessage will be displayed. The user may either cancel the message and return to the list of facilities or \ncontinue to the booking details page. \nBooking details \n1f the customer is not already logged in, the customer login dialogue will be displayed. \nDetails of the facility together with any relevant regulations (such as age restrictions, footwear and \nno-show conditions, etc,) will be displayed. The customer name, member ID and mobile phone number \nfields will be pre-filled. The customer can either cancel or confirm the details. Confirming the details takes \nthe customer to the payment details page: cancelling returns the customer to the list of facilities page. \nPayment details \nPayment can be made by credit card. The following details are required: \n® Card type = Visa or Mastercard \n® Name on card — as printed on card \n® Curd number — 16 digits, no spaces \n® Expiry date - mm/fyy \n® CVC code - last 3 digits on the signature strip \nHaving entered these details, the system will seek authorization for the payment. When this is received a \nfinal confirmation message is displayed. The customer can choose to cancel or confirm. If confirmed, the \nbooking is made and a confirmation email is sent to the customer’s email address (as specified in their \ncustomer profile). If the payment authorization fails, an error message is displayed. \no N Crmpoge Lowming. A3 Kt Bt vt My et b o, s, o4 b . 0 s 18 . o, (b 3 i, e, ot sty comto rế b sy B n el smbs At s e e G et sy H st s 8 ree barwny S Lowmag murves e 1 10wy akbacnd o o 101 S € shargacm A ID EH",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 117,
            "page_label": "118"
        }
    },
    {
        "page_content": "Exercise Solution 105 \nEXERCISE SOLUTION \nThe following are some potential defects in the functional requirement shown in Document 3.1. This is not an \nexhaustive list; you may have found others. We have noted the checklist question number used to discover each \nissue, and a severity level of High, Medium or Low. \nTABLE 3.1 Potential defects in the functional requirements specification \nDefect Description Checklist Severity \n1 Need to know the specific colours in the colour coding to be able to 1 M \nsee Íf the test passes or falls. \n2 Need to know what all the facilities are, ín order to check if they are 1 M \ndisplayed correctly. \n3 What facilities are in the 'Sport' pull-down list? 2 M \n4 What are the rules for selection of options? Does Date have to be s H \nselected before Time, for example? Can you select only one option? \nL l login fais, what 5 supposed to happen: can you continue tọ book 3 H \nanyway? \n6 lf login ð successful, what screen is displayed, already selected, or back 3 L \n10 list? \n7 Can filters be combined? For example can you select Indoor and Tennis? 2 L \n8 Is it possible to choose more than one time slot at once, for example 3 2 M \nconsecutive half-hour times? \n3 lf credit card authorization fails, what happens after the error message 3 L \nis shown? \n10 The customer can cancel after authorization has gone through? Has 3 H \npayment already been taken? lf so, will it be refunded? Automatically? \n\" When booking 5 cancelled after authorization, what screen is shown 3 L \nafter the error message? \n12 Missing from specification: Is cancellation possible after a booking has none H \nbeen made, either by customer or by the sports centre? Is a refund \ngiven? Automatically? \nAs mentioned above, these are some suggested potential defects. Note that we have phrased many of them as \nquestions, which may be less threatening to the author of the requirements document. Note also that we have noted \nan additional high severity defect which was not specifically triggered by an item in the checklist. \nG N0 Compoge Lowning. A3 K Bt vt My et b o, s, o4 s . 0 s 18 . o, (b 3 i g, s sty comto rế b gy B y el smbs At e e ee ety . et et 8 cvent arwny sO ev ey akbacnd o o e} Vo € whacacm IS (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 118,
            "page_label": "119"
        }
    },
    {
        "page_content": "CHAPTER FOUR \nTest techniques \nhapter 3 covered static testing, looking at work products and code, but not \nrunning the code we are interested in. This chapter looks at dynamic testing, \nwhere the software we are interested in is run by executing tests on the running code. \n4.1 CATEGORIES OF TEST TECHNIQUES \nSYLLABUS LEARNING OBJECTIVES FOR 4.1 CATEGORIES \nOF TEST TECHNIQUES (K2) \nFL-4.1.1 Explain the characteristics, commeonalities and differences \nbetween black-box test techniques, white-box test techniques \nand experience-based test techniques (K2) \nIn this section we will look at the different types of test technigues, how they are used, \nhow they differ and the factors to consider when choosing a test technique. The three \ntypes Of categories are distinguished by their primary source: a description of what \nthe system should do (for example requirements, user stories, ete.), the structure of the \nsystem or component, or a person's experience. All categories are useful and the three \nare complementary. \nThe purpose of a test technique is 1o identify test conditions, test cases and test \ndata, Test conditions are identified during analysis, and then used to define test cases \nand test data during test design, which are then used ín test implementation, For cxam- \nple. in risk-based testing strategies, we identify risk items (which are the test condi- \ntions) when performing an analysis of the risks to product quality. Those risk items, \nalong with their corresponding levels of risk, are subsequently used 1o design the test \ncases and implement the test data. Risk-based testing will be discussed ín Chapter 5. \nIn this section, look for the definitions of the Glossary terms black-box test tech- \nnique, coverage, experience-based test technique, test technique and white-box \ntest technique. \n4.1.1 Choosing test techniques \nIn this section we will look at the factors that go into the decision about which tech- \nniques to use when. \nWhich technique is best? This is the wrong question! Each technique ís good for \ncertain things. and not as good for other things. For example, one of the benefits of \nwhite-box techniques is that they can find things in the code that are not supposed to \n106 \nquyynện N Crmpogs Loswming. NS Rt Roerrnd Moy ot b copi, wamnd, o0 et i s 10 e, U 3 vt g, acmms oty commvs sy b sppvesed B e ol snbhe gt T mview s ee sy n H s & ree kưnee: 1 Conpae GS MNISUNUIN NCY IG II O",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 119,
            "page_label": "120"
        }
    },
    {
        "page_content": "Section 1 Categorles of Test Techriques 107 \nbe there, such as Trojan Horses or other malicious code. However, if there are parts \nof the specification that are missing from the code, black-box techniques can find \nthat, White-box techniques can only test what is there, lf there are things missing \nboth from the specification and from the code, then only experience-based techniques \nwould find them. Each individual technique is aimed at particular types of defect as \nwell. For example, state transition testing is unlikely to find boundary đefects. \nThe choice of which test technique to use depends on a number of factors, which  Test technique (test \nwe discuss below, case design technique, \nSome techniques are more applicable to certain situations and test levels; others  1€st specification \nare applicable to all test levels, The best testing uses a combination of test techniques.  (€hnique, test design \n“This chapter covers the most popular and commonly used software test techniques. technique) A procedure \nThere are many others that fall outside the scope of the Syllabus that this book is \nbased on, With so many testing techniques to choose from, how are testers t decide \nwhich ones to use? \nThe use of techniques can vary ín formality, from very informal (with litle or no \n'documentation) to very formal, where information about test conditions and why par- \nticular techniques are used is recorded, The level of formality also depends on other \nfactors as discussed below. For example, safety or regulatory industries require a \nhigher level of formality. The maturity of the organization, the life cycle model being \nused, and the knowledge and skills of the testers also influence the level of formality. \nPerhaps the single most important thing to understand is that the best test tech- \nnique is no single test technique. Because each test technique is good at finding one \nspecific class of defect, using just one technique will help ensure that many (perhaps \nmost but not all) defects of that particular class are found. Unfortunately, it may \nalso help to ensure that many defects of other classes are missed! Using a variety of \ntechniques will therefore help ensure that a variety of defects are found, resulting in \nmore effective testing. \nSo how can we choose the most appropriate test techniques to use? The decision \nwill be based on a number of factors, both internal and external. \nThe factors that influence the decision about which technique to use are: \no Type of component or system. The type 0f component (for example embedded, \ngraphical, financial, etc,) will influence the choice of techniques. For example, a \nfinancial application involving many calculations would benefit from boundary \nvalue analysis. \no Component or system complexity, More complex components or systems are \nlikely to have more defects. and defects may be harder to find. Using a different \ntechnique to address aspects of that complexity will give better defect detection \nfrom the testing. For example, a simple field with numerical input would be \na good candidate for equivalence partitioning and boundary value analysis, \nbut a screen with many fields, with complex dependencies, calculations and \nvalidation rules depending on aspects that change over time, would benefit from \nadditional techniques such as decision table testing, state transition testing and \nwhite-box test technigues. \n® Regulatory standards. Some industrics have regulatory standards or guidelines \nthat govern the test techniques used. For example, the aircraft industry \nrequires the use of equivalence partitioning. boundary value analysis and state \ntransition testing for high integrity systems, together with statement, decision \nor modified condition decision coverage depending on the level of software \nintegrity required. \nG N Compuge Lowming. A3 K Bt vt My et b o, s, o4 e s, 0 s 10 . o, U 3 st g, s e paty comto rế b gy B n el smbs /Ch N v e e e n s et — ì S Lonmay murves e 1y 10 ey akbacnd o o 0} UoGowGbebyDopuyeHVg",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 120,
            "page_label": "121"
        }
    },
    {
        "page_content": "108 Chapter 4 Test tachniques \n® Customer or contractual requirements. Sometimes contracts specify particular \ntest techniques to use (most commonly statement or branch coverage). \n® Risk levels and risk types. The greater the risk level (for example safety-critical \nsystems), the greater the need for more thorough, formal testing, Commercial \nrisk may be influenced by quality issues (so more thorough testing would be \nappropriate) or by time to market issues (so exploratory testing would be a more \nappropriate choice). Risk type might, for example, tell us that a risk is related \nto usability, performance, security or functionality. Of course, the correct test \ntechnique needs to be chosen that is able to address the risk type that is being \nmitigated. \n® Test objectives. ]f the test objective is simply 10 gain confidence that the software \nwill cope with typical operational tasks, then use cases would be a sensible \napproach. lf the objective is for very thorough testing, then more rigorous and \ndetailed techniques (including white-box test techniques) should be chosen. \n® Awailable documentation. Whether or not documentation (for example a work \nproduct such as a requirements specification) exists, and how up-to-date it \nis, will affect the choice of test techniques. The content and style of the work \nproducts will also influence the choice of techniques (for example, if decision \ntables or state graphs have been used, then the associated test techniques should \nbe used). \n® Tester knowledge and skills. How much testers know about the system and about \ntest techniques will clearly influence their choice of techniques. Experience-based \ntechniques are particularly based on tester knowledge and skills, \n® Available tools, |f tools are available for a particular technique, then that \ntechnique may be a good choice. Since test techniques are based on models, \nthe models available (that is, developed and used during the specification, \ndesign and implementation of the system) will to some extent govern which \ntest techniques can be used. For example, 1f the specification contains a state \ntransition diagram, state transition testing would be a good technique to use, \n® Time and budget. Ultimately how much time there is available will always \naffect the choice of test techniques, When more time is available, we can afford \nto select more techniques. When time is severely limited, we will be limited \nto those that we know have a good chance of helping us find just the most \nimportant defects. \no Software development life cycle model. A sequential life cycle model will lend \nitself to the use of more formal techniques, whereas an iterative life cycle model \nmay be better suited to using an exploratory test approach, \n® Expected use of the software. ]f the software is to be used in safety-critical \nsituations, for example medical monitoring devices or car-driving technology, \nthen the testing should be more thorough, and more techniques should be \nchosen. \n® Previous experience with using the test techniques on the component or system \ntø be tested. Testers tend 1o use techniques that they are familiar with and more \nskilled at, rather than less familiar techniques. With this familiarity, you can \nbecome very effective at finding similar defects to those that have occurred \nbefore. But be aware that you may get better results from using a technique \nthat is less familiar, and when you do use ít, you will increase your skill and \nfamiliarity with it. \nG N Compoge Lowming. A3 K Bt vt My et b o, s, o4 e, 0 s 16 . o, U 30 i e, s sty comto rế b gy B n el smbis /Ch v eview e e e n s et — ì SGIeHUSSINGMNNSCNUN GG € whacgacs YIS (74T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 121,
            "page_label": "122"
        }
    },
    {
        "page_content": "Section 1 Categorles of Test Techriques 109 \n® The types of defects expected ín the component or system. Knowledge of the \nlikely defects will be very helpful in choosing test techniques (since each \ntechnique is good at finding a particular type of defect). This knowledge \ncould be gained through experience of testing a previous version of the system \nand previous levels of testing on the current version. \nIt is important to remember that intelligent, experienced testers see test tech- \nnigues (and indeed test strategies, which we'll discuss in Chapter 5) as tools to be \nemployed wherever needed and useful, You should use whatever techniques and \nstrategies, in whatever combinations, make sense to ensure adequate coverage of \nthe system under test, and achievement of the objectives of testing, Feel free to \ncombine the test techniques discussed in this chapter with whatever inspiration you \nhave, along with process-related, rule-based and data-driven techniques. Use your \nbrain and do what makes sense. \n4.1.2 Categories of test techniques and their \ncharacteristics \nThere are many different types of software test techniques, each with its own \nstrengths and weaknesses, Each individual technique is good at finding parti- \ncular types of defect and relatively poor at finding other types. For example, a \ntechnique that explores the upper and lower limits of a single input range is more \nlikely to find boundary value defects than those associated with combinations \nof inputs, Similarly, testing performed at different stages in the software devel- \nopment life cycle will find different types of defects: component testing is more \nlikely to find coding logic defects than system design defects or user experience \nproblems. \nEach test technique falls into one of a number of different categories. Broadly \nspeaking there are two main categories, statíc and dynamic. Static test techniques, \nas discussed in Chapter 3, do not execute the code being examined and are gene- \nrally used before any tests are executed on the software. They could be called \nnon-execution techniques. Most static test techniques can be used to test any form \nof work product including source code, design documents and models, user stories, \nfunctional specifications and requirement specifications. Static analysis is a tool- \nsupported type of static testing that concentrates on testing formal languages and \n50 is most often used to statically test source code. Black-box test \nIn this chapter we look at dynamic test techniques, which are subdivided into . technique M‘bﬂl \nthree more categories: black-box (also known as specification-based, behavioural technique, specification- \nor behaviour-based techniques), white-box (structure-based or structural tech- based techrique, \nniques) and experience-based, Black-box test techniques include both functional and specificatio \nnon-functional techniques (that is, testing of quality characteristics). The techniques procedure 10 derive \ncovered in the Syllabus are summarized in Figure 4.1, andfor select test cases \nbased on I\\ analysis \nBlack-box test techniques 0f the specification, \nThe first of the dynamic test techniques we will look at are the black-box test = Bi\\er functional or \ntechniques. They are called black-box because they view the software as a black box \"“*\"\"m\"\"\"« ofa \nwith inputs and outputs, but they have no knowledge of how the system or compo- W : \"”\"‘;‘!\"s \nnent is structured inside the box, In essence, the tester is concentrating on what the - u a n o \nsoftware does, not how it does it. \nyy N Compogs Lowming. A3 Kiện Bt vt My et b o, s, o4 e n 0 s 10 . e, U 3 i g, s sty conto ey b gy B n el smbs At s H n a n — ì SOIsHDSRSINUGMNNSUNSUN GG UoGwGboGyDopuyeHVg",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 122,
            "page_label": "123"
        }
    },
    {
        "page_content": "110 Chapter 4 Test technques \nStatic \nInformal \nReviews \nWalkthroughs \nTechnical \nFIGURE 4.1 Testtechniques \nAll black-box test techniqucs have the common characteristic that they are based \non a model (formal or informal) of some aspect of the system, which enables test \nconditions and test cases to be derived from them in a systematic way. \nCommon characteristics of black-box test techniques include the following: \n® Test conditions, test cases and test data are derived from a test basis that may \ninclude software requirements, specifications, use cases and user stories. The \nsource of information for black-box tests is some description of what the system \n0f software is supposed to do. \n® Test cases may be used to detect gaps between the requirements and the \nimplementation of the requirements, as well as deviations from the requirements, \nOne of the strengths of test cases is that they make things specific, and this often \nhighlights different understandings about the test basis, showing what is missing \nor interpreted differently. \n® Coverage is measured based on the items tested in the test basis and the technique \napplied to the test basis. As we will see later, whenever you can make a list of \nsome things that could be tested and can tell whether or not they have been tested, \nthen you can measure coverage. Coverage at black-box level is based on items \nfrom the test basis. For example, does every requirement described have at least \none test that exercises it”? \nBlack-box test techniques are appropriate at all levels of testing (component testing \nthrough to acceptance testing) where a specification or other test basis exists. When \not N C-v 129928 A3 B Bt vt Moy et b o, s o bl . 0 b 18 o (b 3 ot g, rnv B paty ree rê b syt B n Pl b et v e A dovmd u n H s 8 neenh kưneeg o Conpae EN vNISUNUN H GG QO/CPHEMN",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 123,
            "page_label": "124"
        }
    },
    {
        "page_content": "Section 1 Categorles of Test Techriques 111 \nperforming system or acceptance testing, the requirements specification or functional \nspecification may form the basis of the tests. When performing component or integra- \ntion testing, a design document or low-level specification may form the basis of the tests. \nNotice that the definition mentions both functional and non-functional testing. \nFunctional testing is concerned with what the system does, its features or functions. \nNon-functional testing is concerned with examining how well the system does \nsomething, rather than what it does. Non-functional aspects (also known as qua- \nlity characteristics or quality attributes) include performance, usability, portability, \nmaintainability, etc. Techniques to test these non-functional aspects are less proce- \ndural and less formalized than those of other categories, as the actual tests are more \ndependent on the type of system, what it does and the resources available for the tests. \nNon-functional testing is part of the Syllabus and is also covered in Chapter 2. \nThere are techniques for deriving non-functional tests [Gilb 1988], but they are not \ncovered at the Foundation level. \nCategorizing test technigues into black-box and white-box is mentioned ín a num- \nber of testing books, including Beizer |1990], Black [2007] and Copeland [2004]. \nWhite-box test techniques \nWhite-box test techniques (which are also dynamic rather than static) use the= White-box test \ninternal structure of the software to derive test cases. They are commonly called r technique (structural \nwhite-box or glass-box techniques (implying you can see into thesystem/box) since  1est technique, \nthey require knowledge of how the software is implemented, that is, how it works,  Stucture-based test \nFor example, a structural technique may be concerned with exercising loops in the mm\" structupe- \nsoftware. Different test cases may be derived to exercise the loop once, twice and ree \nmany times. This may be done regardless of the functionality of the software. All \nstructure-based technigues have the common c?nnclnin_lic that they are based on - n d/ọc catact test cases \nhow the software under test is constructed or designed. This structural information is = based on an analysis of \nused to assess which parts of the software have been exercised by aset of tests (often  the internal structure of \nderived by other techniques). Additional test cases can then be derived in a systematic - component or system. \nway to cover the parts of the structure that have not been touched by any test before. \nCommon characteristics of white-box test techniques include the following: \n® Test conditions, test cases and test data are derived from a test basis that may \ninclude code, software architecture, detailed design or any other source of \ninformation regarding the structure of the software. White-box test technigues \nare most commonly used for code structure, but these techniques are also useful \nfor other structures, For example, the menu structure Of an app could be tested \nusing white-box techniques. \n® Coverage is measured based on the items tested within a selected structure, for Coverage (test \nexample the code statements, the decisions, the interfaces, the menu structureor  Coverage) The degree \nany other identified structural clement. Sec Section 4.3 for more on coverage. 1o which specified \n® White-box test techniques determine the path through the software that ;.\"m“ “m\"': \neither was taken or that you want to be taken, and this ís determined by have been exercised by \nspecific inputs to the software. However, in order to be a test, we also need a test suite, expressed \n10 know what the expected outcome of the test case should be, even though as a percentage. \nthis does not affect the path taken. A test oracle of some kind, for cxample a \nspecification, is used to determine the expected outcome, \n‘White-box test techniques can also be used at all levels of testing. Developers \nuse white-box techniques ín component testing and component integration testing, \nG N Crmpoge Lowming. A3 Kighs Bt vt My et b o, s, o4 A . 0 s 18 . o, U 30 i g, s sty comto rế b rE ee B y el smbs At m v e e e H s et — ì sS GMNNCHUIN GG o € hacacs A (70T ns g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 124,
            "page_label": "125"
        }
    },
    {
        "page_content": "112 Chapter 4 Test techniques \nespecially where there is good tool support for code coverage. White-box techniques \nare also used in system and acceptance testing, but the structures are different. For \nexample, the coverage of major business transactions could be the structural element \nin system or acceptance testing. \nExperience-based test techniques \nExperience-based test Ín experience-based test techniques, people’s knowledge, skills and background \nlexperience- - are a prime contributor to the test conditions, test cases and test data, The expe- \nbased technique) A rience of both technical and business people is important, as they bring different \nprocedure to derive perspectives to the test analysis and design process. Duc to previous experience \nand/or select test cases with similar systems, they may have insights into what could go wrong, which is \nvery useful for testing. Both structural and behavioural insights are used to design \nexperience, knowledge experience-based tests, \nAll experience-based test techniques have the common charactezistic that they are \nbased on human knowledge and experience, both of the system itself (including the \nknowledge of users and stakeholders) and of likely defects. Test cases are therefore \nderived in a less systematic way, but may be more effective. \nExperience-based test techniques are used to complement black-box and white- \nbox techniques, and are also used when there is no specification, or if the specifi- \ncation is inadequate or out-of-date. This may be the only type of technique used for \nlow-risk systems, but this approach may be particularly useful under extreme time \nPressurc. In fact this Ís one of the factors leading to exploratory testing. \n4.2 BLACK-BOX TEST TECHNIQUES \nSYLLABUS LEARNING OBJECTIVES FOR 4.2 BLACK-BOX \nTEST TECHNIQUES (K3) \nFL-4.2.1 Apply equivalence partitioning to derive test cases from given \nrequirements (K3) \nFL-4.22 Apply boundary value analysis to derive test cases from given \nrequirements (K3) \nFL-42.3 Apply decision table testing to derive test cases from given \nrequirements (K3) \nFL-424  Apply state transition testing to derive test cases from given \nrequirements (K3) \nFL-42.5 Explain how to derive test cases from a use case (K2) \nIn this section we will look in detail at four black-box test techniques. These \nfour techniques are K3 in the Syllabus, This means that you need to be able to \nuse these techniques to design test cases. We will also cover briefly (not at K3 \nlevel) the technique of use case testing. In Section 4.3, we will look at the K2 \nwhite-box test techniques. \nG N Compoge 1o9:95g A3 Kin Bt vt My et n o, s, o4 b n 0 s 10 n o, U 3 i, e, re sty conto rế b gy n n el smbs At s e s n H ee & ree kưne: SGIHEASNUSNNSCSUNU GSU IG GIEGSO (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 125,
            "page_label": "126"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Technques 113 \nIn this section, look for the definitions of the Glossary terms boundary value \nanalysis, decision table testing, equivalence partitioning, state transition testing \nand use case testing, \nThe four black-box test techniques we will cover in detail are: \n® Equivalence partitioning. \n® Boundary value analysis. \n® Decision table testing. \n® State transition testing. \n4.2.1 Equivalence partitioning \nEquivalence partitioning (EP) is a good all round black-box test technique. ltcan Equivalence \nbe applied at any level of testing and is often a good technique to use first. Itis a \ncommon sense approach to testing, so much so that most testers practice it infor-  1€sting) Amm \nmally even though they may not realize it. However, while it is better to use the 185t technique in which \ntechnique informally than not at all, it is much better to use the technique in a formal test cases are m \nway 1o attain the full benefits that it can deliver. This technique will be found in ““\"E.'“’\"“\"\"\"\"“ \nmost testing books, including Myers [2011), Copeland [2004), Jorgensen |2014]and  POrttions by using one \nKaner f' al, IZI.'!I 3] — s i - of each partition. \nThe idea behind the technique is to divide (that is, to partition) a set of test con- \nditions into groups or sets where all elements of the set can be considered the same, \nso the system should handle them equivalently, hence ‘equivalence partitioning”. \nEquivalence partitions are also known as equivalence classes: the two terms mean  Equivalence \nexactly the same thing. (equivalence dass) \nThe EP technique then requires that we need test only one condition from each A portion of the value \npartition. This is because we are assuming that all the conditions in one parution will ~domain of a data \nbe treated in the same way by the software. If one condition in a partition works, we element related to the \nassume all of the conditions in that partition will work, and so there is litle point in \ntesting any of these others. Conversely, if one of the conditions in a partition does 1 với 65 fé expecied \nnot work, then we assume that none of the conditions in that partition will work so . ¬me based on the \nagain there is little point in testing any more in that partition. Of course these are specification. \nsimplifying assumptions that may not always be right, but if we write them down, \nat beast it gives other people the chance to challenge the assumptions we have made \nand hopefully help to identify better partitions. If you have time, you may want to try \nmore than one value from a partition, especially if you want to confirm a selection \nof typical user inputs. \nFor example. a savings account in a bank carns a different rate of interest depend- \ning on the balance in the account, In order 10 test the software that calculates the \ninterest due, we can identify the ranges of balance values that earn the different rates \nof interest. For example, if a balance in the range SO up to $100 has a 3% interest rate, \na balance over S100 and up to S1,000 has a 5% interest rate, and balances of $1,000 \nand over have a 7% interest rate, we would initially identify three valid equivalence \npartitions and one invalid partition as shown below. \ninvalid partition | Vald(or3%inweres) | Vafd(or5%) |  Vaid(o7%) \n-50.01$0.00 $10000 [$10001  $999.99 |$1,000.00 \nyy N Compoge Lowming. A3 Kighs Bt vt My et b o, s, o4 e, s 18 . o, (b 3 i g, s sty cmto rế b gy B n el b /Ch v el e i e s gyt oot s o Sy o el g pericnce Cengage Lewing AT e (W 14 Y adlbncnd o o e o € i 30 (s s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 126,
            "page_label": "127"
        }
    },
    {
        "page_content": "114 Chapter 4 Testtechnques \nNotice that we have identified four partitions here, even though the specification \nonly mentions three. This illustrates a very important task of the tester: not only do \nwe test what is in our specification, but we also think about things that have not been \nspecified. In this case we have thought of the situation where the balance is less than \nzero, We have not (yet) identified an invalid partition on the right, but this would \nalso be a good thing to consider. In order to identify where the 7% partition ends, we \nwould need to know what the maximum balance is for this account (which may not \nbe easy to find owt). In our example we have left this open for the time being. Note \nthat non-numeric input is also an invalid partition (for example the letter ‘a’) but we \ndiscuss only the numeric partitions for now. \nWe have made an assumption here about what the smallest difference is between \ntwo values, We have assumed two decimal places, that is $100.00, but we could have \nassumed zero decimal places (that is S100) or more than two decimal places (for \nexample $100.0000). In any case it is a good idea to state your assumptions: then \nother people can see them and let you know if they are correct or not. \nWe have also made an assumption about exactly which amount starts the new \ninterest rate: a cent over to go into the 5% interest rate, but exactly on the $1.000.00 \n1o gỗ into the 7% rate. By making our assumptions explicit, and documenting them \nin this technique, we may highlight any differences in understanding exactly what \nthe specification means. \nWhen designing the test cases for this software we would ensure that the three \nvalid equivalence partitions are each covered once, and we would also test the invalid \npartition at least once. So for example, we might choose to calculate the interest on \nbalances of =$10.00, $50.00, $260.00 and $1,348.00. If we had not specifically iden- \ntified these partitions, it is possible that at least one of them could have been missed \nat the expense of testing another one several times over. Note that we could also apply \nEP to outputs as well, In this case we have three interest rates: 3%, 5% and 7%, plus \nthe error message for the invalid partition (or partitions). In this example, the output \npartitions line up exactly with the input partitions, \nHow would someone test this without thinking about the partitions? A naive tester \n(let’s call him Robbie) might have thought that a good set of tests would be to test every \n$50. That would give the following tests: $50.00, S100.00, $150.00, $200.00, $250,00, ., \nsay up to $800.00 (then Robbic would have got tired of it and thought that enough tests \nhad been carried out), But look rự what Robbác has tested: only two out of four partitions! \nSo if the system does not correctly handle a negative balance o a balance of $1,000 or \nmore, he would not have found these defects, so the naïve approach is less effective than \nEP. At the same time, Robbie has four times more tests (16 tests versus our four tests using \nequivalence partitions), so he is also much less efficient! This is why we say that using \ntechniques such as this makes testing both more effective and more efficient. \nNote that when we say a partition is invalid, it does not mean that it represents \na value that cannot be entered by a user or a value that the user is not supposed to \nenter. It just means that it is not one of the expected inputs for this particular field. \nThe software should correctly handle values from the invalid partition, by replying \nwith an error message such as ‘Balance must be at least $0.00\". \nNote also that the invalid partition may be invalid only in the context of crediting \ninterest payments. An account that is overdrawn will require some different action. \nHere is a summary of EP characteristics: \n® Valid values should be accepted by the component or system. An equivalence \npartition containing valid values is called a valid equivalence partition. \nyy N4 Compoge Lowming. A3 Kt Bt My et b o, s, o4 b s, 0 s 10 . o, L 3 i e, re sty st rế b gy B n el b At v e e s H st P ì Conpge Lewmay murses e 10 10 ey akoacnd o o} IG aCH",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 127,
            "page_label": "128"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Technques 115 \n® Invalid values should be rejected by the component or system, An equivalence \npartition containing invalid values is called an invalid equivalence partition. \n® Partitions can be identified for any data element related to the test object, \nincluding inputs, outputs, internal values, time-related values (for example \nbefore or after an event) and for interface parameters (for example integrated \ncomponents being tested during integration testing). \n® Any partition may be divided into sub-partitions if required, where smaller \ndifferences of behaviour are defined or possible. For example, if a valid input \nrange goes from ~100 to 100, then we could have three sub-partitions: valid and \nnegative, valid and zero, and valid and positive. \n® Each value belongs to one and only one equivalence partition from a set of \npartitions. However, it is possible to apply EP more than once and end up with \ndifferent sets of partitions, as we will sec later under *Applying more than once' in \nthe section on ‘Extending equivalence partitioning and boundary value analysis'). \n® When values from valid partitions are used in test cases, they can be combined \nwith other valid values in the same test, as the whole set should pass. We can \ntherefore test many valid values at the same time. \n® When values from invalid partitions are used in test cases, they should be tested \nindividually, that is, not combined with other invalid equivalence partitions, 1o \nensure that failures are not masked. Failures can be masked when several failures \noccur at the same time but only one is visible, causing the other failures to be \nundetected. \n® EPis applicable at all test levels. \n'We have more things to say about EP (including how to measure coverage), but we \nwill return to that after we cover boundary value analysis, since the two techniques \nare closely related. \n4.2.2 Boundary value analysis \nBoundary value analysis (BVA) is based on testing at the boundaries between par-  Boundary value \ntitions that are ordered, such as a field with numerical input or an alphabetical list of  analysis A black-box \nvalues in a menu, It is essentially an enhancement or extension of EP and can also  test technique in which \nbe used to extend other black-box (and white-box) test techniques. If you have cver 185t CaSE5 are designed \ndone ‘range checking’, you were probably using the BVA technique, even if you were . D356d on boundary \nnot aware of it. Note that we have both valid boundaries (in the valid partitions) and \ninvalid boundaries (in the invalid partitions). \nAs an example, consider a printer that has an input option of the number of copies \nto be made, from 1 to 99. \nwaid | vae | e \no 99 | 100 \nTo apply BVA. we will take the minimum and maximum (boundary) values \nfrom the valid partition (1 and 99 in this case) together with the first or last value \nrespectively in each of the invalid partitions adjacent to the valid partition (0 and 100 \nin this case). In this example we would have three EP tests (one from each of the three \npartitions) and four boundary value tests. \nG N Crmpoge Loswming. A3 K Bt vt My et b o, s, o4 e i, s 18 . o, U 3 i, g, s sty conto rế b gy B y el smbs At e e ee ety / s e T b Conprge Lonmay muries e 1y 10 ey koo o o 1y o € whbacacm A (74T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 128,
            "page_label": "129"
        }
    },
    {
        "page_content": "116 Chapter 4 Testtechnques \nLet's return to the savings account system described in the previous section: \ninvald partition | Vald for 3% iterest) | Valid fors%) |  Vald(for7%) \n-5001 |$0.00 $10000 [$10001 - §999 99 [$1,00000 \nBecause the boundary values are defined as those values on the edge of a partition, \nwe have identified the following boundary values: ~$0.01 (an invalid boundary value \nbecause it is at the edge of an invalid partition), $0.00, $100.00, SIO0.01, $999.99 and \n$1.000.00, all valid boundary values. \nSo by applying BVA we will have six tests for boundary values, Compare what \nour naive tester Robbie had done: he did actually hit one of the boundary values \n(S100) though it was more by accident than design. So ín addition to testing only \nhalf of the partitions, Robbie has only tested one-sixth of the boundaries (so he will \nbe less effective at finding any boundary defects). lf we consider all of our tests for \nboth EP and BVA, the techniques give us a total of ten tests, compared to the 16 that \nRobbie had, so we are still considerably more efficient as well as being over three \ntimes more effective (testing four partitions and six boundaries, so ten conditions in \ntotal compared to three). \nNote that in this savings account interest cxample, we have valid partitions next \n10 other valid partitions. ]f we were to consider an invalid boundary for the 3% inter- \nest rate, we have =$0.01, but what about the value just above $100.00? The value of \n$100,01 is not an invalid boundary; it s actually a valid boundary because it falls \ninto a valid partition. So the partition for 5%, for example, has no invalid boundary \nvalues associated with partitions next to it, \nA good way to represem the valid and invalid partitions and boundaries is in a \ntable such as Table 4.1: \nTABLE 4.1 Equwalence partitions and boundaries \nTest conditions  Valid partitions  Invalidpartitlons  Valid boundaries  Invalid boundaries \nBalance in 50,00 - 510000 <$0.00 5000 - 5001 \n<x $100.01 - $999.99 >$Max $100.00 $Max + 0.01 \n$1,00000 - $Max  non-integer $100.01 \nm“” $999.99 \n$1,000.00 \n5Max \nInterest rates 3% Any other value Not applcable Not applicable \n5% Non-integer \n7% No interest \ncalcuated \nG N Crmpogs 129926 A3 Kt Bt vt My et n o, s, o4 b n 0 s 18 . o, U 3 ot g, s sty comto rế b gy n n el snbs At i e b s H st s & cven sy Conpage Lonmay murses e 1 10 ey akbacnd o o n o € haracs A tren s gt n",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 129,
            "page_label": "130"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Technques 117 \nBy showing the values in the table, we can see that no maximum has been specified \nfor the 7% interest rate. We would now want to know what the maximum value is for \nan account balance, so that we can test that boundary. This is called an open bound- \nary, because one of the sides of the partition is left open, that is, not defined, But that \n'doesn't mean we can ignore it. We should still try to test ít, but how? We have called \nit SMax to remind ourselves to investigate this. \nOpen boundaries are more difficult to test, but there are ways to approach them. \nActually the best solution to the problem is to find out what the boundary should be \nspecified as! One approach is to go back to the specification to see if a maximum has \nbeen stated somewhere else for a balance amount. lf so, then we know what our bound- \nary value is, Another approach might be to investigate other related areas of the system. \nFor example, the ficld that holds the account balance figure may be only six figures \nplus two decimal figures. This would give a maximum account balance of $999,999.99 \nso we could use that as our maximum boundary value. Íf we really cannot find any- \nthing about what this boundary should be, then we probably need to use an intuitive or \nexperience-based approach to probe various large values trying to make it fail. \nWe could also try to find out about the Jower open boundary. What is the lowest \nnegative balance? Although we have omitted this from our example, setting it out in the \ntable shows that we have omitted ít, so helps us be more thorough if we wanted to be. \nRepresenting the partitions and boundaries in a table such as this also makes it \ncasier to see whether or not you have tested each one (if that is your objective). To \nachieve 100% coverage for EP, we need to ensure that there is at least one test for \neach identified equivalence partition. To achieve 100% coverage of boundary values, \nwe need to ensure that there is at least one test for each boundary value identified, Of \ncourse, just one test for either a partition or a boundary may not be sufficient testing, \nbut it does show some degree of thoroughncss, since we have not left any partition \nor boundary untested. \nBVA can be applied at all test levels. \nExtending equivalence partitioning and boundary value analysis \nSo far, by using EP and BVA we have identified conditions that could be tested, that \nis, partitions and boundary values. The techniques are used to identify test conditions, \nwhich could be at a fairly high level (for example low interest account) or at a detailed \nlevel (for cxample value of $100.00). We have been looking at applying these techniques \n1o ranges of numbers. However, we can also apply the techniques to other things, \nApplying to more than numbers \nFor example. if you are booking a flight, you have a choice of Economy/Coach, Premium \nEconomy, Business or First Class tickets. Each of these is an equivalence partition in its \nown right and should be tested, but it does not make sense to talk about boundarics for \nthis type of partition, which is a collection of valid things, The invalid partition would \nbe an attempt to type in any other type of flight class (for example Staff). If this field \nis implemented using a drop-down list, then it should not be possible to type anything \nelse in, but it is still a good test 1o try at least once in some drop-down fiekl. When you \nare analyzing the test basis (for example a requirements specification or user story), EP \ncan help to identify where a drop-down list would be appropriate, \n‘When trying to identify a defect, you might try several values in a partition. If this \nresults ín different behaviour where you expected it to be the same, then there may be \ntwo (or more) partitions where you initially thought there was only one. \nG N Compoge 1o9:92g A3 Kt Bt vt My et b o, s, o4 b i, 0 s 10 . o, U 3 i e, s sty st rế b gy B y el snbs At e e v et H st s & ree arwny Conpge Lewmay murses e 1y 10 ey akoacnd comwm o iy IG aCH",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 130,
            "page_label": "131"
        }
    },
    {
        "page_content": "118 Chapter 4 Testtechnques \nWe can apply EP and BVA to all levels of testing. The examples here were at a \nfairly detailed level, probably most appropriate in component testing or in the detailed \ntesting of a single screen. \nAt a system level, for example, we may have three basic configurations which our \nusers can choose from when setting up their systems, with a number of options for \neach configuration. The basic configurations could be system administrator, man- \nager and customer liaison. These represent three equivalence partitions that could be \ntested. We could have serious problems if we forget to test the configuration for the \nsystem administrator, for example. \nApplying more than once \nWe can also apply EP and BVA more than once to the same specification item. For \nexample, ÍÍ an internal telephone system for a company with 200 telephones has \nthree-digit extension numbers from 100 to 699, we can identify the following parti- \ntions and boundaries: \n® Digits (characters 0 to 9) with the invalid partition containing non-digits, \n® Number of digits, 3 (so invalid boundary values of two digits and four digits). \n® Range of extension numbers, 100 to 699 (so invalid boundary values of 099 \nand 700), \n® Extensions that are in use and those that are not (two valid partitions, no \nboundaries). \n® The lowest and highest extension numbers that are ín use could also be used \nas boundary values. \nOne test case could test more than one of these partitions/boundaries. For example, \nExtension 409 which is in use would test four valid partitions: digits, the number of \ndigits, the valid range and the in use partition. It also tests the boundary values for \ndigits 0 and 9. \nHow many test cases would we need to test all of these partitions and boundaries, \nboth valid and invalid? We would need a non-digit, a two-digit and a four-digit num- \nber, the values of 99, 100, 699 and 700, one extension that is not in use, and possibly \nthe lowest and highest extensions in use. This is 10 or 11 test cases: the exact number \nwould depend on what we could combine in one test case. \nUsing EP and BVA helps us to identify tests that are most likely to find defects, \nand to use fewer test cases to find them. This is because the contents of a partition \nare representative of all of the possible values. Rather than test all ten individual \ndigits, we test one in the middle (for example 4) and the two edges (0 and 9). Instead \nof testing every possible non-digit character, one can represent all of them. \nApplying to output \nAs we mentioned carlier, we can also apply these techniques to output partitions, \nConsider the following extension to our bank interest rate example. Suppose that a \ncustomer with more than one account can have an extra 1% interest on this account \nÍf they have at least $1,000 in it. Now we have two possible output values (7% interest \nand 8% interest) for the same account balance, so we have identified another test \ncondition (8% interest rate). (We may also have identified that same output condition \nby looking at customers with more than one account, which is a partition of types \nof customer.) \no N Compoge Lossming. A3 Kin Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 st e, s s paty comto rế b n ee B y el smbs At el s M e gyt oo s e Sy s el g ericnce Cengage Lty x e (W 14 s n kh Gd o o o € g 3 e , g n",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 131,
            "page_label": "132"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Technques 119 \nApplying to more than human inputs \nEP can be applied to different types of input as well. Our examples have concentrated \non inputs that would be typed in (by a human) when using the system. However, \nsystems receive input data from other sources as well, such as from other systems \nvia some interface. This is also a good place to look for partitions (and boundaries). \nFor example, the value of an interface parameter may fall into valid and invalid \nequivalence partitions, This type of defect is often difficult to find in testing once \nthe interfaces have been joined together, so is particularly useful to apply in integra- \ntion testing, cither component integration, for example between APIs (Application \nProgramming Interfaces) or system integration. If you are receiving data from a \nthird-party supplier, a value sent by the supplier may be larger than the maximum \nvalue your system is expecting. If you do not check the value when it arrives, this \ncould cause problems. \nBoundary value analysis can be applied to a whole string of characters (for exam- \nple a name or address). The number of characters in the string is a partition, for \nexample between 1 and 30 characters is the valid partition with valid boundaries of \n1 and 30. The invalid boundaries would be zero characters (null, just hit the Return \nkey) and 31 characters. Both of these should produce an error message. \nPartitions can also be identified when setting up test data. If there are different \ntypes of record. your testing will be more representative if you include a data record \nof each type. The size of a record is also a partition with boundaries, so we could \ninclude maximum and minimum size records in the test database. \nlf you have some inside knowledge about how the data is physically organized, you \nmay be able to identify some hidden boundaries, For example, if an overflow storage \nblock is used when more than 255 characters are entered into a ficld, the boundary \nvalue tests would include 255 and 256 characters in that fiejd, This may be verging on \nwhite-box testing, since we have some knowledge of how the data is structured, but \nit does not matter how we classify things as long as our testing is effective at finding \ndefects. Don’t get hung up on a fine distinction: just do whatever testing makes sense, \nbased on what you know. An old Chinese proverb says, ‘It docsn't matter whether the \ncat is white or black: all that matters ís that the cat catches mice’, \nTwo- and three-value boundary analysis \nWith BVA, we think of the boundary as a dividing line between two things, Hence \nwe have a value on cach side of the boundary, but the boundary itself is not a value. \nInvalid | Vahd l Invalid \no1 % | 100 \nLooking at the values for our printer example, O is in an invalid partition, | and 99 \nare in the valid partition and 100 is ín the other invalid partition. So the boundary is \nbetween the values of 0 and 1, and between the values of 99 and 100. There is a school \nof thought that regards an actual value as a boundary value, By tradition, these are the \nvalues in the valid partition (that is, the values specified). This approach then requires \nthree values for every boundary, so you would have 0, 1 and 2 for the left boundary, \nand 98, 99 and 100 for the right boundary in this example. The boundary values are \nsaid to be on and cïther side of the boundary and the value that is ‘on” the boundary \nis generally taken to be in the valid partition, \not S04 Crmpoge Loy N3 Kighn Bt ol My et b o s o4 Al 0 s 18 ot (b 3 s, . ot oty st ey l ree e B n ol snbis Ak o1 \n€mpert rview e e s / s e s vl g s e (e 4 t t đ N o N € n N N g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 132,
            "page_label": "133"
        }
    },
    {
        "page_content": "120 Chapter 4 Testtechnques \nNote that Beizer [1990] and Kaner, Padmanabhan and Hoffman [2013] talk about \ndomain testing, a generalization of EP, with three-value boundaries. Beizer makes a \ndistinction between open and closed boundaries, where a closed boundary is one where \nthe point is included in the domain. So the convention is for the valid partition to have \nchosed boundaries. You may be pleased to know that you do not have to know this for \nthe exam! Three-value boundary testing ís covered in ISOVIEC/IEEE 29119-4 [2015]. \nSo which approach is best? If you use the two-value approach together with EP, you \nare equally effective and slightly more efficient than the three-value approạch. (We will \nnot go into the details here, but this can be demonstrated.) In this book we will use the \ntwo-value approach. In the exam, you may have a question based on cither the two-value \nor the three-value approach, but it should be clear what the correct choice is in either case. \nDesigning test cases using EP and BVA \nHaving identificd the test conditions that you wish to test, ín this instance by using EP \nand BVA, the next step is to design the test cases. The more test conditions that can be \ncovered in a single test case, the fewer test cases will be needed in order to cover all \nthe conditions. This ís usually the best approach to take for positive tests and for tests \nthat you are reasonably confident will pass. However if a test fails, then we need to \nfind out why it failed. Which test condition was handled incorrectly? We need to get \na good balance between covering too many and too few test conditions in our tests, \nLet’s look at how one test case can cover one or more test conditions. Using the bank \nbalance example, our first test could be of a new customer with a balance of $500. This \nwould cover a balance in the partition from $100.01 to $999.99 and an output partition \nOf a 5% interest rate. We would also be covering other partitions that we have not dis- \ncussed yet, for example a valid customer, a new customer, a customer with only one \naccount, etc. All of the partitions covered in this test are valid partitions. \nWhen we come to test invalid partitions, the safest option is probably 1o try t cover \nonly one invalid test condition per test case. This is because programs may stop process- \ning input as soon as they encounter the first problem. So if you have an invalid customer \nname, invalid address and invalid balance, you may get an error message saying “Invalid \ninput” and you do not know whether the test has detected only one invalid input or all of \nthem. This is also why specific error messages are much better than general ones! \nHowever, if it is known that the software under test is required to process all \ninput regardless of its validity, then it is sensible to continue as before and design \ntest cases that cover as many invalid conditions in one go as possible. For example, \nÍf every invalid ficld in a form has some red text above or below the field saying that \nthis field is invalid and why, then you know that each field has been checked, so you \nhave tested all of the error processing in one test case. In either case, there should be \nseparate test cases covering valid and invalid conditions. \nTo cover the boundary test cases, it may be possible to combine all of the minimum \nvalid boundaries for a group of fields into one test case and also the maximum valid \nboundary values. The invalid boundaries could be tested together if the validation is done \non every ficld: otherwise they should be tested separately, as with the invalid partitions, \nWhy do both equivalence partitioning and boundary value analysis? \nTechnically, because every boundary is in some partition, if you did only BVA you \nwould also have tested every equivalence partition. However, this approach may cause \nproblems if that value fails ~ was it only the boundary value that failed or did the whole \npartition fail? Also by testing only boundaries we would probably not give the users \nmuch confidence as we are using extreme values rather than normal values. The \nboundaries may be more difficult (and therefore more costly) to set up as well. \nG N Crmpoge Lowming. A3 Kiện Bt vt My et b o, s, o4 A, s 18 . o, (b 3 s e, s sty cmto rế b gy B y el b /Ch N el e A e gyl oot s et Sy o el g pericnce CEngage Liwing TS e (W 14 Y adlbnnd o o e o € g 30 (s s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 133,
            "page_label": "134"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Technques 121 \nFor example, in the printer copies example described earlier we identified the \nfollowing boundary values: \nInwalid | Valid | Invalid \nof1 99 | 100 \nSuppose we test only the valid boundary values 1 and 99 and nothing in between. \nIf both tests pass, this seems to indicate that all the values ín between should also \nwork. However. suppose that one page prints correctly, but 99 pages do not. Now we \ndo not know whether any set of more than one page works, so the first thing we would \ndo would be to test for say ten pages, which is a value from the equivalence partition. \nWe recommend that you test the partitions separately from boundaries. This \nmeans choosing partition values that are NOT boundary values. \nHowever, if you use the three-value boundary value approach, then you would \nhave valid boundary values of 1, 2, 98 and 99, so having a separate equivalence value \nin addition to the extra two boundary values would not give much additional benefit. \nBut notice that one equivalence value, for example 10, replaces both of the extra two \nboundary values (2 and 98). This is why EP with two value BVA is more efficient \nthan three-value BVA. \n‘Which partions and boundaries you decide to exercise (you do not need to test \nthem all), and which ones you decide to test first, depends on your test objectives. \n1f your goal is the most thorough approach, then follow the procedure of testing \nvalid partitions first, then invalid partitions, then valid boundaries and finally invalid \nboundaries, However, íÍ you are under time pressure (and who isn't?), then you can't \ntest as much as you would like. Nơw your test objectives will help you đecide what to \ntest. If you are after user confidence of typical transactions with a minimum number \nof tests, you might do valid partitions only. If you want to find as many defects as \npossible as quickly as possible, you may start with boundary values, both valid and \ninvalid. If you want confidence that the system will handle bad inputs correctly, you \nmay do mainly invalid partitions and boundaries. Your previous experience of types \nof defects can help you find similar defects; for example, if there are typically a lot \nof boundary defects, then you would start by testing boundaries. \nEP and BVA are described ín most testing books, including Myers [2011), Copeland \n[2004], Kaner, Padmanabhan and Hoffman [2013] and Jorgensen [2014]. EP and BVA \nare described in ISO/IEC/IEEE 29119-4 [2015], including designing tests and mea- \nsuring coverage. \n4.2.3 Decision table testing \nThe techniques of EP and BVA are often applied to specific situations or inputs. \nHowever, if different combinations of inputs result in different actions being taken, this \ncan be more difficult to show using EP and BVA, which tend to be more focused onthe  Decision table \nuser interface. The other two specification-based techniques. decision table testing  testing A black-box \nand state transition testing, are more focused on business logic or business rules, test technique in \nA decision table is a good way to deal with combinations of things (for example ~ Which test cases are \ninputs). This technique is sometimes also referred to as a ‘cause-effect’ table. The  designed 10 execute the \nreason for this is that there is an associated logic diagramming technique called :'&zmm'?' iputs \n“cause-effect graphing' which was sometimes used to help derive the decision table. Shown ks & decksion \n(Myers describes this as a combinatorial logic network [Myers 2011]). However. most 0 \npeople find it more useful just to use the table described in Copeland [2004]. \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 e s, 0 s 18 . o, U 3 ot e, rn e paty comto rế b rE ee B n el smbs /Ch N e e ee ety / s e T b S eHRSRSINUGMNNSCNSUN GSU GESD (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 134,
            "page_label": "135"
        }
    },
    {
        "page_content": "122 Chapter 4 Test techniques \n1f you begin using decision tables to explore what the business rules are that should \nbe tested. you may find that the analysts and developers find the tables very helpful \nand want to begin using them too, Do encourage this, as it will make your job easier \nin the future. Decision tables provide a systematic way of stating complex business \nrules, which is useful for developers as well as for testers, Decision tables can be \nused in test design whether or not they are used in development, as they help testers \nexplore the effects of combinations of different inputs and other software states that \nmust correctly implement business rules. Helping the developers do a better job can \nalso lead to better relationships with them. \nTesting combinations can be a challenge, as the number of combinations can often \nbe huge. Testing all combinations may be impractical, if not impaossible. We have to \nbe satisfied with testing just a small subset of combinations, but it's not casy to choose \nwhich combinations to test and which not to test. If you don't have a systematic way \nof selecting combinations, an arbitrary subset will be used and this may well result in \nan ineffective test effort, \nDecision tables aid the systematic selection of effective test cases and can have the \nbeneficial side-cffect of finding problems and ambiguities in the specification. It is \na technique that works well in conjunction with EP. The combination of conditions \nexplored may be combinations of equivalence partitions. \n1n addition to decision tables, there are other techniques that deal with testing com- \nbinations of things: pairwise testing and orthogonal arrays. These are described in \nCopeland [2004), Other sources of techniques are Pol, Teunissen and van Veenendaal \n{2001] and Black [2007]. Decision tables and cause—effect graphing are described in \n1SO/IEC/IEEE 29119-4 [2015], including designing tests and measuring coverage. \nUsing decision tables for test design \nThe first task is to identify a suitable function or subsystem that has a behaviour \nwhich reacts according to a combination of inputs or events, The behaviour of interest \nmust not be too extensive (that is, should not contain too many inputs) otherwise the \nnumber of combinations will become cumbersome and difficult to manage. It is better \n1o deal with large numbers of conditions by dividing them into subsets and dealing \nwith the subsets one at a time. \nOnce you have identified the aspects that need to be combined, then you put them \ninto a table, listing all the combinations of True and False for each of the aspects. \nTake an example of a loan application, where you can enter the amount of the monthly \nrepayment or the number of years you want to take 10 pay it back (the term of the \nloan). 1f you enter both, we assume that the system will make a compromise between \nthe two if they conflict. The two conditions are the repayment amount and the term, \n50 we put them in a table (see Table 4.2). \nTABLE 4.2 Empty decision table \nConditions Rule 1 Rule 2 Rule 3 Rule 4 \nG N Crmpogs Loming. A3 Kigh Bt vt My et b o, s, o4 e, 0 s 18 . o, (b 3 i, g, s sty st rế b gy B y el snbs At sl el i M v syl oot s et Sy o el g ericnce CEngage Lewing AT e (W 14 Y adlbmnd o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 135,
            "page_label": "136"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Techriques 123 \nNext, we will identify all of the combinations of True and False (see Table 4.3). With \ntwo conditions, each of which can be Truc or False, we will have four combinations (two \n1o the power of the number of things to be combined). Note that if we have three things to \ncombine, we will have eight combinations, with four things, there are 16, etc. This is why \nit is good to tack ke small sets of combinations a a time. In order to keep track of which \ncombinations we have, we will alternate True and False on the bottom cơw, put two Trues \nand then two Falses on the row above the bottom row, ctc... so the top row will have all \nTrues amd then all Falses (and this principle applies to all such tables). \nTABLE 4.3 Decision table with input combinations \nConditions Rule 1 Rule 2 Rule 3 Rule 4 \nRepayment amount has T T F F \nbeen entered \nTerm of loan has been T F T F \nentered \ne are using T and F for the inputs ín our example; we could also have used \nY and N or Ì (the number one) and 0 (zero) respectively, The conditions can also \nbe numbers, numeric ranges or enumerated types (for example red, green or blue). \nThe next step (at least for this example) is to identify the correct outcome for \neach combination (see Table 4.4). In this example, we can enter one or both of the \ntwo fields. Each combination is sometimes referred to as a rule. In this cxample, \nwe are using Y for the actions which should occur and leaving it blank if that action \nshould not occur. Other options are to use X or 1 if an action should occur, and \nN.E. — or 0 for actions that should not occur. We could use a number or range of \nnumbers if an outcome occurs for those numbers and does not occur for others. \nYou could also use discrete values, for example an action should occur if an input \nis red, which does not occur if it is green or yellow, \nTABLE 4.4 Decision table with combinations and outcomes \nConditions Rule 1 Rule 2 Rule 3 Rule 4 \nRepayment amount has T T F F \nbeen entered \nTerm of loan has been T F T F \nentered \nActions/Outcomes \nProcess loan amount Y Y \nProcess term L ¥ \nCopr N Crmpoge 129056 A3 K Bt vt My et b o, s, o4 b . 0 s 10 . o, (b 3 i, e, ot sty cmto rế b E ee B y el b At e e ee 9 n commem s e - S IN S HSN GSU IG EIEGSD EH",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 136,
            "page_label": "137"
        }
    },
    {
        "page_content": "124 Chapter 4 Test techniques \nAt thịs point, we may now realize that we had not thought about what happens if \nthe customer doesn't enter anything in cither of the two fields. The table has high- \nlighted a combination that was not mentioned in the specification for this example, \nWe could assume that this combination should result in an error message, so we need \n1o add another action (see Table 4.5), This highlights the strength of this technique to \ndiscover omissions and ambiguities in specifications. It s not unusual for some com- \nbinations to be omitted from specifications: therefore this is also a valuable technique \n10 use when reviewing the test basis. \nTABLE 4.5 Decision table with additional outcome \nConditions Rule 1 Rule 2 Rule 3 Rule 4 \nRepayment amount has T T E F \nbeen entered \nTerm of loan has been T F T F \nentered \nActions/Outcomes \nProcess loạn amount Y Y \nProcess term Y Y \nError message Y \nSuppose we change our example slightly, so that the customer is not allowed to \nenter both repayment and term. Now our table will change, because there should also \nbe an error message if both are entered, so it will look like Table 4.6. \nTABLE 4.6 Decision table with changed outcomes \nConditions Rule 1 Rule 2 Rule 3 Rule 4 \nRepayment amount has T X F F \nbeen entered \nTerm of loạn has been T F T F \nentered \nError message Y Y \nG N Compoge Loy A3 gt Bt My et n o, s, o4 e n 0 s 18 . o, (b 3 st e, rn sty krsnee rế b sy ee y el snbs At e e ee et n H et x G u SGsHEAINUGMNNSUNUIN GG o € whacacm YIS (70T s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 137,
            "page_label": "138"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Technques 125 \nYou might notice now that there is only one Yes Ín each column, that is, our actions \nare mutually exclusive: only one action occurs for each combination of conditions. We \ncould represent this in a different way by listing the actions in the cell of one row, as \nshown in Table 4.7. Note that if more than one action results from any of the combi- \nnations, then it would be better to show them as separate rows rather than combining \nthem into one row. \nTABLE 4.7 Decision table with outcomes in one row \nConditions Rule 1 Rule 2 Rule 3 Rule 4 \nRepayment amount T T F F \nhas been entered \nTerm of loan has T F T F \nbeen entered \nActions/Outcomes \nResult Error Process loan Process Error \nmessage amount term message \nThe final step of this technique is to write test cases to exercise each of the four \nrules in our table. \nIn this example we started by identifying the input conditions and then identifying \nthe outcomes. However, in practice it might work the other way around — we can see \nthat there are a number of different outcomes and have to work back to understand \nwhat combination of input conditions actually drive those outcomes. The technique \nworks just as well doing it ín this way and may well be an iterative approach as you \ndiscover more about the rules that drive the system. \nCredit card worked example \nLets look at another example. If you are a new customer opening a credit card \naccount, you will get a 15% discount on all your purchases today. If you are an \nexisting customer and you hold a loyalty card, you get a 106 discount. lf you have \na coupon, you can get 20% o£f today (but it cannot be used with the new customer \ndiscount). Discount amounts are added, if applicable. This is shown in Table 4.8. \nIn Table 4.8, the conditions and actions are listed in the left-hand column. All \nthe other columns in the decision table each represent a separate rule, one for each \ncombination of conditions. We may choose 10 test cách rule/combination and if \nthere are only a few, this will usually be the case. However, if the number of rules/ \ncombinations is large we are more likely to sample them by selecting a rich subset \nfor testing. \nNote that we have put X for the discount for two of the columns (Rules | and 2). \nThis means that this combination should not occur, You cannot be both a new cus- \ntomer and already hold a loyalty card! There should be an error message stating \nthis, but even if we don't know what that message should be, it will still make a \ngood test, \nG N Compoge Lonsming. A3 Kgh Bt vt My et b o, s, o4 b i, 0 s 18 . o, U 3 o, e, s sty comtos rế b sy B n el smbs At n H i e b s - S IN SGIHERNUMNNCUN GG oGsbuobDooueeurg",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 138,
            "page_label": "139"
        }
    },
    {
        "page_content": "126 Chapter 4 Test techniques \nTABLE 4.8 Decision table for credit card example \nConditions Rule1 Rule2 Rule3 Rule4 Rule5 Rule6 Rule7 Rule8 \nNew customer (15%) T T N K F F F F \nLoyalty card (10%) T T F F T T F F \nCoupon (20%) 5 7 2 F T F T F \nActions \nDiscount (%) x X 20 15 30 10 20 0 \nBecause we have exactly the same action (and presumably the same error message) \nfor both columns | and 2, we can see that having a coupon makes no difference to \nthe øutcome, that is, we do not care whether you have a coupon or not, as it makes \nno difference t the actions or outcome. This means that we can slightly collapse or \nshorten the table as shown in Table 4.9. We have put a dash () to show that it does \nnot matter whether coupon is True or False. This is not very significant for such a \nsmall example. We could also put *N/A' (Not Applicable) to show that this value \ndoes not affect the outcome. We can combine two or more columns in this way, as \nlong as they all have the same resulting actions or outcomes. See Copeland [2004] \nfor more information. \nWith more complex decision tables, being able to collapse the table by combining \ncolumns (or deleting a column) can make the table much easier to read and under- \nstand, For clarity, we have just removed “Rule 2` and kept the other rule numbers as \nthey were. This could be changed to make the rule numbers sequential in the final \ntable. More information about collapsing decision tables is covered in the ISTQB \nAdvanced Level Test Analyst Syllabus. \nTABLE 4.9 Collapsed decision table for credt card example \nConditions Rule 1 Rule 3 Rule 4 Rule 5 Rule 6 Rule 7 Rule 8 \nNew custorner (15%) T T ự F F F F \nLoyalty card (10%) 1 F F T 1/ F F \n'Coupon (20%) - T F T F T F \nActions \nDiscount (%) X 20 15 30 10 20 0 \nWe have made an assumption in Rule 3. Since the coupon has a greater discount \nthan the new customer điscount, we assume that the customer will choose 20% rather \nthan 15%. We cannot add them, since the coupon cannot be used with the new cus- \ntomer discount. The 20% action is an assumption on our part, and we should check \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 B, 0 s 10 . o, (b 3 s, g, rn sty st rế b gy B y el snbs At R ee 6 n H e et - S N Conpuge Linmay muries e 140 10 ey koo GSU GG GGSD UCS",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 139,
            "page_label": "140"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Technques 127 \nthat this assumption (and any other assumptions that we make) is correct, by asking \nthe person who wrote the specification or the users. \nFor Rule 5, however, we can add the discounts, since both the coupon and the \nloyalty card discount should apply (at least that is our assumption). \nRules 4, 6 and 7 have only one type of discount and Rule 8 has no discount, \nso is 0%. \nIf we are applying this technique thoroughly. we would have onc test for each \ncolumn or rule of our decision table. The advantage of doing this is that we may test \na combination of things that otherwise we might not have tested and that could find \na defect, Coverage of decision table testing is measured by the number of columns \nthat have at least one test case, divided by the total number of columns. \nHowever, if we have a lot of combinations, it may not be possible or sensible to test \nevery combination, Or if we are time-constrained, we may not have time to test all \ncombinations. Do not just assume that all combinations need to be tested: it is better \n1o prioritize and test the most important combinations. Having the full table enables \nus to see which combinations we decided to test and which not to test this time. \nThere may also be many different actions as a result of the combinations of condi- \ntions. In the example above we just had one: the discount to be applied. The decision \ntable shows which actions apply to cach combination of conditions. \nIn the example above all the conditions are binary, that is, they have only two \npossible values: True or False (or, if you prefer Yes or No). Often it is the case that \nconditions are more complex, having potentially many possible values. Where this is \nthe case the number of combinations is likely to be very large, so the combinations \nmay only be sampled rather than exercising all of them. \n4.2.4 State transition testing \nState transition testing is used where some aspect of the system can be described  State transition \nin what is called a ‘finite state machine' This simply means that the system can be  testing (finite state \nin a limited (finite) number of different states, and the transitions from one state to  testing) A black-box \nanother are determined by the rules of the ‘machine’. This is the model on which the - 165t technique using a \nsystem and the tests are based. Any system where you get a different output for the state transition diagram \nsame input, depending on what has happened before, is a finite state system. A finite \nstate system is often shown as a state diagram (see Figure 4.2). \nFor example. if you request to withdraw $100 from a bank ATM, you may be m‘;’gﬁ:ﬂ \ngiven cash. Later you may make exactly the same request but be refused the money - valg transitions \n(because your balance is insufficient). This later refusal is because the state of your  and blocks invalid \nbank account has changed from having sufficient funds to cover the withdrawal 10 transitions. \nhaving insufficient funds. When the same event can result in two different transitions, \ndepending on some True/False condition (in this case sufficient funds), this is referred \n1o as a ‘guard condition” on the transition, The funds will be dispensed only when the \nguard condition of sufficient funds is True. Guard conditions are shown on a state \ndiagram in square brackets by the transition that they are guarding. The transaction \nthat caused your account to change its state was probably the earlier withdrawal. \nA state diagram can represent a model from the point of view of the system, the \naccount or the customer. \nAnother example is a word processor, ÍÍ a document is open, you are able to close \nÍt. If no document is open, then Close is not available. After you choose Close once, \nyou cannot choose it again for the same document unless you open that document. \nA document thus has two states: open and closed. \nG N Crmpoge Loswming. A3 K Bt vt My et b o, s, o4 B s, 0 s 18 . o, U 3 i e, ot sty conmo ey b gy B n el smbs At e e ee ety / s e T b Conprge Lonmay muries e 1y 10 ey koo o o 1y o € whbacacm A (74T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 140,
            "page_label": "141"
        }
    },
    {
        "page_content": "128 Chapter 4 Testtechnques \nA state transition model has four basic parts: \n® The states that the software may occupy (open/closed or funded/insufficient \nfunds). \n® The transitions from one state to another (not all transitions are allowed). \n® The events that cause 4 transition (closing a file or withdrawing money). \n® The actions that result from a transition (an error message or being given \nyour cash). \nNote that in any given state, one event can cause only one action, but that the same \nevent from a different state may cause a different action and a different end state. \nWe will look first at test cases that execute valid state transitions. \nFigure 4.2 shows an example of entering a PIN to a bank account. The states are \nshown as circles, the transitions as lines with arrows and the events as the text near \nthe transitions. We have not shown the actions explicitly on this diagram, but they \nwould be a message to the customer saying things such as \"Please enter your PIN'. \nFIGURE 4.2 State diagram for PIN entry \nThe state diagram shows seven states but only four possible events (Card inserted, \nEnter PIN, PIN OK and PIN not OK), We have not specified all of the possible transitions \nhere: there would also be a time-out from “wait for PIN' and from the three tries. The sys- \ntem would go back to the start state after the time had elapsed and would probably eject \nthe card. There would also be a transition from the 'cat card' state hack to the start state. \nWe have not specified all the possibke events either ~ there would be a 'cancel' option \nfrom *wait for PIN' and from the three tries, which would also go back to the start state \nand eject the card. The \"access account” state would be the beginning of another state \ndiagram showing the valid transactions that could now be performed on the account. This \nstate diagram, even though it is incomplete, still gives us information on which to design \nsome useful tests and to explain the state transition technique, \nIn deriving test cases, we may start with a typical scenario. A sensible first test \ncase here would be the normal situation, where the correct PIN is entered the first \ntime. To be more thorough, we may want to make sure that we cover every state \n(that is, at least one test goes through cách state) or we may want to cover every \ntransition. A second test (1o visit every state) would be to enter an incorrect PIN \no S Crmpogs Loy N3 Kighh Bt vl Moy et x oo, . o0 Bl 0 b 18 o, U 3 i, e, o sty st ey l nrp ee B n ol snbis At i el e M v gyt oot s e Sy 4 el Wy pericncs CEngage Ly x e (W 14 s n kh ee o o n o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 141,
            "page_label": "142"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Technques 129 \neach time, so that the system eats the card, We still have not tested every transition \nyet. In order to do that, we would want a test where the PIN was incorrect the first \ntime but OK the second time, and another test where the PIN was correct on the \nthird try. These tests are probably less important than the first two. \nNote that a transition does not need to change 10 a different state (although all of \nthe transitions shown above do go to a different state). There could be a transition \nfrom ‘access account' which just gocs back to 'access account for an action such as \n‘request balance’. \nTest conditions can be derived from the state diagram ín various ways. Each state \ncan be noted as a test condition, as can each transition, In the Syllabus, we need to be \nable to identify the coverage Of a set Of tests in terms Of states Of transitions. \nTests can be designed to cover a typical sequence of states, to visit all states, to \nexercise every transition, to exercise specific transition sequences, or to test invalid \ntransitions (sec below). \nGoing beyond the level expected in the Syllabus, we can also consider transition \npairs and triples and so on. Coverage of all individual transitions is also known as \n0-switch coverage, coverage of transition pairs ís 1-switch coverage, coverage of \ntransition triples is 2-switch coverage, ete. Deriving test cases from the state transition \nmodel is a black-box approach. Measuring how much you have tested (covered) is \ngetting close 1o a white-box perspective. However, state transition testing is regarded \nas a black-box technique. More information on coverage criteria for state transition \ntesting is covered in the ISTQB Advanced Level Test Analyst Syllabus. \nOne of the advantages of the state transition technique is that the model can be \nas detailed or as abstract as you need it to be. Where a part of the system is more \nimportant (that Ís, requires more testing) a greater depth of detail can be modelled. \nWhere the system is less important (requires less testing), the model can use a single \nstate to signify what would otherwise be a series of different states, \nTesting for invalid transitions \nDeriving tests only from a state diagram or chart (also known as a state graph or \nchart) is very good for seeing the valid transitions, but we may not easily see the nega- \ntive tests, where we try to generate invalid transitions. In order to see the total number \nof combinations of states and transitions, both valid and invalid, a state table ís useful. \nThe state table lists all the states down one side of the table and all the events \nthat cause transitions along the top (or vice versa). Each ccll then represents a state- \nevent pair. The content of each cell indicates which state the system will move to \nwhen the corresponding event occurs while in the associated state. This will include \npossible erroneous events = events that are not expected to happen in certain states, \nThese are negative test conditions. \nTable 4.10 lists the states in the first column and the possible events across the top \nrow. So, for example, if the system is in State |, inserting a card will take it to State 2. \nIf we are in State 2, and a valid PIN is entered, we go to State 6 to access the account. \nIn State 2 Íf we enter an invalid PIN, we go to State 3. We have put a dash in the cells \nthat should be impossible, that is. they represent invalid transitions from that state. \n‘We have put a question mark for two cells, where we enter either a valid or invalid \nPIN when we are accessing the account. Perhaps the system will take our PIN number \nas the amount of cash to withdraw? It might be a good test! Most of the other invalid \ncells would be physically impossible in this example. Invalid (negative) tests will \nattempt to generate invalid transitions, transitions that should not be possible (but \noften make good tests when it turns out they are possible). \noyt S Crmpogs Lo N3 B Bt vl Moy et b o, s o4 A 0 b 18 ot (b 3 o, . ot st ey l n\"g s B n n ol snbis gt i el e A Av gyt oot s e Sy o el ey eTicncs CEngag L BTG e (W 14 Y ol o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 142,
            "page_label": "143"
        }
    },
    {
        "page_content": "130 Chapter 4 Test techniques \nTABLE 4.10 State table for the PIN example \nInsert card Valid PIN Invalid PIN \n51) Start state s2 - - \n52) Wait for PIN - 56 s3 \n53) 1st try invalid - s % \n54) 2nd try invalid = S6 S5 \n55) 3rd try invalid = = s7 \n56) Access account - ? ? \n57) Eat card 51 (for new card) = K \nState trunsition testing is used for screen-based applications, for example an ATM, \nand also within the embedded software industry. It is useful for modelling business \nscenarios which have specific states, or for testing navigation around screens. \nA more extensive description of state machines is found ín Marick [1994]. State \ntransition testing is also described in Craig and Jaskiel [2002], Copeland [2004], \nBeizer [1990]. Brockman and Notenboom {2003], and Black [2007]. State transition \ntesting is described in ISOVIEC/IEEE 29119-4 [2015]. including designing tests and \ncoverage measures, \n4.2.5 Use case testing \nUse case testing Use case testing is a technique that helps us identify test cases that exercise the whole \n(scenario testing, ưser system on a transaction by transaction basis from start to finish, They are described \nscenario testing) by Ivar Jacobson in his book Object-Oriented Software Engineering: A Use Case \nAN“,H’“‘ test Driven Approach [Jacobson 1992]. Information on use cases can also be found at \n;f\"s\"\"\"‘: T E Omgong in UML 2.5 |2017). \nQusoáe scanarios of A use case is a description of a particular use of the system by an actor (a human \nuser of the system, external hardware or other components or systems). Each use case \ndescribes the interactions the actor has with the subject (i.e. the component or system \nto which the use case is applied), ín order to achieve a specific task (or, at least, produce \nsomething of value to the actor). Use cases are a sequence of steps that describe the \ninteractions between the actor and the subject. A use case is a specific way of designing \ninteractions with software items, incorporating requirements for the software functions \nrepresented by the use cases. \nEach use case specifies some behaviour that a subject can perform in collaboration \nwith one or more actors, The interactions between the actors and the subject may \nresult ín changes to the state of the subject. These interactions can be represented \ngraphically by workflows activity diagrams or business process models. They can \nalso be described as a series Of steps or even in natural language. \nUse cases are defined ín terms of the actor, not the system, describing what the \nactor does and what the actor sees rather than what inputs the system expects and \nuse cases. \nG N Compoge Lonsming. A3 Kin Bt vt My et b o, s, o4 b s, 0 s 18 n o, U 3 ot g, s sty conto rế b sy B n el snbs At i e b s / st - S N SGIHERNSNNSCNUN GSU IGNGSGEGSD (74T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 143,
            "page_label": "144"
        }
    },
    {
        "page_content": "Section 2 Black-Box Test Technques 131 \nwhat the system outputs. They often use the language and terms of the business \nrather than technical terms, especially when the actor is a business user. They serve \nas the foundation for developing test cases, mostly at the system and acceptance \ntesting levels. \nUse cases can uncover integration defects, that is, defects caused by the incorrect \ninteraction between different components. Used in this way, the actor may be some- \nthing that the system interfaces to such as a communication link or subsystem. \nUse cases describe the process flows through a system based on its most likely \nuse. This makes the test cases derived from use cases particularly good for finding \ndefects in the real-world use of the system (that is, the defects that the users are \nmost likely to come across when first using the system). Each use case usually has a \nmainstream (or most likely) scenario and sometimes additional alternative branches \n(covering, for example, special cases or exceptional conditions and error conditions, \nsuch as system response and recovery from errors in programming. the application \nand communication), Each use case must specify any preconditions that need to be \nmet for the use case to work. Use cases must also specify postconditions that are \nobservable results and a description of the final state of the system after the use case \nhas been executed successfully, \nThe PIN example that we used for state transition testing could also be defined \nin terms of use cases, as shown in Figure 4.3. We show a success scenario and the \nextensions (which represent the ways in which the scenario could fail to be a success). \nFor use case testing, we would have a test of the success scenario and one test for \neach extension. In this example, we may give extension 4b a higher priority than a \nfrom a security point of view. \nSystem requirements can also be specified as a set Of use cases, This approach \ncan make it easier to involve the users in the requirements gathering and defini- \ntion process, \nStep |Description \nA: Inserts card \nMuễn Success: Scanaric S: Validates card and asks for PIN \nA: Actor sS A Enters PIN \nS: Validates PN \nwla|lw|~ S: Allows access to account \nCard not vakd \nS: Display message and reject card \nPIN not valid \nS: Dsplay message and ask for re-try (twice) \nPIN invalid 3 times \nS S: Eat card and exit \nFIGURE 4.3 Partial use case for PIN entry \nG N Crmpoge Lossming. A3 Kn Bt vt My et b o, s, o4 b . 0 s 10 . o, U 3 s, g, s s paty comto rế b sy B n el smbs At vl e a8 1 appresact comm dres ey S8 el GD AƯS Loway BuTSCs u IN kot o o 1 u G M 0 g CỆ",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 144,
            "page_label": "145"
        }
    },
    {
        "page_content": "132 Chapter 4 Test techniques \nCoverage of use case testing can be measured by the percentage of use case \nbehaviours tested divided by the total number of use case behaviours. It is possible \n10 measure coverage of normal behaviour, exceptional and/or error behaviour, or all \nbehaviours. More information on coverage criteria for use case testing is covered in \nthe ]STQB Advanced Level Test Analyst Syllabus, \n4.3 WHITE-BOX TEST TECHNIQUES \nLLABUS LEARNING OBJECTIVES FOR 4.3 WHITE-BOX \nTEST TECHNIQUES (K2) \nFL-43.1 Explain statement coverage (K2) \nFL-432 Explain decision coverage (K2) \nFL-433 Explain the value of statement and decision coverage (K2) \nIn this section we will look ín detail at the concept of coverage and how it can be \nused to measure some aspects of the thoroughness of testing. In order to see how \ncoverage actually works, we will use some code-level examples (although coverage \nalso applies to other levels such as business procedures). In particular, we will show \nhow to measure coverage of statements and decisions, and how to write test cases to \nextend coverage Íf it is not 100%, \nAs mentioned, we will illustrate white-box test techniques that would typically \napply at the component level of testing. At this level, white-box techniques focus \non the structure Of a software component, such as statements, decisions, branches \nor even distinct paths. These techniques can also be applied at the integration \nlevel. At this level, white-box techniques can examine structures such as a call \ntree, which is a diagram that shows how modules call other modules. These tech- \nniques can also be applied at the system level. At this level, white-box techniques \ncan examine structures such as a menu structure, business process or web page \nstructure. \nThere are also more advanced white-box test techniques at component level, par- \nticulariy for safety-critical, mission-critical or high-integrity environments to achieve \nhigher levels of coverage, For more information on these techniques see the ISTQB \nAdvanced Level Technical Test Analyst Syllabus. \nIn this section, look for the definitions of the Glossary terms decision coverage \nand statement coverage. \nWhite-box test techniques serve two purposes: coverage measurement and \nstructural test case đesign, They are often used first to assess the amount of testing \nperformed by tests derived from black-box test techniques, that is, to assess coverage. \nThey are then used to design additional tests with the aim of increasing the coverage. \nWhite-box test techniques used to design tests are a good way of generating addi- \ntional test cases that are different from existing tests. They can help ensure more \nbreadth of testing, in the sense that test cases that achieve 100% coverage in any \nmeasure will be exercising all parts of the software from the point of view of the \nitems being covered. \nG N Crmpoge Loswming. A3 Kinn Bt vt My et b o, s, o4 b . 0 s 18 . o, U 3 i, e, s sty st rế b gy B n el smbs At sl I s commem s et - S u Conpge! — I — U _- M",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 145,
            "page_label": "146"
        }
    },
    {
        "page_content": "Section 3 White-Box Test Technques 133 \nWhat is coverage? \nCoverage measures the amount of testing performed by a set of tests that may have \nbeen derived in a different way, for example using black-box techniques. Wherever \nwe can count things and can tell whether or not each of those things has been tested \nby some test, then we can measure coverage. The basic coverage measure is: \nNumber of coverage items exercised x100% \nTotal number of coverage items \nCoverage = \nwhere the coverage item is whatever we have been able to count and see whether a \ntest has exercised or used this item, \nA Christmas gift to one of the authors was a map of the world where you can \nscratch off the countries you have visited. This is a very good analogy for coverage. \nlf 1 visit a small country, for example Belgium, it seems quite OK to scratch off the \nwhole country. But if 1 visit the US, Canada, Australia, China or Russia, then do | \nseratch off the whole country? Even if 1 have only visited one or a few cities, | can \nstill count the whole country and this seems not quite right. But this would be country \ncoverage. | can count the countries Í have visited, divided by the total number of \ncountries in the world, and get a percentage, for example around 18% for visiting \n36 countries. However, if you look at the map, it looks like I have covered around 25% \nof the area (since ] have been to the US, Canada, Australia and Chína). lf | were to do \narca-of-the-map coverage, Í would get a higher coverage percentage, but 1 would not \nhave seen any more of the world. lf ] now visit St Petersburg in Russia, | increase my \ncountry coverage by less than 1%, but increase my area coverage by 12.5%! Why are \nwe talking about this map? Because it highlights some of the same aspects and pitfalls \nOf coverage for components and systems, \nThere are several dangers (pitfalls or caveats) in using coverage measures: \n® 100% coverage does nor mean 1005 tested! Coverage techniques measure only \none dimension of a multi-dimensional concept. Country coverage is a higher \nlevel than state, county or city coverage. \n® Two different test cases may achieve exactly the same coverage but the input \ndata of one may find an error that the input data of the other does not. lf you do \nnot execute a line or block of code that contains a bug, you are guaranteed not \n1o see the failures that bug can cause. However, just because you did execute \nthat line or block, does not guarantee that you will see the failures that bug can \ncause, There may be many different data combinations that exercise the same \nTrue/False decision outcome, but some will cause a failure and others will not. \n® Coverage looks only at what hias been written, that is the code itself. It cannot \nsay anything about the software that has not been written. Tf a specified \nfunction has not been implemented, black-box test techniques will reveal \nthis, If a function was omitted from the specification, then experience-based \ntechniques may find it. But white-box techniques can only look at a structure \nwhich is already there. \n® Just because some coverage item has been covered, this does NOT mean \nthat this part of the system is actually doing what it should. Coverage only \nassesses whether or not you have exercised something, not whether that test \npassed or failed or whether it was a good test worth running. Coverage says \nnothing about the quality of cither the system or the tests. \nL e M Conpg i, 4 s e My v e e o Ayt 1 ek o Do e s Bk s v s rn b n ry b sl e e a et — ì Congage Lisrming K e rN 1 vy kh eHe o ey o € mhnnghem 440 e , s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 146,
            "page_label": "147"
        }
    },
    {
        "page_content": "134 Chapter 4 Testtechnques \nSo Ís coverage worth measuring? Yes, coverage can b useful. It is a way of assess- \ning one aspect of thoroughness. But it is best used when you understand exactly what \nyou are measuring, and are aware of the pitfalls, particularly if you are reporting \ncoverage to stakeholders. \nTypes of coverage \nCoverage can be measured based on a number of different structural elements in a \nsystem or component. Coverage can be measured at component testing level, inte- \ngration testing level or at system or acceptance testing levels. For example, at system \nor acceptance level, the coverage items may be requirements, menu options, screens \nor typical business transactions. Other coverage measures include things such as \ndatabase structural clements (records, fields and sub-ficlds) and files. It is worth \nchecking for any new tools, as the test tool market develops quite rapidly. \nAt integration level, we could measure coverage of interfaces of specific interac- \ntions that have been tested. The call coverage of APIs, modules, objects or procedure \ncalls can also be measured (and is supported by tools to some extent). \nThere is good tool support for code coverage, that is, for component-level testing, \nWe can measure coverage for each of the black-box test techniques as well: \n® Equivalance partitioning: percentage of equivalence partitions exercised (we \ncould measure valid and invalid partition coverage separately if this makes sense). \n® Boundary value analysis: percentage of boundaries exercised (we could also \nseparate valid and invalid boundaries if we wished). \n® Decision tables: percentage of business rules or decision table columns tested. \n® State transition testing: there are a number of possible coverage measures: \n- Percentage of states visited. \n— Percentage of (valid) transitions exercised (this is known as Chow’s 0-switch \ncoverage). \n- Percentage of pairs of valid transitions exercised ('transition pairs' or Chow’s \n1-switch coverage) — and longer series of transitions, such as transition \ntrịples, quadruples, etc, \n— Percentage of invalid transitions exercised (from the state table). \n“The coverage measures for black-box tcchniques would apply at whichever test \nlevel the technique has been used (for example system or component level). \nWhen coverage is discussed by product owners, business analysts, system testers \nor users, it most likely refers to the percentage of requirements that have been tested \nby a set of tests. This may be measured by a tool such as a requirements management \ntool or a test management tool, \nHowever, when coverage is discussed by programmers, it most likely refers to \nthe coverage of code, where the structural elements can be identified using a tool. \nWe will cover statement and decision coverage shortly, However, at this point, note \nthat the word coverage is often misused to mean, \"How many or what percentage of \ntests have been run?” This is NOT what the term coverage refers to, Coverage is the \ncoverage of something else by the tests. The percentage of tests run should be called \ntest completeness or something similar. \nStatements and decision outcomes are both structures that can be measured in \ncode and there is good tool support for these coverage measures. Code coverage \nG N Compogs Lowming A3 g Bt vt My et b o, s, o4 A s, 0 s 10 . o, (b 3 st e, s s paty conto rế b gy B y el b At s el e M e gyt oot s e Sy o el ey ericncs Cengg L AT e (W 14 Y albnnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 147,
            "page_label": "148"
        }
    },
    {
        "page_content": "Section 3 White-Box Test Technques 135 \nis normally done in component and component integration testing, if it is done at \nall. 1f someone claims to have achieved code coverage. it is important to establish \nexactly what elements of the code have been covered, as statement coverage (often \nwhat is meant) is significantly weaker than decision coverage or some of the other \ncode coverage measures, \nHow to measure coverage \nFor most practical purposes, coverage measurement is something that requires tool \nsupport. However, knowledge of the steps typically taken to measure coverage is \nuseful ín understanding the relative merits of each technique, Our example assumes an \nintrusive coverage measurement tool that alters the code by inserting instrumentation: \n1 Decide on the structural element to be used, that is, the coverage items to be \ncounted, \n2 Count the structural elements or items. \n3 Instrument the code. \n4 Run the tests for which coverage measurement is required. \n5 Using the output from the instrumentation, determine the percentage of elements \nor items exercised. \nInstrumenting the code (step 3) involves inserting code alongside each structural \nelement in order to record when that structural element has been exercised, Determin- \ning the actual coverage measure (step 5) is then a matter of analyzing the recorded \ninformation. \nCoverage measurement of code is best done using tooks (as described in Chapter 6) and \nthere are a number of such tools on the market. These tools can help to support increased \nquality and productivity of testing. They support increased quality by ensuring that more \nstructural aspects are tested, so defects on those structural paths can be found (and fixed, \notherwise quality has not increased ). They support increased productivity and efficiency \nby highlighting tests that may be redundant, that is, testing the same structure as other \ntests, although this is not necessarily a bad thing, since we may find a defect testing the \nsame structure with different data. \nIn common with all white-box test techniques, code coverage techniques are best \nused on areas of software code where more thorough testing is required, Safety- \ncritical code, code that is vital to the correct operation of a system, and complex \npieces of code are all examples of where white-box techniques are particularly \nworth applying. For example, some standards for safety-critical systems such as avi- \nonics and vehicle control, require white-box coverage for certain types of system. \nWhite-box test techniques should normally be used in addition to black-box and \nexperience-based test techniques rather than as an alternative to them. \nWhite-box test design \nlf you are aiming for a given level of coverage (say 95%) but you have not reached your \ntarget (for example you only have 87% so far), then additional test cases can be designed \nwith the aim of exercising some or all of the structural elements not yet reached. This is \nwhite-box or structure-based test design. These new tests are then run through the instru- \nmented code and a new coverage measure is calculated. This is repeated until the required \ncoverage measure is achieved (or until you decide that your goal was too ambitious!). \nIdeally all the tests ought to be run again on the un-instrumented code. \nG N Compoge Lowming. A3 K Bt vt My et b o, s, o4 e, 0 s 16 . o, U 30 i e, s sty comto rế b gy B n el smbis /Ch v eview e e e n s et — ì SGIeHUSSINGMNNSCNUN GG € whacgacs YIS (74T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 148,
            "page_label": "149"
        }
    },
    {
        "page_content": "136 Chapter 4 Test techniques \nWe will look at some examples of structure-based coverage and test design for \nstatement and decision testing below. \n4.3.1 Statement testing and coverage \nStatement coverage Statement coverage is calculated by: \nThe of \ne Sty Statement coverage = NTber f staements exercised 1010 \nthat have been Total number of statements \nexercised by a test suite. Studies and experience in the industry have indicated that what is consid- \nered reasonably thorough black-box testing may actually achieve only 60% to \n75% statement coverage. Typical ad hoc testing is likely to achieve only around \n30%, leaving 70% of the statements untested. \nDifferent coverage tools may work in slightly different ways, so they may give \ndifferent coverage figures for the same set of tests on the same code, although at \n1004 coverage they should be the same. \nWe will illustrate the principles of coverage on code. In order to explain our \nexamples, we will use two types of code examples, one a basic pseudo-code — this \nis not any specific programming language, but should be readable and under- \nstandable to you, even if you have not done any programming yourself — and the \nsecond is more like javascript. Both give the same control flow. We have omitted \nthe set-up code that is needed to actually run the code, to concentrate on the logic, \nFor example, consider Code samples 4.1a and 4.1b. \nREAD A \nREAD B \nIFA>BTHENC = 0 \nENDIF \nCode sample 4.la \nlet a = Number(args(2]) \nlet b = Number(args(3]) \nif (a > b) |{ \nc=0 \n} \nCode sample 4.1b \nTo achieve 100% statement coverage of this code segment just one test case is \nrequired. one which ensures that variable A contains a value that is greater than the \nvalue of variable B, for example, A = 12 and B = 10. Note that here we are doing \nstructural test design first, since we are choosing our input values in order to ensure \nstatement coverage. \nLet’s look at an example where we measure coverage first. In onder to simplify the \nexample, we will regard cach line as a statement. Different tools and methods may \ncount different things as statements, but the basic principle is the same however they are \ncounted. A statement may be on a single line, or it may be spread over several lines. \nOne line may contain more than one statement, just one statement or only part of a state- \nment. Some statements can contain other statements inside them. In Code samples 4.2, \nwe have two read statements, one assignment statement and then one IF statement on \nthree lines, but the IF statement contains another statement (print) as part of it \nG N Compogs Lowming. A3 Kt Bt My et b o, s, o4 b i, s 10 . o, U 3 i g, st s sty comto ey b smpppened B e el smbs At el s M e gyt oot s ot Sy 4 B el g pericncs CEngage Ly BTN e (W 14 Y adibacnd o o e o € g 30 (s e",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 149,
            "page_label": "150"
        }
    },
    {
        "page_content": "Section 3 White-Box Test Techriques 137 \nPRINT 'Large C' \nENDIF \nCode sample 4.2a \nA s w N \n- \nb \nn \nv \n“ o \nlet a = Number(args(2]) \nlet b = Number (args(3]) \nlet ¢ = a + 2*b \nif (c » 50) { \nconsole.log('C large’) \n} \nCode sample 4.2b \nAlthough it is not completely correct, we have numbered cach line and will regard \neach line as a statement. Some tools may group statements that would always be \nexecuted together in a basic block which is regarded as a single statement. How- \never, we will just use numbered lines 1o illustrate the principle of coverage of \nstatements (lines). Let’s analyze the coverage of a set of tests on our six-statement \nprogram: \nTEST SET | \nowmawN \ne \nTest1_ LA=2,8=3 \nTest1 22A=0,8=25 \nTest1 _3:A=47,8=1 \n‘Which statements have we covered? \n® InTest I_1, the value of C will be 8, so we will cover the statements on lines \n1 to 4 and line 6. \n® InTest 1_2, the value of C will be 50, so we will cover exactly the same \nstatements as Test 1_1 \n@ InTest 1_3, the value of C will be 49, so again we will cover the same \nstatements. \nSince we have covered five out of six statements, we have 83% statement coverage \n(with three tests). What test would we need in order to cover statement 3, the one \nstatement that we haven't exercised yet? How about this one: \nTest1.4:A=208=25 \nThis time the value of C is 70, so we will print ‘Large C* and we will have exer- \ncised all six of the statements, so now statement coverage = 100%. Notice that we \nmeasured coverage first, and then designed a test to cover the statement that we had \nnot yet covered. \nNote that Test 1_4 on its own is more effective (towards our goal of achieving \n100% statement coverage) than the first three tests together. Just taking Test 1_4 on \nits own is also more efficient than the set of four tests, since it has used only one test \ninstead of four. Being more effective and more efficient is the mark of a good test \ntechnique. \no N4 Compogs Loy N3 Bt Bt vl Moy et b o, s, o4 b 0 bk 18 o, (ke 3 i, . ot st ey b gt B e ol snbis At s el i A e gyt oo s et Sy 48 el ey pericncs CEngage Liwing BTN e (W 14 Y adlBicnd o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 150,
            "page_label": "151"
        }
    },
    {
        "page_content": "138 Chapter 4 Test techniques \n4.3.2 Decision testing and coverage \nA decision is an IF statement, a loop control statement (for example DO-WHILE or \nREPEAT-UNTIL), or « CASE statement, where there are two or more possible exits \nor outcomes from the statement. With an IF statement, the exit can either be True \nor False, depending on the value of the logical condition that comes after IF. With a \nloop control statement, the outcome is either to perform the code within the loop or \nDecision coverage not - again a True or False exit. Decision coverage is calculated by: \nThe coverage of decision \noutcomes. (Note: this is m(mm_nmmmmmmmx‘m \nthe Glossary definition Total number of dedision outcomes \n:m‘li buta What feels like reasonably thorough black-box testing may achieve only 40 \n: 1o 60% decision coverage. Typical ad hoe testing may cover only 20% of the \no The peecetage of decisions, leaving 80% of the possible outcomes untested. Even if your testing \nhave been exercised by seems reasonably lhumug.ll from a functional or blnck—hof Penpeclive: you may \natest suite) have only covered two-thirds or three-quarters of the decisions. Decision cover- \nage is stronger than statement coverage. It ‘subsumes” statement coverage — this \nmeans that 100% decision coverage always guarantees 100% statement coverage. \nAny stronger coverage measure may require more test cases to achieve 100% \ncoverage. \nLet’s go back to Code samples 4.2 again. We saw earlier that just one test \ncase was required to achieve 100% statement coverage. However, decision \ncoverage requires each decision to have had both a True and False outcome, \nTherefore, to achieve 100% decision coverage, a second test case is necessary \nwhere A is less than or equal to B, This will ensure that the decision statement \nIF A > B has a False outcome. So one test is sufficient for 100% statement \ncoverage, but two tests are needed for 100% decision coverage. Note that 100% \ndecision coverage guarantees 100% statement coverage, but nor the other way \naround! \nNow let us consider slightly different Code samples shown in Code samples 4.3a \nand 4.3b. \n1 READ A \n2 READ B \n3C=A-2\"B \n4 IF C < 0 THEN \ns PRINT 'C negative' \n6 ENDIF \nCode sample 4.3a \n1 = Number (args(2]) \n2 let b = Number(args(3]) \n3 let c =a ~ 2*b \n4 if (c < 0) { \n5 console.log('C negative’) \n6} \nCode sample 4.3b \nLet us suppose that we already have the following test, which gives us 100% \nstatement coverage for Code samples 4.3, \nlet a \no N Compogs Loswming. A3 gt Bt vt My et b o, s, o4 b, s 10 . o, U 3 i g, s sty comtos ey b gy B e el smbs At el e M e gyt oot s o Sy S el g pericnce Cengage Lty BTN e (W 14 Y adibicnd o o e o € g 30 (s e",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 151,
            "page_label": "152"
        }
    },
    {
        "page_content": "Section 3 White-Box Test Techriques 139 \nTEST SET 2 \nTest2 1:A =208 =15 \n‘Which decision outcomes have we exercised with our test? The value of C is —10, \nso the condition C < 0s True, so we will print ‘C negative’ and we have exercised the \nTrue outcome from that decision statement. But we have not exercised the decision \noutcome of False. What other test would we need to exercise the False outcome and \nto achieve 100%% decision coverage? \nBefore we answer that guestion, let’s have a look at another way to represent this \ncode. Sometimes the decision structure is easier to see in a control flow diagram \n(see Figure 4.4). \nThe dotted line shows where Test 2_1 has gone and clearly shows that we have not \nyet had a test that takes the False exit from the IF statement. \nLet's modify our existing test set by adding another test: \nTEST SET 2 \nTest2 1:A = 20,8 = 15 \nTest2 2:A=10,8=2 \nThis now covers both of the decision outcomes, True (with Test 2_1) and False \n(with Test 2_2). If we were to draw the path taken by Test 2_2, it would be a straight \nline from the read statement down the False exit and through the ENDIF. Note that \nwe could have chosen other numbers to achieve either the True or False outcomes. \n4.3.3 The value of statement and decision testing \nCoverage is 4 partial measure of some aspect of the thoroughness of testing: in this \nsection, we have looked at two types of coverage, statement and decision. The value \nof statement and decision testing is in seeing what new tests are needed in order to \nachieve a higher level of coverage, whatever dimension of coverage we are looking \nat. By being more thorough (even in a very limited way), we are leaving less of the \ncomponent or system completely untested. \nFIGURE 4.4 Control flow diagram for Code samples 4.3 \no S Compogs Loy N3 B Bt vl Moy et b oo, . o0 B 0 b 18 o, L 3 i, . ot st ey b gt B e ol snbis At s el s M e 5 gyt oot s et Sy e el g ericnce Cengage L BTN e (W 14 Y adiBmcnd o o e o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 152,
            "page_label": "153"
        }
    },
    {
        "page_content": "140 Chapter 4 Test techniques \nOne client decided to measure coverage and found that their supposedly thor- \nough tests only reached 60% decision coverage. They decided to write more tests \n10 increase this 1o 80%. Although they spent 3 weeks designing and implementing \nthese new tests, they found enough high-severity defects to justify spending that time. \nWhite-box coverage measures and related test techniques are described in ISO/IEC \n/1IEEE 29119-4 [2015]. White-box test techniques are also discussed in Copeland \n{2003] and Myers [2011]. A good description of the graph theory behind structural \ntesting can be found in Jorgensen [2014], and Hetzel [1988] also shows a structural \napproach. Pol, Teunissen and van Veenendaal [2001] describes a white-box approach \ncalled an algorithm test. \n4.4 EXPERIENCE-BASED TEST TECHNIQUES \nLLABUS LEARNING OBJECTIVES FOR 4.4 \nEXPERIENCE-BASED TEST TECHNIQUES (K2) \nFL-44.1 Explain error guessing (K2) \nFL-442 Explain exploratory testing (K2) \nFL-443 Explain checklist-based testing (K2) \nIn this section we will look at three experience-based techniques, why and when they \nare useful, and how they fit with black-box test techniques. \nAlthough it is true that testing should be rigorous, thorough and systematic, this is \nnot all there is to testing, There is a definite role for non-systematic techniques, that \nis, tests based on a person's knowledge, experience, imagination and intuition. The \nreason is that some defects are hard 1o find using more systematic approaches, so a \ngood bug hunter can be very creative at finding those elusive defects. \nOne aspect which is more difficult for experience-based techniques is the measure- \nment of coverage. In order to measure coverage, we need to have some idea of a full \nset of things that we could possibly test, coverage then being the percentage of that \nset that we did test. For example, we could look at the coverage of the items in the test \ncharter, a checklist or a set of heuristics. We should also be able to use the traceability \nof tests to requirements or user stories and look at coverage of those. \nIn this section, look for the definitions of the Glossary terms checklist-based \ntesting, error guessing and exploratory testing, \n4.4.1 Error guessing \nError guessing A test Error guessing is a technique that is good 10 be used as a complement 1o other more \ntechnique in which formal techniques. The success of error guessing is very much dependent on the skill \ntests are derived on of the tester, as good testers know where the defects are most likely to lurk. Some \nthe basis of the tester's people seem 10 be naturally good at testing. Others are good testers because they have \n:m of past a lot of experience either as a tester or working with a particular system and so are \n¥9 ;'\" m\"‘,e able to pin-point its weaknesses, This is why error guessing, used after more formal \ntechnigues have been applied to some extent, can be very effective. In using more \nformal techniques, the tester is likely to gain a better understanding of the system, \nwhat it does and how it works. With this better understanding. they are likely 1o be \nbetter at guessing ways in which the system may not work properly. \nmodes. \nG N Crmpoge Lowming. A3 Kighs Bt vt My et b o, s, o4 b s, 0 s 18 . o, U 3 i, e, s sty comtr ey b gy B e el smbs At e e dmd sy oo s et s 8 cven earwny Conpuge Lowmay murses e 1y 10 ey kbacnd o o a1 wow € whacacs YA (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 153,
            "page_label": "154"
        }
    },
    {
        "page_content": "Section 4 ExpenenceBased Test Techniques 141 \nThere are no rules for error guessing. The tester is encouraged to think of \nsituations in which the software may not be able to cope. Here are some typical \nthings to try: division by zero, blank (or no) input, empty files and the wrong kind \nof data (for example alphabetic characters where numeric are required). If anyone \never says of a system or the environment in which it is to operate “That could \nnever happen’, it might be a good idea to test that condition, as such assumptions \nabout what will and will not happen in the live environment are often the cause \nof failures, \nError guessing may be based on: \n® How the application has worked in the past. \n® What types of mistakes the developers tend to make. \n® Failures that have occurred in other applications. \nA structured approach to the error-guessing technique is to list possible defects \nor failures and to design tests that attempt to produce them. These defect and failure \nlists can be built based on the tester’s own experience or that of other people, available \ndefect and failure data, and from common knowledge about why software fails. This \nway of trying 1o force specific types of fault to occur is sometimes called an “attack’ \nor ‘fault attack”. See Whittaker [2003]. \n4.4.2 Exploratory testing \nExploratory testing is a hands-on approach in which testers are involved in minimum  Exploratory testing \nplanning and maximum test execution. The planning involves the creation of a test  An approach to testing \ncharter, a short declaration of the scope of a short (one- o two-hour) time-boxed test W\"'“’l the testers \neffort, the objectives and possible approaches 1o be used. dynamically design and \nThe test design and test execution activities are performed in parallel, typically ~ ©Xecute tests based \nwithout formally documenting the test conditions, test cases or test scripts. The on ther kﬁ?’ \ntests are informal, because they are not pre-defined or documented in advance m ml::f \nin detail (although a test charter is most often written in advance). This does not previous tests. \nmean that other, more formal testing techniques will not be used. For example, \nthe tester may decide to use BVA, but will think through and test the most import- \nant boundary values without necessarily writing them down. Some notes will be \nwritten while the exploratory testing is going on, so that a report can be produced \nafterwards. \nOne typical way 1o organize and manage exploratory testing is to have sessions, \nhence this is also known as session-based testing. Each session is time-boxed, for \nexample with a firm time limit of 90 minutes. A test charter will give a list of test \nconditions (sometimes referred to as objectives for the test session), but the testing \ndoes not have to conform completely to that charter, particularly if new areas of high \nrisk are discovered in the session, The test session is completely dedicated to the \ntesting, without extraneous interruptions. \nTest logging is undertaken as test execution is performed, documenting (at a high \nlevel) the key aspects of what is tested, any defects found and any thoughts about \npossible further testing, possibly using test session sheets. A key aspect of exploratory \ntesting is learning: learning by the tester about the software, its use, its strengths and \nits weaknesses. As its name implies, exploratory testing is about exploring, finding \nout about the software, what it does, what it does not do, what works and what does \nnot work. The tester is constantly making decisions about what to test next and where \nto spend the (limited) time. \nG N Crmpoge Lowming. A3 Kighs Bt vt My et b o, s, o4 e i, 0 s 18 . o, (b 30 o e, s sty comto ey b gy B e el b At s el e M e 1 gyl oot s et Sy o el g pericncs Cengge Liwing BT e (W 14 Y adlbicnd o o e o € g 30 (s s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 154,
            "page_label": "155"
        }
    },
    {
        "page_content": "142 Chapter 4 Test techniques \nThis is an approach that is most useful when there are no or poor specifications \nand when time is severely limited. It can also serve to complement other, more formal \ntesting, helping 1o establish greater confidence in the software. In this way, explor- \natory testing can be used as a check on the formal test process by helping to ensure \nthat the most serious defects have been found. \nExploratory testing is described in Kaner, Bach and Petticord [2002] and Cope- \nland [2003). Other ways of testing in an exploratory way (attacks) are described by \nWhittaker [2003]. \n4.4.3 Checklist-based testing \nChecklist-based Checklist-based testing is testing based on experience, but that experience has \ntesting An experience- been summarized and documented in a checklist. Testers use the checklist to design, \nbased test technique implement and execute tests based on the items or test conditions found in the check- \nwhereby the expenenced  iqt. The checklist may be based on: tester uses a high- \nlevel list of ftems to ® experience of the tester \nbe noted, checked, or isi oy ol . knwledgc.‘fw example what is important lu-r the user \nrules or criteria against o understanding of why and how software fails, \n\"':m‘mhl’h When using a checklist, the tester may modify it by adding new questions or \nthings to check, or may just use what is already there. An experienced tester may be \nthe one who writes the first version of the checklist as a way 1o help less experienced \ntesters do better testing. \nChecklists can be general, or more likely, aimed at particular arcas such as dif- \nferent test types and levels. For example, a checklist for functional testing would be \nquite different from one aimed at testing non-function quality attributes. \n11 different people use the same checklist, it is more likely that there will a degree \nof consistency in what is tested. However, the actual tests executed may be quite \ndifferent, since the checklist is only mentioning high-level items. This variability, \neven while using the same checklist, may help to uncover more defects and achieve \ndifferent levels of coverage (if that is measured), even though it is less repeatable due \n1o human creativity (which is a good thing). \no N Compoge Lossming. A3 Kt Bt My et b o, s, o b, s 10 . o, U 3 o e, st s paty comto ey b gy B e el snbs At el i A e gyt oo s ot Sy S el g pericnce Cengage Ly BTG e (W 14 Y adiBicd o o e o € g 30 (s e",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 155,
            "page_label": "156"
        }
    },
    {
        "page_content": "Chapter Revew 143 \nCHAPTER REVIEW \nLet's review what you have learned in this chapter. \nFrom Section 4.1 (categories of test techniques), you should be able to give reasons \nwhy black-box, white-box, and experience-based approaches are useful, and be able \nto explain the characteristics and differences between these types of technigues. You \nshould be able to list the factors that influence the selection of the appropriate test \ntechnique for a particular type of problem, such as the type of system, risk, customer \nrequirements, models for use case modelling, requirements models or testing knowl- \nedge. You should know the Glossary terms black-box test technique, coverage, \nexperience-based test technique. test technique and white-box test technique. \nFrom Section 4.2, you should be able to write test cases from given software mod- \nels using equivalence partitioning (EP), Boundary Value Analysis (BVA), decision \ntable testing and state transition testing. You should understand and be able to apply \ncach of these four technigues, understand what level and type of testing could use \neach technique and how coverage can be measured for each of them. You should \nalso understand the concept and benefits of use case testing. You should know the \nGlossary terms boundary value analysis, decision table testing, equivalence par- \ntitioning, state transition testing and use case testing. \nFrom Section 4.3, you should be able to describe the concept and importance of \ncode coverage. You should be able to explain the concepts of statement and decision \ncoverage and understand that these concepts can also be used ot test levels other than \ncomponent testing (such as business procedures at system test Jevel). You should be \nable to write test cases from given control flows using statement testing and decision \ntesting. and you should be able 1o assess statement and decision coverage for com- \npleteness. You should know the Glossary terms coverage, decision coverage and \nstatement coverage. \nFrom Section 4.4, you should be able to explain the reasons for writing test cases \nbased on intuition, experience and knowledge about common defects and you should \nbe able to compare experience-based techniques with black-box test techniques. \nYou should know the Glossary terms checklist-based testing, error guessing and \nexploratory testing. \ne 328G Lo 4 St U ot . . Gt 0 10 et B vt i e G ot s P e et e et [P ay pa pe——— o Lamang et gl b s bbb et o 7 b § bt o4l b gt &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 156,
            "page_label": "157"
        }
    },
    {
        "page_content": "144 Chapter 4 Test techmiques \nSAMPLE EXAM QUESTIONS \nQuestion 1 Which of the following statements \nabout checklist-based testing is true? \na. A checklist contains test conditions and detailed \ntest cases and procedures. \nb. A checklist can be used for functional testing, but \nnot for non-functional testing. \n€. Checklists should always be used exactly as \nwritten and should not be modified by the \ntester., \n. Checklists may be based on experience of why \nand how software fails, \nQuestion 2 In a competition, ribbons are \nawarded as follows: less than 12 metres, no \nribbon, a yellow ribbon up to 25 metres, a red \nribbon up to 35 metres, and a blue ribbon for \nfurther than that. \nWhat distances (in metres) would be chosen \nusing BVA? \na 0,11, 12,25, 26, 35, 36. \nb 11,12, 13, 29, 30, 31, 40. \ne 718,32, 3. \no 0,12,13,26,27, 36, 37. \nQuestion 3 Which ststement about tests based \non increasing statement or decision coverage is \ntrue? \na. Increasing statement coverage may find defects \nwhere other tests have not taken both true and \nfalse outcomes. \nb. Increasing statement coverage may find defects in \ncode that was exercised by other tests. \n¢. Increasing decision coverage may find defects \nwhere other tests have not taken both true and \nfalse outcomes. \no Increasing decision coverage may find defects in \ncode that was exercised by other tests. \nQuestion 4 Why are both black-box and white-box \ntest technigues useful? \na. They find different types of defect. \nb. Using more techniques is always better. \nc. Both find the same types of defect, \nd. Because specifications tend to be unstructured. \nQuestion 5 What is a key characteristic of \nwhite-box test techniques? \na. They are mainly used 10 assess the structure of a \nspecification. \nb They are used both to measure coverage and to \ndesign tests 1o increase coverage, \n<. They are based on the skills and experience of \nthe tester. \nd. They use a formal or informal model of the \nsoftware or component. \nQuestion 6 Which of the following would be an \nexample of decision table testing for a financial \napplication, applied at the system-test level? \na. Atable containing rules for combinations of \ninputs 1o two fields on a screen. \nb. A table coataining rules for interfaces between \ncomponents. \n€. Atable containing rules for mortgage \napplications. \n. Atable containing rules for basic arithmetic to \ntwo decimal places. \nQuestion 7 Which of the following could be a \ncoverage measure for state transition testing? \nV All states have been reached, \nW The response time for each transaction is \nadequate. \nX Every transition has been exercised. \nY All boundaries have been exercised. \nZ Specific sequences of transitions have been \nexercised. \na X.YandZ. \nb V.X.YandZ \ne W, XandY. \nd V.Xand Z, \ne 328 Cogs Lo 4 g St U ot . . et 0 10 et B et i e G ot s AP e et e gt [Pay pa Iy p o Lramang et gl b s adb et o 7 b § bt o4l b gt &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 157,
            "page_label": "158"
        }
    },
    {
        "page_content": "Question 8 Postal rates for light letters are $0.25 \nup to 10g, $0.35 up to 50g plus an extra $0.10 for \neach additional 25g up to 100g, \nWhich test inputs (in grams) would be selected \nusing equivalence partitioning (EP)? \na. 8,42,82,102, \nb. 4,15, 65,92, 159. \n< 10, 50,75, 100. \nd. 5,20, 40, 60, 80. \nQuestion 9 Which of the following could be used \nto assess the coverage achieved for black-box test \ntechniques? \nV Decision outcomes exercised, \nW Partitions exercised. \nX Boundaries exercised. \nY State transitions exercised. \nZ Statements exercised. \na V,W.YorZ \nb W.XorY. \n¢ V.XorZ \nd WX YorZ \nQuestion 10 Which of the following would \nwhite-box test techniques be most likely to be \napplied to? \n1) Boundaries between mortgage interest rate bands, \n2) An invalid transition between two different \narrears statuses, \n3) The business process flow for mortgage approval. \n4) Control flow of the program to calculate \nrepayments. \na 2. 3and 4. \nb. 2and 4. \n¢ 3andd. \nd. 1. 2and 3. \nSample Exam Questons 145 \nQuestion 11 Use case testing is useful for which of \nthe following? \nP Designing acceptance tests with users or customers. \nQ Making sure that the mainstream business \nprocesses are tested. \nR Finding defects in the interaction between \ncomponents. \nS Identifying the maximum and minimum values for \nevery input field, \nT Identifying the percentage of statements exercised \nby a sets of tests. \na P.QandR. \nb. Q. SandT. \nc. P.QandS. \nd. R.Sand T. \nQuestion 12 Which of the following statements \nabout the relationship between statement coverage \nand decision coverage is correct” \na. 100% decision coverage is achieved if \nstatement coverage is greater than 90%. \nb. 100% statement coverage is achieved if decision \ncoverage is greater than 90%. \nc. 100% decision coverage always means 100% \nstatement coverage. \nd. 100% statement coverage always means 100% \ndecision coverage. \nQuestion 13 Why are experience-based test \ntechniques good 1o use? \na. They can find defects missed by black-box and \nwhite-box test techniques. \nb. They do not require any training to be as effective \nas formal techniques. \nc. They can be used most effectively when there are \ngood specifications. \nd. They will ensure that all of the code or system is \ntested, \no N Crmpogs Lonsming. A3 K Bt vt My et b o, s, o4 b, s 10 . o, U 3 st g, s s paty conto ey b sy B e el smbs At s el s M v gyt oo s e Sy S G el g peTicnce Cengage L BTN e (W 14 Y adlbicnd o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 158,
            "page_label": "159"
        }
    },
    {
        "page_content": "146 Chapter 4 Test techniques \nQuestion 14 If you are flying with an economy \nticket, there is a possibility that you may get \nupgraded to business class, especially if you hold \na gold card in the airline’s frequent flier program. \nIf you do not hold a gold card, there is a possibility \nthat you will get bumped off the flight if it is full \nand you check in late. This is shown in Figure 4.5, \nNote that each box (that is, statement) has been \nnumbered. \nThree tests have been run: \nTest 11 Gold card holder who gets upgraded to busi- \nness class, \nTest 2:  Non-gold card holder who stays in economy. \nTest3: A person who is bumped from the flight, \n‘What is the statement coverage of these three tests? \na. 60%, \nb. 705, \nP - \n] \not \nb \nFIGURE 4.5 Control flow diagram for flight check-n \nQuestion 15 Consider the control flow shown \nin Figure 4.6. In this example. if someone is a \nmember, then they get a 10% discount, but \nonly on items with an ItemCode of 25 or less. \nThe following tests have already been run; \nTest 1: Name is a member, ltemCode = 50 \nTest 2: Name is not a member, ItemCode = 27 \nDiscount = 10% \nRead Itern Code \nDiscount = 0% \nPrint Name gets Discount off itemCode \nFIGURE 4.6 Control flow diagram for Question 15 \nWhat is the statement coverage and decision coverage \nof these tests? \na. Statement coverage = 100%, Decision \ncoverage = 100%. \nb. Statement coverage = 100%, Decision \ncoverage = 50%. \n<. Statement coverage = 75%, Decision \ncoverage = 1006, \nd. Statement coverage = 100%, Decision \ncoverage = 75%. \no S Compoge Lo A3 B Bt vt Moy et b oo, s, o4 b i, 0 b 18 ot (ke 3 i, e, ot sty st ey b gt B e ol snbis gt s el e i e 5 gyt oot s et sy w8 el Wy pericncs Cengg Ly BTN e (W 14 Y adlBicnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 159,
            "page_label": "160"
        }
    },
    {
        "page_content": "Question 16 When choosing which technique \n1o use in a given situation, which factors should be \ntaken into account”? \nU Previous experience of types of defects found in \nthis or similar systems. \nV The existing knowledge of the testers. \nW Regulatory stundards that apply. \nX The type of test execution tool that will \nbe used. \nY The work products available. \nZ Previous experience in the development language. \na V.W.Yand Z. \nb. U, V.Wand Y. \nc U XandY. \nd. V.Wand Y. \nSample Exam Questons 147 \nQuestion 17 Given the state diagram in Figure 47, \nwhich test case is the minimum series of valid transi- \ntions to cover every state? \na. SS-S1-S2-S4-SI1-S3-ES. \nb. $5 81 -52-83-84-ES. \n¢ S5-81-52-54-51-83-84-5]-S3-ES. \nd. SS-SI-S4-S2-SI1-S3-ES. \nFIGURE 4.7 State diagram for PIN entry \no S Compoge Loy N3 Rt Bt vl My et b o, . o4 Bl i, 0 b 18 o (o 3 s, e, ot e paty ontoss ey b spqpeseed B e ol snbis gt s el s M e 0 gyt oot s ot Sy o ool g pericnce CEngage Lewng BTG e (0 4 W ol o o e o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 160,
            "page_label": "161"
        }
    },
    {
        "page_content": "148 Chapter 4 Test techniques \nEXERCISES \nExercises based on the techniques covered in this chapter are given in this section. Worked solutions are given \nin the next section. \nEquivalence Partitioning/Boundary Value Analysis exercise \nScenario: If you take the train before 9:30 am or in the afternoon after 4:00 pm until 7:30 pm (the rush hour). you \nmust pay full fare. A saver ticket is available for trains between 9:30 am and 4:00 pm, and after 7:30 pm. \n‘What are the partitions and boundary values to test the train times for ticket types? Which are valid partitions \nand which are invalid partitions? What are the boundary values? (A table may be helpful to organize your parti- \ntions and boundaries.) Derive test cases for the partitions and boundaries. \nAre there any questions you have about this requirement? s anything unclear? \nDecision table exercise \nScenario: If you hold an over 60s rail card, you get a 34% discount on whatever ticket you buy. If you are travelling \nwith a child (under 16), you can get a 50% discount on any ticket if you hold a family rail card, otherwise you get \n 10% discount. You can only hold one type of rail card, \nProduce a decision table showing all the combinations of fare types and resulting discounts and derive test \ncases from the decision table. \nState transition exercise \nScenurio: A website shopping basket starts out as emply. As purchases are selected, they are added to the shopping \nbasket. ltems can also be removed from the shopping basket. When the customer decides to check out, a summary \nof the items in the basket and the total cost are shown, for the customer 10 say whether this is OK or not. If the \ncontents and price are OK, then you leave the summary display and go to the payment system. Otherwise you go \nback to shopping (so you can remove items if you want). \na. Produce a state diagram showing the different states and transitions. Define a test, in terms of the sequence \nof states, to cover all transitions, \nb. Produce a state table. Give an example test for an invalid transition. \nStatement and decision testing exercise \nNote that statement and decision testing are no longer K3, as they were in the previous Syllabus, but this exercise \nshould help you to understand the technique a bit better anyway! \nScenario: A vending machine dispenses either hot or cold drinks. If you choose a hot drink (for example tea \nor coffee), it asks if you want milk (and adds milk if required), then it asks if you want sugar (and adds sugar if \nrequired), then your drink is dispensed. \na. Draw a control flow diagram for this example. (Hint: regard the selection of the type of drink as one \nstatement ) \nb. Given the following tests, what is the statement coverage achieved? What is the decision coverage achieved? \nTest 1: Cold drink. \nTest 2: Hot drink with milk and sugar. \n€ What additional tests would be needed to achieve 100% statement coverage? What additional tests would be \nneeded to achieve 100% decision coverage? \ne 328Gy Lo 4 St U ot . i et 0 10 et B et i e B ot s AP e et e it [Pay pa pe——— g Lramtng et e gl b s b et o 7 b § bt o4l b g &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 161,
            "page_label": "162"
        }
    },
    {
        "page_content": "Exercise Solutions 149 \nEXERCISE SOLUTIONS \nEP/BVA exercise \n“The first thing to do is 1o establish exactly what the boundaries are between the full fare and saver fare. Let's put \nthese in o table to organize our thoughts: \nScheduled departure time = 929 am 9:30 am - 4:00 pm | 4:01 pm - 7:30 pm =7:31 pm \nTicket type tull saver full saver \n‘We have assumed that the boundary values are: 9:29 am, 9:30 am, 4:00 pm, 4:01 pm, 7:30 pm and 7:31 pm. By \nsetting out exactly what we think is meant by the specification, we may highlight some ambiguities or, at least, \nraise some questions. This is one of the benefits of using the technique! For example: \n“When does the moming rush hour start? At midnight? At 11:30 pm the previous day? At the time of the first \ntrain of the day? If so, when is the first train? 5:00 am” \nThis is a rather important omission from the specification. We could make an assumption about when it starts, \nbut it would be better 1o find out what is correct. \n® Ifatrain is due to leave at exactly 4:00 pm, is a saver ticket still valid? \n® What if a train is due to leave before 4:00 pm but is delayed until after 4:00 pm? Is a saver ticket still \nvalid? (That is, if the actual departure time is different from the scheduled departure time.) \nOur table above has helped us to see where the partitions are. All of the partitions in the table above are valid \npartitions. It may be that an invalid partition would be a time that no train was running, for example before 500 am, \nbut our specification did not mention that! However it would be good 1o show this possibility also, We coukd be a bit \nmore formal by listing all valid and invalid partitions and boundaries in a table, as we described in Section 4.3.1, but \nin this case it does not actually add a Jot. since all partitions are valid. \nHere are the test cases we can derive for this example: \nTest case reference Input Expected outcome \n1 Depart 4:30 am Pay full tare \n2 Depart 9:29 am Pay full fare \n3 Depart 9:30 am Buy saver ticket \n4 Depart 11:37 am Buy saver ticket \n5 Depart 4.00 pm Buy saver ticket \n6 Depart 4:01 pm Pay full fare \n7 Depart 5:55 pm Pay full fare \n8 Depart 7:30 pm Pay full fare \n9 Depart 7:31 pm Buy saver ticket \n10 Depart 10:05 pm Buy saver ticket \nNote that test cases 1, 4, 7 and 10 are based on equivalence partition values; test cases 2, 3, 5, 6,8 and 9 are \nbased on boundary values. There may also be other information about the test cases, such as preconditions, that \nwe have not shown here. \nS0 g e, Mg S M . e Ay o 18 p St s e e oy eqpes e et e et spey R o et e g e e v 2y o £ —",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 162,
            "page_label": "163"
        }
    },
    {
        "page_content": "150 Chapter 4 Test techniques \nDecision table exercise \nThe fare types mentioned are an over 60s rail card, a family rail card and whether you are travelling with a child \nor not. With three conditions or causes, we have eight columns in our decision table below. \nCauses (inputs) R1 R2 R3 R4 RS R6 R7 R8 \nover 60s rail card? Y Y  / N N \nfamily rail card? Y Y N N Y N \nchild also travelling? N Y N N N \nEffects (outputs) \nDiscount (%) XN50% | X7/34% | 34% 34% 50% 0% 10% 0% \nWhen we come to fill in the effects, we may find this a bit more difficult. For the first two rules, for exam- \nple, what should the output be? Is it an X because holding more than one rail card should not be possible? \nThe specification does not actually say what happens if someone does hold more than one card, that is, it has \nnot specified the output, so perhaps we should put a question mark in this column. Of course, if someone \ndoes hold two rail cards, they probably would not admit this, and perhaps they would claim the 50% discount \nwith their family rail card if they are travelling with a child, so perhaps we should put 50% for Rule 1 and \n34% for Rule 2 in this column. Our notation shows that we do not know what the expected outcome should \nbe for these rules! \nThis highlights the fact that our natural language (English) specification is not very clear as to what the effects \nshould actually be. A strength of this technique is that it forces greater clarity. If the answers are spelled out in a \ndecision table, then it is clear what the effect should be. When different people come up with different answers \nfor the outputs, then you have an unclear specification! \nThe word ‘otherwise’ in the specification is ambiguous. Does ‘otherwise’ mean that you always get at least a \n10% discount or does it mean that if you travel with a child and an over 60s card but not a family card you get \n10% and 34%? Depending on what assumption you make for the meaning of ‘otherwise’, you will get a different \nlast row in your decision table, \nNote that the effect or output is the same (34%) for both Rules 3 and 4. This means that our third cause (whether \nor not a child is also travelling) actually has no influence on the output. These columns could therefore be com- \nbined with ‘do not care’ as the entry for the third cause. This rationalizing of the table means we will have fewer \ncolumns and therefore fewer test cases. The reduction in test cases is based on the assumption we are making \nabout the factor having no effect on the outcome. so a more thorough approach would be to include each column \nin the table. \nHere is a rationalized table, where we have shown our assumptions about the first two outcomes and we have \nalso combined Rules 6 and 8 above, since having a family rail card has no effect if you are not travelling with \nachild. \nCauses (inputs) R1 R2 R3 RS R6 R7 \nover 60s rail card? Y Y N N N \nfamily rail card? Y Y N ¥ - N \nchild also travelling? N - Y N Y \nEffects (outputs) \nDiscount (%) 50% 34% 34% 50% 0% 10% \no S Compogs Loy N3 Wit Bt vl Moy et b o, . o4 Bl 0 s 18 o, U 3 o, . ot oty ontoss ey b gt B e ol b gt s el i s e gyt oot s e Sy 48 el g pericnce Cengge L BTG e (W 14 Y adlbicnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 163,
            "page_label": "164"
        }
    },
    {
        "page_content": "Exercise Solutons 151 \nHere are the test cases that we derive from this table. (If you did not rationalize (or collapse) the table, then \nyou will have eight test cases rather than six.) Note that you would not necessarily test each column, but the table \nenables you to make a decision about which combinations 10 test and which not to test this time. \nTest case reference | Input Expected outcome \n1 S. Wilkes, with over 60s rail card and family rail 50% discount for both tickets \ncard, travelling with grandson Josh (age 11) \n2 Mrs M. Davis, with over 60s rail card and family 34% discount \nrail card, traveling alone \n3 J. Rogers, with over 60s rail card, travelling with 34% discount (for ). Rogers only, \nhis wife not his wife) \n4 S. Gray, with family rad card, traveling with her 50% discount for both tickets \ndaughter Betsy \nMiss Congeniality, no rail card, traveliing alone No discount \n6 Joe Bloggs with no rail card, travelling with his 10% discount for both tickets \nS-year-old niece \nNote that we may have raised some additional issues when we designed the test cases. For example, does the \ndiscount for a rail card apply only to the traveller or to someone travelling with them? Here we have assumed that \nit applies to all travellers for the family rail card, but to the individual passenger only for the over 60s rail card. \nState transition exercise \nThe state diagram is shown in Figure 4.8, The initial state (S1) is when the shopping basket is empty. When an \nitem is added to the basket, it goes 1o state (S2), where there are potential purchases. Any sdditional items added \nto the basket do not change the state (just the total number of things to purchase). Items can be removed. which \ndoes not change the state unless the total items ordered goes from 1 10 0, In this case, we go back to the empty \nShopping basket \nFIGURE 4.8 State diagram for shopping basket \not S0 Compoge Loy A3 Bt Bt vl Moy et b o, . o4 B 0 b 18 o, (ko 3 o, . ot ity st ey b gt b e ol snbis At s el s M e gyt oot s e Sy R Tl g peTicnce CEngage Liwrng TGS e (W 14 Y adibmnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 164,
            "page_label": "165"
        }
    },
    {
        "page_content": "152 Chapter 4 Test techniques \nbasket (S1). When we want to check out, we go to the summary state (S3) for approval. If the list and prices are \napproved, we go to payment (S4): if not, we go back to the shopping state (possibly to remove some items to reduce \nthe total price we have to pay). There are four states and seven transitions, \nNote that SI is our start state for this example and S4 is the end state — this means that we are not concerned \nwith any event that happens once we get to State S4. \nHere is a test to cover all transitions. Note that the end state from one step or event is the start state for the next \nevent, so these steps must be done in this sequence. \nState Event (action) \ns1 Add item \n52 Remove (last) item \ns1 Add item \n52 Add item \ns2 Remove item \ns2 Check out \n$3 Not OK \ns2 Check out \n53 oK \n54 Payment \nAlthough our example is not interested in what happens from State 4, there would be other events and actions \nonce we enter the payment process that could be shown by another state diagram (for example check validity of \nthe credit card, deduct the amount. email a receipt, etc.). \nThe corresponding state table is: \nState orevent | Additem | Removeitem | Remove lastitem | Check out | Not OK \n1 Empty s2 - - - - \n2 Shopping s2 52 s1 53 - \n53 Summary - - - - 52 \n54 Payment - - - - - \nAll of the boxes that contain ‘- (dash) are invalid transitions in this example. Example negative tests would \ninclude: \n® Attempt to add an item from the summary and cost state (S3), \n® Try to remove an item from the empty shopping basket (SI). \n@ Try w enter OK while in the shopping state (S2). \nGt N4 Compogs Loy N3 B Bt vl My et b o, . o4 B i, 0 b 18 o, (o 3 s, . ot sty cosmoss ey b gt B he ol smbis A s el i i e s gyt oot s e Sy o el g pericnce Cengage L BTG e (W 14 Y ol o o o € g 430 (s s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 165,
            "page_label": "166"
        }
    },
    {
        "page_content": "Exercise Solutons 153 \nStatement and decision testing exercise \nThe control flow diagram is shown in Figure 4.9. Note that drawing a coatrol diagram here illustrates that white- \nbox testing can also be applied to the structure of general processes, not just to computer algorithms. Flowcharts \nare generally easier to understand than text when you are trying to describe the results of decisions taken on later \nevents. \nOn Figure 4,10, we can see the route that Tests | and 2 have taken through our control flow graph. Test | has \ngone straight down the left-hand side to select a cold drink. Test 2 has gone to the right at each opportunity. adding \nboth milk and sugar to a hot drink. \nEvery statement (represented by a box on the diagram) has been covered by our two tests, so we have 100% \nstatement coverage. \n‘We have not taken the No Exit from either the ‘milk?\" or ‘sugar?” decisions, so there are two decision outcomes \nthat we have not tested yet. We did test both of the outcomes from the “hot or cold?\" decision, so we have covered \nfour out of six decision outcomes. Decision coverage is 4/6 or 67% with the two tests. \nNo additional tests are needed to achieve statement coverage, as we already have 100% coverage of the \nstatements. \nOne additional test is needed to achieve 100% decision coverage: \nTest 3: Hot drink, no milk, no sugar. \nThis test will cover both of the ‘No' decision outcomes from the milk and sugar decisions, so we will now \nhave 100% decision coverage. \nFIGURE 4.9 C Sow digram for drinks & ma.m Control flow diagram showing coverage \no S Compogs Lo A3 B Bt vl Moy et b oo, o4 B 0 s 18 o, (ke 3 o, . ot e paty sontons ey b smppreneed B e ol snbis At s el i M v gyt oo s ot Sy 4 el g pericnce (g Lewting BTG e (W 14 Y ol s o s o € g 3 (E s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 166,
            "page_label": "167"
        }
    },
    {
        "page_content": "CHAPTER FIVE \nTest management \nTc,\\lmg is a complex activity. It can be a distinct sub-project within the larger \nsoftware development, maintenance or integration project. It usually accounts for \na substantial proportion of the overall project budget. Therefore we must understand \nhow we should manage the testing we do, \nIn this chapter, we cover essential topics for test management in six sections, \nThe first relates 1o how to organize the testers and the testing, The second concerns \nthe estimation, planning and strategizing of the test effort. The third addresses test \nprogress monitoring, test reporting and test control. The fourth explains configuration \nmanagement and its relationship to testing. The fifth covers the central topic of risk \nand how testing affects and is affected by product and project risks. The sixth and \nfinal section discusses the management of defects. \n5.1 TEST ORGANIZATION \nSYLLABUS LEARNING OBJECTIVES FOR 5.1 TEST \nORGANIZATION (K2) \nFL-5.1L.1  Explain the benefits and drawbacks of independent testing (K2) \nFL-5.1.2  Identify the tasks of a test manager and tester (K1) \nIn this section, We'll talk about organizing a test effort within a project. We'll look \nat the value of independent testing, and discuss the potential benefits and risks asso- \nciated with independent testing. We'll examine the various types of different team \nmembers we might want on a test team. And we'll familiarize ourselves with the \ntypical tasks performed by test managers and testers. \nAs we go through this section, keep your eyes open for the Glossary terms tester, \nand test manager. \n5.1.1 Independent testing \nIn Chapter | we talked about independent testing from the perspective of individual \ntester psychology. In this chapter, we'll look at the organizational and managerial \nimplications of independence. \nTesting tasks may be done by people with a specific testing role, for example with \ntester as part of their job title, but testers are definitely not the only people who do \ntesting. Developers, business analysts, users and customers also do testing tasks for \n154 \noy N Crmpogs Loswming. A3 Fightn Rererrnd My et b copi, wamnd, o0 et o s 10 e, U 3 vt g, acmms i sty commvs sy b sppowsad B e ol snbhe gt T mview s devmed sy Je——— s & cvent barwny o Conpae Lowmay mures e 1 1wy albmcnd o o a1 S € shaeacm A (7aTa s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 167,
            "page_label": "168"
        }
    },
    {
        "page_content": "Section 1 Test Organization 155 \ndifferent reasons and at different times, Even those who are full-time testers may do a \nvariety of different tasks at different times. But if lots of people do testing, why have \npeople dedicated to it? One reason is that the view of a different person, especially \none who is trained to look for problems, can be much more effective at finding those \nproblems. Many organizations, especially in safety-critical areas, have separate teams \nof independent testers 1o capitalize on this effect. As we saw in Chapter | Section L5, \nindependence can overcome cognitive bias. \nIf there is a separate test team, approaches 10 organizing it vary, as do the places \nin the organizational structure where the test team fits. Since testing is an assessment \nof quality, and since that assessment may not always be perceived as positive, many \norganizations strive to create an organizational climate where testers can deliver an \nindependent, objective assessment of quality. \nLevels of independence \nWhen thinking about how independent the test team is, recognize that independence \nis not an eitherfor condition, but a continuum. \nAt one end of the continuum lies the absence of independence, where the devel- \noper performs testing on their own ¢code within the development team. Of course, \nevery good developer does do some testing of their own code, but this should not be \nthe ONLY testing! \nMoving toward independence, you find an integrated tester or group of testers \nworking alongside the developers, but still within and reporting to the development \nmanager. For example, developers may test each other’s code after testing their own, \nor a tester on an Agile team may help developers and do some independent testing \nwithin the team. Pair programming is one way of having another pair of eyes on the \ncode as it is being developed, whether it is two developers pairing, or a developer \nand a tester. \nA further level of independence would be to have a team of testers who are inde- \npendent and outside the development team, reporting to project management or busi- \nness management. \nSometimes there are testers or teams of testers with special skills or responsibilities \nwithin an organization; such specialists may also be outside the development organi- \nzation, which would be the other end of the independence continuum. For example, \nthere may be a tester or team that specializes in performance or security testing, \nIn fully independent testing. you might see a separate test team reporting into the \norganization at a point equal to the development or project team or at a higher level. \nYou might find specialists in the business domain (such as users of the system), spe- \ncialists in technology (such as database experts), and specialists in testing (such as \nsecurity testers or performance test experts) in a separate test team, as part of a larger \nindependent test team, or as part of a contracted outsourced test team. \nPotential benefits of independence \nLet's examine the potential benefits and risks of independence, starting with the \nbenefits. \nAn independent tester can often see more, different defects than a tester working \nwithin a development team ~ or a tester who is by profession a developer. While \nbusiness analysts, marketing staff, designers and developers bring their own assump- \ntions to the specification and implementation of the item under test, an independent \ntester brings a different set of assumptions to testing and to reviews, which often \nhelps expose hidden defects and problems related to the group’s way of thinking, \nG N Compoge Lowming. A3 K Bt vt My et b o, s, o4 A i, 0 s 18 . o, U 3 i e, s sty comto ey b gyt B e el snbs At s e e b et oo s et e Conpage Lonmay murses e 1y 10 ey kbacnd o o 0} S € whacgacs YIS (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 168,
            "page_label": "169"
        }
    },
    {
        "page_content": "156 Chapter 5 Test management \nas we discussed in Chapter 3. An independent tester brings a sceptical attitude \nof professional pessimism, a sense that, if there’s any doubt about the observed \nbehaviour, they should ask: “Is this a defect?\" \nAt the team level, an independent test team reporting to a senior or executive \nmanager may enjoy (once they earn it) more credibility in the organization than a \ntest manager or tester who is part of the development team. An independent tester \nwho reports to senior management may be able to report his or her results honestly \nand without concern for reprisals that might result from pointing out problems in \nco-workers” or, worse yet, the manager’s work. An independent test team often has \na separate budget, which helps ensure the proper level of money is spent on tester \ntraining, testing tools, test equipment and so forth. In addition, in some organizations, \ntesters in an independent test team may find it casier to have a carcer path that leads \nup into more senior roles in testing. \nPotential drawbacks of test independence \nIndependent test teams are not risk-free. It is possible for the testers and the test \nteam to become isolated. This can take the form of interpersonal isolation from \nthe developers, the designers, and the project team itself. It can also take the form \nof isolation from the broader view of quality and the business objectives, for \nexample, an obsessive focus on defects, often accompanied by a refusal to accept \nbusiness prioritization of defects. This leads to communication problems, feelings \nof alienation and antipathy, a lack of identification with and support for the project \ngoals, spontaneous blame festivals and political backstabbing. \nEven well-integrated test teams can suffer problems. Other project stakeholders \nmight come to see the independent test team (rightly or wrongly) as a bottleneck and \na source of delay. \nSome developers abdicate their responsibility for quality, saying, “Well, we have \nthis test team now, so why do | need to unit test my code? \nIndependent testers may not have all of the information that they need about the \ntest object, since they may be outside the development organization itself (where \noften much information is communicated informally). This leads to them being less \neffective than they should or could be. \nIndependence is not a replacement for familiarity. Although an independent view \nsees things that those closer 10 it miss, those who know the code or the test object \nbest may be able to see some things that the independent tester would miss because \nof their limited knowledge. It is not a case of independence is always best, but of \ngetting the best balance between independence and familiarity. \nIndependence varies \nDue to a desire for the benefits of an independent test team, companies sometimes \nestablish them, only to break them up again later. Why does that happen? A common \ncause is the failure of the test manager to effectively manage the risks of indepen- \ndence listed above. Some test teams succumb to the temptation to adopt a ‘No can \ndo’ attitude, coming up with reasons why the project should bend 1o their needs \nrather than each side being flexible so as to enable project success. Testers take to \nacting as enforcers of process or as auditors, without a proper management mandate \nand support. Resentments and pressures build, until at last the organization decides \nthat the independent test team causes more problems than it solves. It is especially \nimportant for testers and test managers to understand the mission they serve and the \nreasons why the organization wants an independent test team. Often, the entire test \nG N Crmpogs Lowming. A3 K Bt vt Moy et b o, s, o4 e i, 0 s 18 . o, U 3 i e, s sty st ey b gyt B e el snbs At Je—p—— e e b et e Conpage Linmay muries e 1y 10 ey kbacnd o o a1 o € whacacs YA (7T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 169,
            "page_label": "170"
        }
    },
    {
        "page_content": "Section 1 Test Organization 157 \nteam must realize that, whether they are part of the project team or independent, they \nexist to provide a service to the project team. \nThere is no one right approach 1o organizing testing, For each project, you must \nconsider whether to use an independent test team, based on the project, the appli- \ncation domain and the levels of risk, among other factors. As the size, complexity \nand criticality of the project increases, it is important to have independence in later \nlevels of testing (like integration test, system test and acceptance test), though some \ntesting is often best done by other people such as project managers, quality managers, \ndevelopers, business and domain experts or infrastructure or IT operations experts. \nIn most projects, there will be multiple test levels; the amount of independence \nin testing varies between levels as well (as it should). Often more independence is \nmost effective at the higher levels, for example system and user acceptance testing. \nThe type of development life cycle also influences the level of independence of \ntesting. In Agile development, a tester may provide some independence as part of \nthe development team and may (also) be part of an independent team performing \nindependent testing at higher levels. Product owners may perform acceptance testing \nto validate user stories at the end of each iteration. \n5.1.2 Tasks of a test manager and tester \n‘We have seen that the location of testers or of a test team within a project organiza- \ntion can vary widely. Similardy, there is wide variation in the roles that people within \ntesting play. Some of these roles occur frequently, some infrequently. Two roles that \nare found within many organizations are those of the test manager and the tester,  Test manager The \nthough the same people may play both roles at various points during the project.  person responsible for \nThe activities and tasks performed by these two roles will vary, depending on the  Project management \norganization, the project and product context, and the skills of the individuals, Ler's  ©f testing activities \ntake a look at the work typically done in these roles, starting with the test manager. and resources, and \nTest manager tasks obyect: The eicividuel \nA test manager tends to be the person tasked with overall responsibility for the test \nprocess and successful Ie-dcnhipd‘lhci:ﬂ:cl_x . The test manager role may requiates the evaluation \nbe performed by someone who holds this as their full-time position (a professional  of 5 test object. \ntest manager). or it might be done by a quality assurance manager, project manager \nor a development manager. Regarding the last two people on this list, warning bells Tester A skilled \nabout independence should be ringing in your head now, in addition to thoughts about  Professional who is \nhow we can ensure that such non-testers gain the knowledge and outlook needed to \nmanage testing. The test manager tasks may also be done by a senior tester. Whoever system \nis playing the role, expect them to plan, monitor and control the testing work. \nA test manager may be a single person trying 1o ensure that all testing is done as \nwell as it can be, or a test manager may have one or more teams of people reporting \nto them (for example in larger organizations). Sometimes the person at the top of the \nhierarchy would be called a test coordinator or test coach, and the individual teams \nmay be led by a test leader or lead tester. In any case, the test manager's role is to \nmanage the testing! \nTypical tasks of the test manager role may include the following: \n® Atthe outset of the project, in collaboration with the other stakeholders, devise \nthe test objectives, organizational test policies (if not already in place), and test \nstrategies. \nG N Compuge Lonwming. A3 g Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 3 st e, s sty commr ey b gy B e el snbs At Je—p—— e e b et P Conpage Lonmay murves e 1y 10 ey akbacnd o o 0} wow € hacacs YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 170,
            "page_label": "171"
        }
    },
    {
        "page_content": "158 Chapter 5 Test management \n@ Plan the test activities, based on the test objectives and risks, and the context of \nthe organization and the project. This may involve selecting the test approaches, \nestimating time, effort and cost for testing, acquiring resources, defining test \nlevels, types and test cycles and planning defect management. \n@ Write and update over time any test plangs). \n@ Coordinate the test planis) with other project stakeholders, project managers, \nproduct owners and anyone else who may affect or be affected by the project or \nthe testing, \n® Share the testing perspective with other project activities, such as integration \nplanning, especially where third-party suppliers are involved. \n® Lead, guide and monitor the analysis, design, implementation and execution of \nthe tests, monitor test progress and results, and check the status of exit criteria \n(or definition of done). \n® Prepare and deliver test progress reports and test summary reports, based on \ninformation gathered from the testers. \n® Adapt the test planning based on test results and progress (whether documented \nin test progress or summary reports or not) and take any actions necessary for \ntest control. \n® Support setting up the defect management system and adequate configuration \nmanagement of the testware, and traceability of the tests 1o the test basis, \n® Produce suitable metrics for measuring test progress and evaluating the quality \nof the testing and the product (test object). \n® Recognize when test automation is appropriate and, if it is, plan and support \nthe selection and implementation of tools to support the test process, including \nsetting a budget for ol selection (and possible purchase, lease and support and \ntraining of the team), allocating time and effort for pilot projects and providing \ncontinuing support in the use of the tool(s). (See Chapter 6 for more on tool sup- \nport for testing.) \n® Decide about the implementation of test environment(s) and ensure that they are \nput into place before test execution and managed during test execution. \n® Promote and advocate the testers, the test team and the test profession within \nthe organization. \n® Develop the skills and careers of testers, through training, performance \nevaluations, coaching and other activities, such as lunch-time discussions or \npresentations. \nAlthough this is a list of typical activities of a test manager, the tasks may be \ncarried out by people who are not labelled as a test manager. For example, in Agile \ndevelopment, the Agile team may perform some of these tasks, especially those to do \nwith day-to-day testing within the team. Test managers who are outside an individual \ndevelopment team, who work with several teams or the whole organization, may be \ncalled a test coach. See Black [2009] for more on managing the test process. \nTester tasks \nAs with test managers, projects should include testers at the outset. In the planning \nand preparation of the testing, testers should review and contribute 1o test plans, as \nwell as analyzing, reviewing and assessing requirements and design specifications. \nG N Crmpogs Loswming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At s v e e s oo st P Conpge Lewmay mures e 13 10 ey akoacd o o sy Vo € whacacs A (7O s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 171,
            "page_label": "172"
        }
    },
    {
        "page_content": "Section 1 Test Organization 159 \nThey may be involved in or even be the primary people identifying test conditions \nand creating test designs, test cases, test procedure specifications and test data, and \nmay automate or help 1 automate the tests. They often set up the test environments \nor assist system administration and network management staff in doing so. \nIn sequential life cycles, as test execution begins, the number of testers often \nincreases, starting with the work required to implement tests in the test environment. \nThey may play such a role on all test levels, even those not under the direct control \nof the test group; for example they might implement unit tests which were designed \nby developers. Testers execute and log the tests, evaluate the results and document \nproblems found. They monitor the testing and the test environment, often using tools \nfor this task, and often gather performance metrics. Throughout the testing life cycle, \nthey review each other’s work, including test specifications, defect reports and test \nresults. \nIn Agile development, testers are involved in every iteration or sprint, performing \na wide variety of testing tasks on whatever is being developed at the time, so they are \ncontinuously involved throughout. They may be reviewing user stories and identify- \ning test conditions, implementing the tests, running them and reporting on results, \nTesters may have specializations such as test analysis, test design, specific test \ntypes (especially non-functional testing), or test automation. Such specialists may \ntake the role of tester at different test levels and at different times, For example, the \nperson doing the testing tasks at component or component integration level is often \nthe developer (but see caveats about independence in Section 5.1.1). At system testing \nor system integration testing, an independent test team may do the testing activities. \nIn acceptance testing, the test tasks may be done by business analysts, subject matter \nexperts, users or product owners. At operational acceptance testing, operations and/ \nor system administration staff may be performing the tester tasks. \nTypical tasks of the tester may include the following: \n® Reviewing and contributing to test plans from the tester perspective. \n® Analyzing, reviewing and assessing requirements, user stories and acceptance \ncriteria, specifications and models (that is, the test basis) for testability and to \ndetect defects early. \no Identifying and documenting test conditions and test cases, capturing \ntraceability between test cases, test conditions and the test basis to assist in \nchecking the thoroughness of testing (coverage), the impact of failed tests and \nthe impact on the tests of changes in the test basis. \n® Designing, setting up and verifying test environments(s), coordinating with sys- \ntem administration and network management. \n® Designing and implementing test cases and test procedures, including auto- \nmated tests where appropriate. \n® Acquiring and preparing test data to be used in the tests. \n@ Creating a detailed test execution schedule (for manual tests). \no Exccuting the tests, evaluating the results and documenting deviations from \nexpected results as defect reports, \n@ Using appropriate tools to help the test process. \n® Automating tests as needed (for technical test specialists), as supported by a test \nautomation engineer or expert or 4 developer. (See Chapter 6 for more on test \nautomation.) \no S Crmpogs Lot N3 B Bt vl My et b o, . o4 A 0 b 18 o, (b 3 ot e, ot e paty somtrs ey b smppresend B e ol snbis At s el i A v gyt oot s e Sy o el g pericnce Cengage L AT e (W 14 Y adlbmnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 172,
            "page_label": "173"
        }
    },
    {
        "page_content": "160 Chapter 5 Test management \no Evaluating non-functional characteristics such as performance efficiency, \nreliability, usability, security, compatibility and portability. \n® Reviewing tests developed by others, including other testers, business ana- \nlysts, developers or product owners. Part of a tester’s role is to help educate \nothers about doing better testing. \nDefining the skills test staff need \nThis section is outside what is required by the Syllabus, but we include it as useful \nand practical advice anyway! \nDoing testing properly requires more than defining the right positions and number \nof people for those positions. Good test teams have the right mix of skills based on \nthe tasks and activities they need to carry out, and people outside the test team who \nare in charge of test tasks need the nght skills, oo, \nPeople involved in testing need basic professional and social qualifications such \nas literacy, the ability to prepare and deliver written and verbal reports, the ability to \ncommunicate effectively and so on. Going beyond that, when we think of the skills \nthat testers need, three main areas come to mind: \no Application or business domain: A tester must understand the intended \nbehaviour, the problem the system will solve, the process it will automate and \n50 forth, in order to spot improper behaviour while testing. and recognize the \n“must-work” functions and features, \no Technology: A tester must be aware of issues, limitations and capabilities of the \nchosen implementation technology, in order to effectively and efficiently locate \nproblems and recognize the ‘likely-to-fail” functions and features, \no Testing: A tester must know the testing topics discussed in this book, and \noften more advanced testing topics, in order to effectively and efficiently \ncarry out the test tasks assigned. \nThe specific skills in each area and the level of skill required vary by project, \norganization, application and the risks involved. \nThe set of testing tasks and activities are many and varied, and so 100 are the \nskills required, so we often see specialization of skills and separation of roles. For \nexample, due 1o the special knowledge required in the areas of testing, technology \nand business domain, respectively, test automation experts may handle automating \nthe regression tests, developers may perform component and integration tests and \nusers and operators may be involved in acceptance tests. \nWe have long advocated pervasive testing, the involvement of people throughout \nthe project team in carrying out testing tasks. Let’s close this section, though, on a \ncautionary note. Software and system companies (for example producers of shrink- \nwrapped software and consumer products) typically overestimate the technology \nknowledge required to be an effective tester. Businesses that use information technol- \nogy (for example banks and insurance companies) typically overestimate the business. \ndomain knowledge needed. \nAll types of projects tend to underestimate the testing knowledge required. We \nhave seen a project fail in part because people without proper testing skills tested \ncritical components, leading to the disastrous discovery of fundamental architectural \nproblems later. Most projects can benefit from the participation of professional \ntesters, as amateur testing alone will usually not suffice. \nG N Compoge Lossming. A3 Kghts Bt vt My et b o, s, o4 B s, 0 s 10 . o, U 3 i, e, s sty conto ey b gy B e el smbs At s el e M e 0 gyt oo s e Sy R el g pericncs Cengage Ly BTN e (W 14 Y adiBacnd o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 173,
            "page_label": "174"
        }
    },
    {
        "page_content": "Section 2 Test Planning and Estimation 161 \n5.2 TEST PLANNING AND ESTIMATION \nSYLLABUS LEARNING OBJECTIVES FOR 5.2 TEST PLAN \nAND ESTIMATION (K3) \nFL-5.2.1 Summarize the purpose and content of a test plan (K2) \nFL-52.2 Differentinte between various test strategies (K2) \nFL-5.2.3  Give examples of potential entry and exit criteria (K2) \nFL-5.2.4  Apply knowledge of prioritization, and technical and logical \ndependencies, to schedule test execution for a given set of test \ncases (K3) \nFL-5.2.5 Identify factors that influence the effort related to testing (K1) \nFL-5.2.6 Explain the difference between two estimation techniques: the \nmetrics-based technique and the expert-based technique (K2) \nIn this section, we'll talk about a complicated trio of test topics: plans, estimates \nand strategies. Plans, estimates and strategies depend on a number of factors, \nincluding the level, targets and objectives of the testing we are setting out to do. \nWriting a plan, preparing an estimate and selecting test strategies tend to hap- \npen concurrently and ideally during the planning period for the overall project, \nthough we must be ready to revise them as the project proceeds and we gain more \ninformation. \nLet’s look closely at how to prepare a test plan, examining issues related to plan- \nning for a project, for a test level, for a specific test type and for test execution. We'll \ndiscuss selecting test strategies and ways to establish adequate exit criteria for testing. \nIn addition, we'll look at various tasks related 1o test execution that need planning. \nWe'll examine typical factors that influence the effort related to testing and see two \ndifferent estimation techniques: metrics-based and expert-based. \nLook out for the Glossary terms entry criteria, exit criteria, test approach, test \nestimation, test plan, test planning and test strategy in this section. \n5.2.1 The purpose and content of test plan \nWhile people tend to have different definitions of what goes in a test plan, for us  Test plan \na test plan is the project plan for the testing work 1o be done. It is not a test design ~ Documentation \nspecification, a collection of test cases or a set of test procedures; in fact, most of our  de€scribing the test \ntest plans do not address that level of detail. objectives to be \nWhy do we write test plans? We have three main reasons: to guide our thinking, \nto communicate to others and to help manage future changes. \nFirst, writing a test plan guides our thinking. We find that if we can explain some- / \nthing in words, we understand it. If we cannot explain it, there's a good chance we ml:‘mm \ndon't understand. Writing a test plan forces us to confront the challenges that await \nus and focus our thinking on important topics, In Chapter 2 of Fred Brooks” brilliant \no N Compoge Lossming. A3 Kt Bt vt My et b o, s, o4 b s, s 18 . o, U 3 st e, s s sty comto ey b gy B e el smbis At s i e dmd ety Je——— s & cven barwny Conpuge Lonmay murses e 1 10 ey akbmcnd o o a1 S € whacgacs A (7 0TA s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 174,
            "page_label": "175"
        }
    },
    {
        "page_content": "162 Chapter 5 Test management \nand essential book on software engineering management, The Mythical Man-Month, \nhe explains the importance of careful estimation and planning for testing as follows: \nFailure to allow enough time for system test, in particular, is peculiarly disas- \ntrous, Since the delay comes at the end of the schedule, no one is aware of \nschedule trouble until almost the delivery date [and] delay at this point has \nunusually severe ... financial repercussions, The project is fully staffed, and \ncost-per-day is maxinmum [as are the associated opportunity costs]. It is there- \nSore very importani to allow enough system test time in the eriginal schedule. \nBrooks [1995] \nThis is particularly applicable to sequential development life cycles (as many were \nback in 1995). Agile development was designed to address some of these problems, \nbut it does not obviate the need for system testing, so the advice is still relevant. \nWe find that using a template when writing test plans helps us remember the \nimportant challenges. You can use the template and examples shown in ISOVIEC/IEEE \n291193 [2013), use someone else’s template or create your own template over time. \nTest planning The Second, the test planning process and the plan itself serve as vehicles for commu- \nactivity of establshing nicating with other members of the project team, testers, peers, managers and other \norupdating a test plan.  srakeholders. This communication allows the test plan 1o influence the project team \nand allows the project team to influence the test plan, This is especially important \nin the areas of organization-wide testing policies and motivations; test scope, objec- \ntives, and critical areas to test: project and product risks, resource considerations and \nconstraints; and the testability of the item under test. \nYou can accomplish this communication through circulation of one or two test \nplan drafts and through review meetings. Such a draft may include many notes such \nas the following examples: \n[To Be Determined: Jennifer: Please tell me what the plan is for releasing \nthe test items into the test lab for each cycle of system lest execution?] \n[Dave ~ please let me know which version of the test tool will be used for \nthe regression tests of the previous increments.| \nAs you document the answers to these kinds of questions, the test plan becomes \na record of previous discussions and agreements between the testers and the rest of \nthe project team. Thus test planning is an ongoing activity performed throughout the \nproduct’s life cycle (and beyond into maintenance). \nThird, the test plan helps us manage change. During early stages of the project, as \nwe gather more information, we revise our plans. As the project evolves and situations. \nchange, we adapt our plans. Written test plans give us a baseline against which to \nmeasure such revisions and changes. Furthermore, updating the plan at major mile- \nstones helps keep testing aligned with project needs. As we run the tests, we make \nfinal adjustments to our plans based on the results. You might not have the time - or \nthe energy — to update your test plans every time a variance 0ccurs, 4s Some projects \ncan be quite dynamic. A simple approach is described in Black [2009] Chapter 6, for \ndocumenting variances from the test plan. You can implement it using a database or \nspreadsheet. You can include these change records in a periodic test plan update, as \npart of a test status report, or as part as an end of project test summary. \nWe have found that it is better to write multiple test plans in some situations, For \nexample, when we manage both integration and system test levels, those two test exe- \ncution periods may occur at different points in time and will have different objectives. \not S04 Crmpoge Loy N3 K Bt vl My et b o s o4 A 0 b 18 ot (b 3 s, . ot kot st ey b smpqpesmed B e ol sndis Akt 11 el e A e s gyt oo s e Sy 4 ool g erience Cengage L BT e (W 4 Y adibicnd o o o € g 30 (s e",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 175,
            "page_label": "176"
        }
    },
    {
        "page_content": "Section 2 Test Planning and Estimation 163 \nFor some systems projects, a hardware test plan and a software test plan will address \ndifferent techniques and tools, as well as different audiences. However, since there \nmight be overlap between these test plans, a master test plan that addresses the com- \nmon elements can reduce the amount of redundant documentation. \nWhat to do with your brain while planning tests \nWriting a good test plan is easier than writing a novel. but both tasks require an \norganized approach and careful thought. In fact, since a good test plan is kept short \nand focused, unlike some novels, some might argue that it is harder to write a good \ntest plin. Let's look at some of the planning tasks you need to carry out. \nAt a high level, you need to consider the purpose served by the testing work. In \nterms of the overall organizational needs, this purpose is referred to variously as the \ntest team's mission or the organization's testing policy. A test plan is influenced by \nmany factors: as well as test policy and the organization’s test strategy, the devel- \nopment life cycle and methods being used for development, and the availability of \nresources. In terms of the specific project, understanding the purpose of testing \nmeans knowing the answers to questions such as: \n‘What is in scope and what is out of scope for this testing effort? \n‘What are the test objectives? \n‘What are the important project and product risks? (More on risks in Section 5.5.) \nWhat is the overall approach of testing in this project? \nHow will test activities be integrated and coordinated into the software life \ncycle activities? \nHow do we decide what to test, what people and other resources are needed to \nperform test activities, and how test activities will be carried out? \n® What constraints affect testing (for example budget limitations, hard deadlines, ete)? \n® What is most critical for this product and project? \n® Which aspects of the product are more (or less) testable? \n® What should be the overall test execution schedule and how should we decide \nthe order in which to do test analysis, test design, implementation, execution \nand evaluation of specific tests, either on specific dates or in the context of an \niteration? (Product and planning risks, discussed later in this chapter, will influ- \nence the answers to these questions.) \n® What metrics will be used for test monitoring and control and how will they be \ngathered and analyzed? \n® What is the budget for all test activities” \n® What should be the level of detail and structure for test documentation? \n(Templates or example documents or work products are helpful for this.) \nYou should then select strategies which are appropriate to the purpose of testing \n(more on the topic of selecting strategies in Section 5.2.2). \nIn addition, you need to decide how to split the testing work into various levels, as \ndiscussed in Chapter 2 (for example component, integration, system and acceptance). \nIf that decision has already been made, you need to decide how to best fit your testing \nwork in the level you are responsible for with the testing work done in those other test \nlevels. During the analysis and design of tests, you will want to reduce gaps and over- \nlap between levels and, during test execution, you will want to coordinate between \nG N Crmpogs Lowming. A3 Kighs Bt vt My et b o, s, o4 A i, 0 s 10 . o, U 3 ottt e, s sty comto ey b gy B e el snbs At e e b ety oo s et e Conpage Linmay murses e 1y 10wy akbacnd o o 10} W € hocacs YA (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 176,
            "page_label": "177"
        }
    },
    {
        "page_content": "164 Chapter 5 Test management \nthe levels. Such details dealing with inter-level coordination are often addressed in \nthe master test plan. \nIn addition to integrating and coordinating between test levels, you should also \nplan to integrate and coordinate all the testing work to be done with the rest of the \nproject. For example, what items must be acquired for the testing? Are there ongoing \nsupply issues, such as with imitation bank notes (that is, simulated bank notes) for & \nfinancial application such as an ATM? When will the developers complete work on \nthe component or system under test? What operations support is required for the test \nenvironment? What kind of information must be delivered to the maintenance team \nat the end of testing? \nMoving down into the details, what makes a plan a plan (rather than a statement \nof principles, a laundry list of good ideas or a collection of suggestions) is that the \nauthor specifies in it who will do what when and (at least in a general way) how, \nResources are required to carry out the work. There are often hard decisions that \nrequire careful consideration and building a consensus across the team, including \nwith the project manager. \nThe entire testing process, from planning through to completion, produces infor- \nmation, some of which you will need to document. How precisely should testers write \nthe test designs, cases and procedures? How much should they leave to the judgement \nof the tester during test execution, and what are the reproducibility issues associated \nwith this decision? What kinds of templates can testers use for the various documents \nthey will produce? How do those documents relate 1o one another? If you intend 0 \nuse tools for tasks such as test design or execution, as discussed in Chapter 6, you \nwill need to understand how the models or automated tests will integrate with manual \ntesting, and plan who will be responsible for automation design, implementation and \nsupport. There may be a separate test automation plan, but this needs to be coordi- \nnated with other test plans. \nSome information you will need to gather in the form of raw data and then distil. What \nmetrics to do you intend 1o use to monitor, control and manage the testing? Which of \nthose metrics, and perhaps other metrics, will you use to report your results? We'll look \nmore closely at possible answers to those questions in Section 5.3, but a good test plan \nprovides answers early in the project. \nTest strategy il 5.2.2 Test strategy and test approach \ntest strategy) A test strategy is the general way in which testing will happen within each of the lev- \nDocumentation that els of testing. independent of project, across the organization. The test approach is the \nexpresses the generic name the ISTOB gives to the implementation of the test strategy on a specific project, \nrequirements for Since the test approach is specific to a project, you should define and document the \nfesting one or more approach in the test plans, refining and providing further detail in the test designs. \nProgects run within T\\q Deciding on the test approach involves careful consideration of the testing objec- \no M\"\"‘“’. Snqs  tives. the project’s goals and overall risk assessment. These decisions provide the \n1o be performed, and starting point for planning the test process, for selecting the test design techniques \n& aligned with the test and test types to be applied, and for defining the entry and exit criteria. In your \npolicy. decision-making on the approach, you should take into account the project, product \nand organization context, issues related 1o risks, hazards and safety, the available \nTost M 3\" resources, the team's level of skills, the technology involved. the nature of the system \nw' et st e under test, considerations related to whether the system is custom-built or assembled \nspecific project. from commercial off-the-shelf components (COTS), the organization’s test objectives \nand any applicable regulations. \nG N Crmpoge Lowming. A3 Kighs Bt vt My et b o, s, o4 e, s 18 . o, (b 3 i, g, s sty comto ey b gy B e el b AChapat sl el e i e gyt oot s et sy o el g ericncs CEngage Ly TS e (W 14 Y ol o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 177,
            "page_label": "178"
        }
    },
    {
        "page_content": "Section 2 Test Planning and Estimation 165 \nThe choice of test approaches or strategies is one powerful factor in the success \nof the test effort and the accuracy of the test plans and estimates. This factor is under \nthe control of the testers and test managers. Of course, having choices also means that \nyou can make mistakes, so we'll go into more detail about how to pick the right test \nstrategies in a minute. First, though, let’s survey the major types of test strategies that \nare commonly found. \n® Analytical: In this strategy, tests are determined by analyzing some factor, such \nas requirements (or other test basis) or risk. For example, the risk-based strategy \ninvolves performing a risk analysis using project documents and stakeholder \ninput, then planning. estimating. designing and prioritizing the tests based on risk. \nWe will talk more about risk analysis later in this chapter. Another analytical test \nstrategy is the requirements-based strategy. where an analysis of the requirements \nspecification forms the basis for planning. estimating and designing tests. Analy- \ntical test strategies have in common the use of some formal or informal analytical \ntechnique, usually during the requirements and design stages of the project. \n® Model-based: In this strategy, tests are designed based on some model of the \ntest object. For example, you can build mathematical models for loading and \nresponse for e-commerce servers, and test based on that model. If the behaviour \nof the system under test conforms to that predicted by the model, the system is \ndeemed to be working. Model-based test strategies have in common the creation \nor selection of some formal or informal model for eritical system behaviours, \nusually during the requirements and design stages of the project. Examples also \ninclude business process models, state models, for example state transition dia- \ngrams, etc., or reliability grown models. \n® Methodical: In this strategy. a pre-defined and fairly stable list of test condi- \ntions is used. For example, you might have a checklist that you have put together \nover the years that suggests the major areas that testing should cover, or you \nmight follow an industry standard for software quality, such as ISO/IEC 25010 \n[2011], for your outline of major test areas. You then methodically design, \nimplement and execute tests following this outline, Methodical test strategies \nhave in common the adherence to a pre-planned, systematized approach. This \nmay have been developed in-house, assembled from various concepts developed \nin-house and gathered from outside, or adapted significantly from outside ideas, \nand may have an early or late point of involvement for testing. Some examples \ninclude: working through a list (that is, taxonomy) of typical defects, or working \nthrough a list of desired quality characteristics, such as company-wide look-and- \nfeel standards for websites and mobile apps. \n® Process- or standard-compliant: In this strategy, an external standard or set \nof rules is used 1o analyze, design and implement tests. For example, you might \nadopt the standard ISOVIEC/IEEE 29119-3 [2013] for your testing, or you may \nuse TMMi (Test Maturity Model integration, www.tmmi.org) 10 assess your \ncurrent testing and improve it. More information about TMMi can be found in \nvan Veenendaal and Wells [2012]. Alternatively, process- or standard-compliant \nstrategies have in common reliance upon an externally developed approach to \n\" The catalogue of testing strategics that has been inchaded in the ISTQB Foundation Syllabas grew out \nof an enail discussion between Res Black, Ross Collard, Kathy Iberle and Cem Kaner, We thank them \nfor their thought provok ing comments, \nG N Crmpogs Lowming. A3 Kt Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 s e, s sty comto ey b gy B e el smbs At v e e s oo st P Conpge Lewmay mures e 13 10 ey akoacd o o sy Vo € whacacs A (7O s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 178,
            "page_label": "179"
        }
    },
    {
        "page_content": "166 Chapter 5 Test management \ntesting, often with little (if any) customization. They may have an early or late \npoint of involvement for testing. For example, some organizations need to con- \nform to safety-critical or financial regulations which may include processes for \nidentifying tests in a rigorous way from the test basis. \no Directed (or consultative): In this strategy, stakeholders or experts (technology \nor business domain experts) may direct the testing according to their advice \nand guidance. For example, you might ask the users or developers of the system \nto tell you what to test or even rely on them to do the testing. Consultative or \ndirected strategies have in common the reliance on a group of non-testers to \nguide or perform the testing effort, and typically emphasize the later stages of \ntesting simply due to the lack of recognition of the value of early testing. This \nstrategy may be helpful when the developing organization is a new start-up \nwithout a lot of testing knowledge or expertise. For example, an outside expert \nmay advise on security testing. \n® Regression-averse: In this strategy, the most important factor is to ensure that \nthe system’s performance does not deteriorate or get worse when it is changed \nand enhanced. To protect existing functionality, automated regression tests \nwould be extensively used, as well as standard test suites and reuse of existing \ntests and test data. For example. you might try to astomate all the tests of system \nfunctionality so that, whenever anything changes, you can re-run every test to \nensure nothing has broken. Regression-averse strategies have in common a set of \nprocedures ~ usually awtomated ~ that allow them to detect regression defects. \nA regression-averse strategy may involve automating functional tests prior to \nrelease of the function, in which case it requires carly testing. but sometimes \nthe testing is almost entirely focused on testing functions that have already been \nreleased, which is in some sense a form of post-release test involvement. \n® Reactive (or dynamic): In this strategy, the tests react and evolve based on \nwhat is found while test execution occurs, rather than being designed and \nimplemented before test execution starts, For example, you might create a \nlightweight set of testing guidelines that focus on rapid adaptation or known \nweaknesses in software. Reactive strategies, using exploratory testing, have in \ncommon concentrating on finding us many defects as possible during test exe- \ncution and adapting to the realities of the system under test as it is when deliv- \nered, and they typically emphasize the later stages of testing. See, for example, \nthe attack-based approach of Whittaker [2002] and Whittaker and Thompson \n[2003] and the exploratory approach of Kaner, Bach and Petticord [2002]. \nSome of these strategies are more preventive, others more dynamic, For example, \nanalytical test strategies involve up-front analysis of the test basis and tend to identify \nproblems in the test basis prior to test execution. This allows the carly, cheap removal \nof defects. That is a strength of preventive approaches. \nReactive test strategies focus on the test execution period. Such strategies allow \nthe location of defects and defect clusters that might have been hard to anticipate until \nyou have the actual system in front of you. That is a strength of reactive approaches. \nRather than see the choice of strategies, particularly the preventive or reactive \nstrategies, as an either/or situation, we'll let you in on the worst kept secret of testing \n(and many other disciplines). There is no one best way. We suggest that you adopt \nwhatever test strategies make the most sense for your particular test approach, and \nfeel free to borrow and blend. \nv M Compg i, 4 s Srre My v e s o A1 ek o D e e Bk s v s gyl b i b sl e e b et e Conpage Linmay murves e 1y 10 ey kbacnd o o 01 S € whacacs YA (74T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 179,
            "page_label": "180"
        }
    },
    {
        "page_content": "Section 2 Test Planning and Estimation 167 \nHow do you know which strategies to pick or blend for the best chance of success? \nThere are many factors to consider, but let's highlight a few of the most important: \n® Risks: Testing is about risk management, so consider the risks and the level of \nrisk. For a well-established application that is evolving slowly, regression is an \nimportant risk, so regression-averse strategies make sense. For a new applica- \ntion, a risk analysis may reveal different risks if you pick a risk-based analytical \nstrategy. \n® Skills: Strategies must not only be chosen, they must also be executed. So \nyou have to consider the skills that your testers possess and lack. A standard- \ncompliant strategy is a smart choice when you lack the time and skills in your \nteam to create your own approach. \n® Objectives: Testing must satisfy the needs of stakeholders to be successful, If \nthe objective is to find as many defects as possible with a minimal amount of \nup-front time and effort invested - for example, at a typical independent test \nlab — then a dynamic or reactive strategy makes sense. \n@ Regulations: Sometimes you must satisfy not only stakeholders, but also regu- \nlators, In this case, you may need to devise a methodical test strategy that satis- \nfies these regulators that you have met all their requirements. \n® Product: Some products such as weapons systems and contract development \nsoftware tend to have well-specified requirements, This leads to synergy with a \nrequirements-based analytical strategy. \n® Business: Business considerations and business continuity are often import- \nant. If you can use a legacy system as a model for a new system, you can use \na model-based strategy. \nWe mentioned above that a good team can sometimes triumph over a situation \nwhere materials, process and delaying factors are ranged against its success. How- \never. talented execution of an unwise strategy is the equivalent of going very fast \ndown a motorway in the wrong direction. Therefore, you must make smart choices \nin terms of testing strategies. Furthermore, you must choose testing strategies with an \neye toward the factors mentioned earlier, the schedule, budget and feature constraints \nof the project and the realities of the organization and its politics. \n5.2.3 Entry criteria and exit criteria (definition of ready \nand definition of done) \nTwo important things 1o think about when determining the approach and planning \nthe testing are: how do we know we are ready to start a given test activity, and how \ndo we know we are finished (with whatever testing we are concerned with)? At what \npoint can you safely start a particular test level? When are you confident that it is \ncomplete? The factors to consider in such decisions are called entry and exit criteria \nor in Agile development, definition of ready and definition of done (DoD). \nTypical entry criteria include the following: Entry criteria \n@ Availability of testable requirements, user stories and/or models, for example g:m of ;:nmm \nwhen following a model-based testing strategy, thit is, the test basis is available. for officially starting a \n@ Availability of test items that have met the exit criteria for any previous test levels.  defined task. \n® Availability of the test environment. \nG N Crmpoge Loswming. A3 K Bt vt My et b o, s, o4 B s, 0 s 18 . o, U 3 i e, ot sty conmo ey b gy B e el smbs At e e b ety oo s et T Conprge Lonmay muries e 1y 10 ey koo o o 1y o € whbacacm A (74T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 180,
            "page_label": "181"
        }
    },
    {
        "page_content": "168 Chapter 5 Test management \no Availability of necessary test tools and any other materials needed. \n® Availability of test data and other necessary resources. \no Awailability of staff for testing tasks. \n® Availability of the component or system 10 be tested, in a state where tests \ncan be done, that is, availability of the test object. \n‘Why are entry criteria important? If everything needed for testing to go ahead is \nin place before you start, the testing will go much more smoothly. The problems of \ngetting started with the testing often turn out to be that something needed is not actu- \nally ready or in place. Then the testing process gets the blame for the delays. If entry \ncriteria, which are the preconditions for testing, are enforced, or at least thought about \nbeforehand, everything is more likely 1o go better. If not, you are increasing risk, \nintroducing delays and additional costs, and making life more difficult for yourself. \nExit criteria Typical exit criteria include the following: \ntest completion criteria,  ® Tests: the number planned, prepared, run, passed, failed, blocked, skipped etc. \ndefinition of done) The are acceptable. \nset of conditions for o Coverage: the extent 1o which the test basis (for example requirements, user \nofficially completing a stories, acceptance criteria), risk, functionality, supported configurations, \ndefined task. and the software code have been tested (that is, achieved a defined level of \ncoverage) - or have not. \n® Defects: the number known to be present, the arrival rate, the number estimated \nto remain, the number resolved and the number of unresolved defects are within \nan agreed limit. \n® Quality: the status of the important quality characteristics for the system, for \nexample reliability, performance efficiency. usability, security and other relevant \nquality characteristics are adequate. \n® Money: the cost of finding the next defect in the current level of testing com- \npared to the cost of finding # in the next level of testing (or in production). \n® Schedule: the project schedule implications of starting or ending testing. \no Risk: the undesirable outcomes that could result from shipping too early \n(such as latent defects or untested areas), or too late (such as loss of market \nshare). \nWhen writing exit criteria, we try to remember that a successful project or iteration \nis a balance of quality, budget, schedule and feature considerations. This is important \nin each sprint and is important in other development life cycles when exit criteria are \napplied at the end of the project. \nWhy are exit criteria important? Knowing what your goals are is important in \nany human endeavour. If we do not think about ‘How will we know we are done’ \nbeforehand, then we may stop before we have done enough testing (increasing risk) \nor we might keep testing when we have done enough (not likely but possible, and \nthis would be wasteful). \nIn practice, testing is often stopped rather than finished, because time pressure \nseems 10 hold the trump card for everything else. But if (or rather when) this happens, \nif you do have documented exit criteria, you can make the risks more visible to stake- \nholders and managers by showing what has not yet been completed in the testing. \nYou may not win the argument for doing more testing at the time, but you will have \ngood information to explain next time why the exit criteria are important, especially \nG N Crmpugs Lowming. A3 Kighs Bt vt My et b o, s, o4 e, 0 s 18 . o, U 3 i e, s sty commo ey b gyt B e el smbs At s v e e e oo s et P Conpuge Linmay murses e 1y 10wy kbmcnd o o a1 wow € whacacs YA (7T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 181,
            "page_label": "182"
        }
    },
    {
        "page_content": "Section 2 Test Planning and Estimation 169 \nwhen/if there are undesirable consequences 10 not meeting them this time, We also \nwant a ‘Stop testing now!\" decision to be made with knowledge of all the factors, not \nJust basing that decision on a calendar date, \n5.2.4 Test execution schedule \nPart of test management is the management of tests, Once test cases and test proce- \ndures have been designed and implemented (some as automated tests), they may be \nassembled into test suites for convenience of running sets of tests together. A schedule \nfor the execution of the test suites should be based on a number of factors, including \nthe prioritics of the tests (from risk analysis), technical or logical dependencies of \ntests or test suites and the type of tests, for example confirmation tests after defects \nhave been fixed. or regression tests. These factors need to be balanced with a sensible \nand efficient sequence of executing the tests. \nFor example, it may be that several high-priority tests are dependent on a single \nlow-priority test 1o set up essential data or starting conditions. In that case, the low- \npriority test should be executed before the high-priority tests, even when risk priority \nis the most important factor. \nThe same logic may apply where tests are dependent on other tests, Actually, this \nis one reason why tests should ideally be designed to be independent of any other \ntests. Independent tests can be run in any order. \nIn Agile development, rapid feedback from tests is key to efficient working of the \nteam. but this means that the test execution schedule is biased toward confirmation tests \nand short tests, which can be run quickly. This also shows why it is important to be able \nto identify (or tag) tests so that they can be assembled into different test suites for dif- \nferent execution schedules, something that is particularly important for automated tests. \nIt is more likely to be the tester or the team rather than the test manager who is mak- \ning the test execution schedule, but whoever does it, they need to balance the trade-off \nbetween efficiency, priority of the tests and the objective of the test execution at the time. \nThis learning objective is a K3 in the Syllabus, which means that you need to be \nable to produce a test execution schedule, taking priorities and dependencies into \naccount. See the exercise at the end of this chapter, and the mock exam in Chapter 7. \n5.2.5 Factors influencing the test effort \nIn this section, we'll look at test estimation, first describing how 1o go about esti-  Test estimation \nmating testing, what it will involve and what it can cost, and then looking at factors  The calculated \nthat influence the test effort, many of which are significant for testing even though ~ approximation of a \ni i i result related to various they are not part of what we are estimating when we estimate testing. of for \nEstimating what testing will involve and what it will cost ipketioo dats \nThe testing work to be done can often be seen as a subproject within the larger o ben o mbar \nproject. We can adapt fundamental technigues of estimation for testing. We could  cacae e ), mo;tesl \nstart with a work-breakdown structure that identifies the stages, activities and tasks.  ysable even if input data \nStarting at the highest level, we can break down a testing project into major activi-  may be incomplete, \nties using the test process identified in the ISTQB Syllabus (and described in Chapter | uncertain or noisy. \nSection 1.4.2): test planning, test monitoring and control, test analysis, test design, test \nimplementation, test execution and test completion, Within each activity, we identify \ntasks and perhaps subtasks, To identify the activities and tasks, we work both forward \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 e s, 0 s 18 . o, U 3 ot e, st e paty comto ey b gy B e el smbs AChagat s e e b ety oo s et T Conprge Lowmay murves e 1y 10 ey kbacnd o o a1 v € whacacs YA (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 182,
            "page_label": "183"
        }
    },
    {
        "page_content": "170 Chapter 5 Test management \nand backward, When we say we work forward, we mean that we start with the planning \nactivities and then move forward in time step-by-step, asking ‘Now, what comes next?’ \nWorking backward means that we consider the risks that we identified during risk \nanalysis (which we'll discuss in Section 5.5). For those risks which you intend to address. \nthrough testing, ask yourself, *“What activities and tasks are required in cach stage to \ncarry out this testing?” Let's look at an example of how you might work backward. \nSuppose that you have identified performance as a major arca of risk for your \nproduct, Performance testing is an activity in test execution. You now estimate the \ntasks involved with running a performance test, how long those tasks will take and \nhow many times you will need 1o run the performance tests, \nNow, those tests did not just appear out of thin air: someone had to develop them. \nPerformance test development entails activities in test analysis, design and implemen- \ntation. You now estimate the tasks involved in developing a performance test, such \nas writing test scripts and creating test data. \nTypically, performance tests need to be run in a special test environment that is \ndesigned to look like the production or field environment, at least in those ways which \nwould affect response time and resource utilization. Performance test environment \nacquisition and configuration is an activity in the test implementation activity. You \nnow estimate tasks involved in acquiring and configuring such a test environment, \nsuch as simulating performance based on the production environment design 1o look \nfor potential bottlenecks, getting the right hardware, software and tools and setting \nup that hardware, software and tools, Performance tests need special tools to generate \nload and check response. The acquisition and implementation of such tools also needs \nto be planned (more on tools in Chapter 6). \nTt may be possible to use virtualization for your test environment and performance \ntesting tools: this seems to be an attractive idea, since it should save money as you \ndo not have 1o acquire your own environment or tools, However, the use of virtual \nenvironments and using the tools in those environments still needs careful planning. \nNot everyone knows how 1o use performance testing tools or to design perfor- \nmance tests. Performance testing training or staffing is a task in the test planning \nactivity. Depending on the approach you intend to take, you now estimate the time \nrequired to identify and hire a performance test professional or 1o train one or more \npeople in your organization to do the job. \nFinally, in many cases a detailed test plan is written for performance testing. \ndue to its differences from other test types. Performance testing planning is a task \nin test planning. You now estimate the time required to draft, review and finalize a \nperformance test plan. \n‘When you are creating your work-breakdown structure, remember that you will want \n1o use it for both estimation (at the beginning) and monitoring and control (as the project \ncontinues). To improve the accuracy of the estimate and enable more precise control, \nmake sure that you subdivide the work finely enough. This means that tasks should be \nshort in duration, say one to three days. If they are much longer — say two weeks — then \nyou run the risk that long and complex subtasks are hiding within the larger task, only \n10 be discovered later, This can lead to nasty surprises during the project, \nFactors that affect the test effort \nTesting is a complex endeavour on many projects and a variety of factors can influ- \nence it. When creating test plans and estimating the testing effort and schedule, you \nmust keep these factors in mind or your plans and estimates will deceive you at the \nbeginning of the project and betray you at the middle or end. \nG N Crmpogs Loswming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At s P v e e s oo st P Conpge Lewmay mures e 13 10 ey akoacd o o sy Vo € whacacs A (7O s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 183,
            "page_label": "184"
        }
    },
    {
        "page_content": "Section 2 Test Planning and Estimation 171 \nThe test strategies or approaches you pick will have a major influence on the \ntesting effort, as we discussed in Section 52.2. In this section, let’s look at factors \nrelated to the product, the development process, the people involved and the results \nof testing. \nProduct characteristics \nThe characteristics of the product that we are testing have a major impact on how \nwe will test it: \n@ The risks associated with the product. A high-risk product needing a lot of test- \ning will suffer much more severe impacts if testing is not estimated correctly, \nespecially if it is under-estimated. \n® The quality of the test basis. We want sufficient product documentation so that \nthe testers can figure out what the system is, how it is supposed to work and \nwhat correct behaviour looks like. In other words, adequate and high-quality \ninformation about the test basis will help us do a better, more efficient job of \ndefining the tests. \n® The size of the product. A larger product leads to increases in the size of the \nproject and the project team. This will also increase the difficulty of predicting \nand managing the projects and the team. This leads to the disproportionate rate \nof collapse of large projects, \n® The requirements for quality characteristics such as usability, reliability, secu- \nrity, performance etc. also influences the testing effort. These test types can be \nexpensive and time-consuming. \n® The complexity of the product domain. Examples of complexity considerations \ninclude: \n= The difficulty of comprehending and correctly handling the problem the \nsystem is being built to solve, for example avionics and oil exploration \nsoftware. \n~ The use of innovative technologies, especially those long on hyperbole and \nshort on proven track records. \n=~ The need for intricate and perhaps multiple test configurations, especially \nwhen these rely on the timely arrival of scarce software, hardware and other \nsupplies, \n~ The prevalence of stringent security rules, strictly regimented processes or \nother regulations, \n® The geographical distribution of the team, especially if the team crosses time \nzones (as many outsourcing efforts do). \n® The required level of detail for test documentation, While good project \ndocumentation is a positive factor, it is also true that having to produce detailed \ndocumentation, such as meticulously specified test cases, results in delays. \nDuring test execution, having to maintain such detailed documentation requires \nlots of effort, as does working with fragile test data that must be maintained or \nrestored frequently during testing. \n® Requirements for legal and regulatory compliance. If you are working in \na regulated industry, the time and effort taken to meet those regulatory \nrequirements can be significant. \no S04 Compoge Loty N3 Bt Bt vl My et b oo, s o4 Bl 0 b 18 ot (ko 3 ot . ot ety st ey b smpqpesmed B e ol sndis Akt s el s A e s gyt oot s et sy e el ey pericnce Cengage Liwring AT e (W 14 Y adibicnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 184,
            "page_label": "185"
        }
    },
    {
        "page_content": "172 Chapter 5 Test management \nDevelopment process characteristics \nThe life cycle and development process in use has an impact on how the test effort \nis spent. Testing is quite different in Agile development compared to a sequential \ndevelopment life cycle, and other aspects of development also influence testing: \n® The stability and maturity of the organization. Mature organizations tend to \nhave better requirements. architecture and unit tests, thus saving test effort later \nin the life cycle. \n® The development life cycle model in use. The life cycle model itself is an influ- \nential process factor, as the V-model tends to be more fragile in the face of late \nchange while incremental models such as Agile development tend to have high \nregression testing costs. \n® The test approach. Choosing the right approach is important for good testing, as \nwe have seen. With a less than ideal approach, testing will take longer and take \nmore effort than is necessary. \n® The tools used. Tools are supposed to increase efficiency and reduce time spent \non some tasks, so test tols, especially those that reduce the effort associated \nwith test execution which is on the critical path for release, should decrease exe- \ncution time for tests, Of course, other factors about automation may be far more \nsignificant, as we will see in Chapter 6. On the development side, debugging \ntools and a dedicated debugging environment (as opposed to debugging in the \ntest environment) also reduce the time required to complete testing. \n® The test process. A test process that is well-understood. with testers trained to \nperform the activities and tasks they need to do in the most effective and effi- \ncient way is the optimum. Test process maturity is another factor, since more \nmature testing will be more efficient and effective. \no Time pressure. This is another factor to be considered. Pressure should not \nbe an excuse to take unwarranted risks. However, it is a reason to make care- \nful, considered decisions and 1o plan and re-plan intelligently throughout the \nprocess, which is another hallmark of mature processes. \nPeople characteristics \nPeople execute the process, and people factors are as important or more important \nthan any other. Indeed. even when many troubling things are true about a project, \nan excellent team can often make good things happen on the project and in testing. \nTmportant people factors include: \n® The skills and experience of the people involved. The skills of individuals and \nthe team as a whole are important, as well as the alignment of those skills with \nthe project’s needs. Domain knowledge is likely to be more relevant when the \nproblem being solved is complex, thus requiring specific knowledge on the part \nof the tester 1o operate the software and to determine correct versus incorrect \nbehaviour, \n® Team cohesion and leadership. Since a project team is a team, solid relation- \nships, reliable execution of agreed-upon commitments and responsibilities \nand a determination to work together toward a common goal are important. \nThis is especially important for testing, where so much of what we test, \nG N Compoge Lossming. A3 K Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 i, e, s sty comto ey b gy B e el smbs At Ll e Aas i 1 appresact comem dres o ey S8 -l Warag peencs Cenpuge Lowmay BuTSCs e 1 1wt kot o o 1} € o A (7T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 185,
            "page_label": "186"
        }
    },
    {
        "page_content": "Section 2 Test Planning and Estimation 173 \nuse and produce either comes from, relies upon or goes to people outside \nthe testing group. Because of the importance of trusting relationships and \nthe lengthy learning curves involved in software and system engineering, the \nstability of the project team is an important people factor, too. \nTest results \nThe test results themselves are important in the total amount of test effort during \ntest execution: \n® The number and severity of defects found. Wouldn't testing be easy if we never \nfound any defects? Initial tests would just run with no problems, and there \nwould be no need for any tests to be repeated. However, in the real world, the \nmore defects there are and the more severe those defects, the greater the impact \non the testing and the test estimates, \n® The amount of rework required. Delivery of good quality software at the \nstart of test execution and quick, solid defect fixes during test execution pre- \nvents delays in the test execution process, A defect, once identified, should \nnot have to go through multiple cycles of fix/retest/re-open, at least not if the \ninitial estimate is going to be held 1o, Good design of the test object should \nmake changes easier. For example, good modular design may have a low- \nlevel function called by a number of higher-level functions. If a change is \nmade in the low-level function, the functions calling it should work without \nbeing changed (if correctly designed). \nYou probably noticed from this list that we included a number of factors outside \nthe scope and control of the test manager, Indeed, events that occur before or after \ntesting can bring these factors about. For this reason, it is important that testers, \nespecially test managers, be attuned to the overall context in which they operate. \nSome of these contextual factors result in specific project risks for testing, which \nshould be addressed in the test plan. Project risks are discussed in more detail in \nSection 5.5. \n5.2.6 Test estimation techniques \nThere are two techniques for estimation covered by the ISTQB Foundation Sylla- \nbus. One involves consulting the people who will do the work and other people with \nexpertise on the tasks to be done (expert-based). The other involves analyzing metrics \nfrom past projects and from industry data (metrics-based). Let's look at each in turn. \nAsking the individual contributors and experts involves working with experienced \nstaff members to develop a work-breakdown structure for the project. With that done, \nyou work together to understand, for each task, the effort, duration, dependencies and \nresource requirements. The idea is to draw on the collective wisdom of the team to \ncreate your test estimate. Using project management software or a whiteboard and \nsticky notes, you and the team can then predict the testing end date and major mile- \nstones, This technigue is often called bottom-up estimation, because you start at the \nlowest level of the hierarchical breakdown in the work-breakdown structure (the task) \nand let the duration, effort. dependencies and resources for each task add up across \nall the tasks. This is the expert-based technique. \nG N Compoge Lonsming. A3 K Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 st e, st sty comto ey b gy B e el smbs At s el e A e gyt oot s e sy SR Tl g eTicncs CEngage Ly BTG e (W 14 Y adiBmcnd o o o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 186,
            "page_label": "187"
        }
    },
    {
        "page_content": "174 Chapter 5 Test management \nAnalyzing metrics can be as simple or sophisticated as you make it. The sim- \nplest approach is to ask, ‘How many testers do we typically have per developer on \na project?” A somewhat more reliable approach involves classifying the project in \nterms of size (small, medium or large) and complexity (simple, moderate or complex) \nand then seeing on average how long projects of a particular size and complexity \ncombination have taken in the past. Another simple and reliable approach we have \nused is to look at the average effort per test case in similar past projects and to \nuse the estimated number of test cases 1o estimate the total effort. Sophisticated \napproaches involve building mathematical models in a spreadsheet or other tool \nthat look at historical or industry averages for certain key parameters ~ number of \ntests run by a tester per day, number of defects found by a tester per day, etc. - and \nthen plugging in those parameters to predict duration and effort for key tasks or \nactivities on your project. The tester-to-developer ratio is an example of a top-down \nestimation technigue, in that the entire estimate is derived at the project level, while \nthe parametric technigue is bottom-up, at least when it is used 1o estimate individual \ntasks or activities. \nWe prefer to start by drawing on the team's wisdom 1o create the work-breakdown \nstructure and a detailed bottom-up estimate. We then apply models and rules of \nthumb to check and adjust the estimate bottom-up and top-down using past history. \nThis approach tends to create an estimate that is both more accurate and more defen- \nsible than cither technique by itself. \nEven the best estimate must be negotiated with management. Negotiating ses- \nsions exhibit amazing variety, depending on the people involved. However, there \nare some classic negotiating positions. It is not unusual for the test manager to try \n10 sell the management team on the value added by the testing or to alert man- \nagement to the potential problems that would result from not testing enough. It is \nnot unusual for management 1o look for smart ways to accelerate the schedule or \nto press for equivalent coverage in less time or with fewer resources. In between \nthese positions, you and your colleagues can reach compromise, if the parties are \nwilling. Our experience has been that successful negotiations about estimates \nare those where the focus is less on winning and losing and more about figuring \nout how best 10 balance competing pressures in the realms of quality, schedule, \nbudget and features. \nIn Agile development, planning poker is an example of the expert-based technique; \nteam members estimate based on their own experience of the effort needed to deliver \nand test a feature. Burndown charts are an example of the metrics-based technique, \nsince the effort being spent is captured and reported and used 1o feed into the team’s \nvelocity to determine the amount of work the team can do in the next iteration. This \nis actually monitoring of the current sprint in order to use the results 1o help pre- \ndict and estimate the effort needed for the following sprint. The ISTQB Foundation \nLevel Agile Tester Extension Syllabus has more on estimation of testing in Agile \ndevelopment. \nIn sequential life cycle models, a technique such as Wideband Delphi estimation is \nan example of expert-based estimation, since groups of engineers provide estimates \nwhich are then aggregated together. \nDefect removal models are examples of metrics-based techniques. Data is gath- \nered from previous projects about the number of defects and the time to remove them; \nthis provides a basis for future similar projects. More information about these are \ngiven in the ISTQB Advanced Level Test Manager Syllabus. \nG N Crmpogs Loswming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At s v e e s oo st P Conpge Lewmay mures e 13 10 ey akoacd o o sy Vo € whacacs A (7O s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 187,
            "page_label": "188"
        }
    },
    {
        "page_content": "Section 3 Test Montorng and Control 175 \n5.3 TEST MONITORING AND CONTROL \nSYLLABUS LEARNING OBJECTIVES FOR 5.3 TEST \nMONITORING AND CONTROL (K2) \nFL-5.3.1 Recall metrics used for testing (K1) \nFL-53.2 Summarize the purposes, contents and audiences for test \nreports (K2) \nIn this section, we'll review techniques and metrics that are commonly used for \nmonitoring test implementation and execution. We'll focus especially on the use and \ninterpretation of such test metrics for reporting. controlling and analyzing the test \neffort, including those based on defects and those based on test data. We'll also look \nat options for reporting test status using such metrics and other information. \nAs you read, remember 1o watch for the Glossary terms test control, test \nmonitoring. test progress report and test summary report. \nTest monitoring is concerned with gathering data and information about test \nactivities; test control is using that information to guide or control the remaining \ntesting. We will look briefly at test control in this introduction, then consider various \nmetrics that could be gathered, and finally discuss how information should be com- \nmunicated in test reports. \nProjects do not always unfold as planned. In fact, any human endeavour more com- \nplicated than a family picnic is likely to vary from plan, Risks become occurrences. \nStakeholder needs evolve. The world around us changes. When plans and reality \ndiverge, we must act 1o bring the project back under control, and testing is no exception. \nIn some cases, the test findings themselves are behind the divergence; for example, \nsuppose the quality of the test items proves unacceptably bad and delays test prog- \nress. In other cases, testing is affected by outside events; for example, testing can be \ndelayed when the test items show up late or the test environment is unavailable, Test \ncontrol is about guiding and corrective actions 1o try to achieve the best possible \noutcome for the project. \nThe specific corrective or guiding actions depend, of course, on what we are try- \ning to control. Consider the following hypothetical examples of test control actions: \n@ A portion of the software under test will be delivered late, after the planned test \nstart date. Market conditions dictate that we cannot change the release date. Test \ncontrol might involve re-prioritizing the tests so that we start testing against \nwhat is available now. \n@ For cost reasons, performance testing is normally run on weekday evenings \nduring off-hours in the production environment. Due to unanticipated high \ndemand for your products, the company has temporarily adopted an evening shift \nthat keeps the production environment in use I8 hours a day, five days a week. \nTest control might involve rescheduling the performance tests for the weekend. \n® The item being tested had previously met its entry (or exit) criteria, but since \nthe criteria were evaluated, the test item has changed due 1o defects found \nand resulting rework. Control actions may include re-evaluating the entry \n{or exit) criteria. \nTest monitoring A test \nmanagement actwity \nthat involves checking \nthe status of testing \nactivities, identifying \nany variances from the \nplanned or expected \nstatus and reporting \nstatus to stakeholders. \nTest control A test \nG N Crmpoge Lowming. A3 g Bt vt My et b o, s, o4 e . s 18 . o, U 3 st e, s sty conto ey b gy B e el b At Je—p—— e s 8 cven oy Conpage Lonmay murses e 1y 10wy akbacnd o o 1y o € whacacm YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 188,
            "page_label": "189"
        }
    },
    {
        "page_content": "176 Chapter 5 Test management \nWhile these examples show test control actions that affect testing, the project team \nmight also have to take some actions that affect others on the project. For example, \nsuppose that the test completion date is at risk due 1o a high number of defect fixes \nthat fail confirmation testing in the test environment. In this case, test control might \ninvolve requiring the developers making the fixes to thoroughly retest the fixes prior \nto checking them in to the code repository for inclusion in a test build. \n5.3.1 Metrics used in testing \nThe purpose of metrics in testing \nHaving developed our plans, defined our test strategies and approaches, and estimated \nthe work to be done, we must now track our testing work as we carry it out. Test \nmonitoring can serve various purposes during the project, including the following: \n@ Give the test team and the test manager feedback on how the testing work is \ngoing, allowing opportunities to guide and improve the testing and the project. \nThis would be done by collecting time and cost data about progress versus the \nplanned schedule and budget. \n@ Provide the project team with visibility about the test results and the quality of \nthe test object, \n® Measure the status of the testing, test coverage and test items against the exit \ncriteria to determine whether the test work is done, and to assess the effective- \nness of the test activities with respect to the objectives. \no Gather data for use in estimating future test efforts, including the adequacy \nof the test approach, \nEspecially for small projects, the test manager or a delegated person can gather \ntest progress monitoring information manually using documents, spreadsheets and \nsimple databases. When working with large teams, distributed projects and long-term \ntest efforts, we find that the efficiency and consistency of data collection is aided by \nthe use of automated tools (see Chapter 6). \nCommon test metrics \nWe will show some examples of using metrics in testing and conclude this section \nwith a list of other common test metrics, \nIn Figure 5.1, columns A and B show the test ID and the test case or test suite \nname. The state of the test case is shown in column C (\"Warn® indicates a test that \nresulted in a minor failure). Column D shows the tested configuration, where the \ncodes A, B and C correspond to test environments described in detail in the test \nplan. Columns E and F show the defect (or bug) 1D number (from the defect tracking \ndatabasc) and the risk priority number of the defect (ranging from 1, the highest \nrisk, 10 25, the least risky). Column G shows the initials of the tester who ran the \ntest. Columns H to L capture data for each test related to dates, effort and duration \n(in hours). We have metrics for planned and actual effort and dates completed which \nwould allow us to summarize progress against the planned schedule and budget. This \nspreadsheet can also be summarized in terms of the percentage of tests which have \nbeen run and the percentage of tests which have passed and failed. \nFigure 5.1 might show a snapshot of test progress during the test execution period, \nor perhaps even at test closure if it were deemed acceptable to skip some of the tests, \nG N Compogs Lowming A3 g Bt vt My et b o, s, o4 A s, 0 s 10 . o, (b 3 st e, s s paty conto ey b gy B e el b At s el e M e gyt oot s e Sy o el ey ericncs Cengg L AT e (W 14 Y albnnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 189,
            "page_label": "190"
        }
    },
    {
        "page_content": "Section 3 Test Monttoring and Control 177 \nW - i \nP | 4 \ng I [ al— 3 r : L 20 \nR PorformanceVsiress { \n;%ﬁnnm Virn %&gme4‘;&%o VIF N - - plan 1711 \ni Suite Summary. i i 1‘% 194 Ig 2 \n13000 | Ervor Handingecovery I 113001 | Comupt File Fal A [701] 1 (OW| @18 | 48 8 1 06 2 1 7071 4 \n09 \n}mm A ;r!g‘”gtm‘ LW P — ” i {100 \n2 ‘Suite Summary i e 8 W 1 ....... e — I I —— | e . = \niﬁ’?w Skip 1 \n> et — — 4 — - o \nLt = 1 - ) \nFIGURE 5.1 Test case summary worksheet \nDuring the analysis, design and implementation of the tests, such a worksheet would \nshow the state of the tests in terms of their state of development. \nIn addition 10 test case status, it is also common to monitor test progress during the \ntest execution period by looking at the number of defects found and fixed. Figure 5.2 \nshows a graph that plots the total number of defects opened and closed over the course \nof the test execution so far. \nTt also shows the planned test period end date and the planned number of defects \nthat will be found. Ideally, as the project approaches the planned end date, the total \nnumber of defects opened will settle in at the predicted number and the total number \nof defects closed will converge with the total number opened. These two outcomes tell \nus that we have found enough defects to feel comfortable that we have finished, that \nwe have no reason to think many more defects are lurking in the product and that all \nknown defects have been resolved. \nCharts such as Figure 5.2 can also be used to show failure rates or defect density. \nWhen reliability is a key concern, we might be more concerned with the frequency \nwith which failures are observed than with how many defects are causing the failures. \nIn organizations that are looking to produce ultra reliable software, they may plot the \not 3000 Compoge Loy A3 Boghts Bt nd My et b o, s o bl s, 3 e 18 1 b 3 ot . e Sl ey ot ey b s B el b bt bl i A e sy gt oot s et Bty 1 el g icncs Cengage Lenang T (0 0 vt A s 1 e € b M s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 190,
            "page_label": "191"
        }
    },
    {
        "page_content": "178 Chapter 5 Test management \nIntegration and System Test Execution \nSystem Quality Problems Analysis \n2500 \nProdicted number of bugs IST will | 2250 \nfind by planned project end date, 321 2000 \n1750 \n1500 \n11250 \n1000 \nContemation testing of faed bugsin | o \neach new release makes the total \ndosed curve jump on Mondays 500 \nand Tuesdays. 250 \n107 1021 118 1118 122 3216 1230 113 127 N0 R4 N0 \nDate Opened/C losed \nKey Milestones Total Opened  —— \nIntegration Test Entry 1077 Integration Test Ext 2/14 Total Closed  —— \nSystem Test Entry 10/21 System Test Ext 321 N8, Closed counts incude deferred bugs \nFIGURE 5.2 Total defects opened and closed chart \nnumber of unresolved defects normalized by the size of the product, either in thou- \nsands of source lines of code (KSLOC), function points (FP) or some other metric \nof code size. Once the number of unresolved defects falls below some predefined \nthreshold ~ for example, three per million lines of code ~ then the product may be \ndeemed to have met the defect density exit criteria, \nMeasuring test progress based on defects found and fixed is common and useful, \nif used with care. Avoid using defect metrics alone, as it is possible to achieve a flat \ndefect find rate and to fix all the known defects by stopping any further testing, by \ndeliberately impeding the reporting of defects and by allowing developers to reject, \ncancel or close defect reports without any independent review. \nThat said. test progress monitoring techniques vary considerably depending on \nthe preferences of the testers and stakeholders, the needs and goals of the project, \nregulatory requirements, time and money constraints and other factors. \nCommon metrics for test progress monitoring include: \n® The percentage of planned work done in test case preparation (or percentage of \nplanned test cases implemented). \n@ The percentage of planned work done in test environment preparation. \n® Metrics relating to test execution, such as the number of test cases run and not \nrun, the number of test cases passed or failed, or/or the number of test condi- \ntions passed or failed. \n® Metrics relating to defects, such as defect density, defects found and fixed, fail- \nure rate and confirmation test results. \n® The extent of test coverage achieved, measured against requirements, user stories, \nacceptance criteria, risks, code, configurations or other areas of interest. \no N Compogs Loy A3 Wi Batnd Moy et b copi, s, o4 B i, 0 s 18 o, (ko 3 o, e, ot sty commoss ey b smpqreseed B e ol smbis At s el i A e gyt oot s e sy 4 el ey pericnce Cengag Ly BT e (W 14 Y adibicnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 191,
            "page_label": "192"
        }
    },
    {
        "page_content": "Section 3 Test Monttoring and Control 179 \n@ The status of the testing (including analysis, design and implementation) com- \npared to various test milestones, that is, task completion, resource allocation and \nusage, and effort, \n® The economics of testing, such as the costs and benefits of continuing test \nexecution in terms of finding the next defect or running the next test. \nAs a complementary monitoring technique, you might assess the subjective level \nof confidence the testers have in the test items. However, avoid making important \ndecisions based on subjective assessments alone, as people’s impressions have a way \nof being inaccurate and coloured by bias. \n5.3.2 Purpose, contents and audiences for test reports \nThe purpose of test reports \nTest monitoring is about gathering detailed test data; reporting test progress is \nabout effectively communicating our findings to other project stakeholders. As \nwith test progress monitoring, in practice there is wide variability observed in how \npeople report test progress, with the variations driven by the preferences of the \ntesters and stakeholders, the needs and goals of the project, regulatory require- \nments, time and money constraints and limitations of the tools available for test \nprogress reporting. Often variations or summaries of the metrics used for test \nprogress monitoring. such as Figure 5.1 and Figure 5.2, are used for test progress \nreporting, 100, Regardless of the specific metrics, charts and reports used, test \nprogress reporting is about helping project stakeholders understand the results of a \ntest period, especially as it relates to key project goals and whether (or when) exit \ncriteria were satisfied. \nIn addition to notifying project stakeholders about test results, test progress report- \ning is often about enlightening and influencing them. This involves analyzing the \ninformation and metrics available to support conclusions, recommendations and deci- \nsions about how to guide the project forward or to take other actions. For example, we \nmight estimate the number of defects remaining to be discovered, present the costs \nand benefits of delaying a release date to allow for further testing, assess the remain- \ning product and project risks and offer an opinion on the confidence the stakeholders \nshould have in the quality of the system under test. \nYou should think about test progress reporting during test planning, since you will \noften need 1o collect specific metrics during and at the end of a test period to generate \nthe test progress reports in an effective and efficient fashion, The specific data you \nwill want to gather will depend on your specific reports, but common considerations \ninclude the following: \n® How will you assess the adequacy of the test objectives for a given test level and \nwhether those objectives were achieved? \n@ How will you assess the adequacy of the test approaches taken and whether they \nsupport the achievement of the project’s testing goals? \n® How will you assess the effectiveness of the testing with respect to these \nobjectives and approaches? \nFor example. if you are doing risk-based testing, one main test objective is to \nsubject the important product risks to the appropriate extent of testing. Table 5.1 \nshows an example of a chart that would allow you to report your test coverage and \nG N Crmpogs Lowming. A3 Kighs Bt vt My et b o, s, o4 A i, 0 s 10 . o, U 3 ottt e, s sty comto ey b gy B e el snbs At \nJe—p—— Conpaet e e b ety e s e (e 4 e o o o o € g 0 (F s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 192,
            "page_label": "193"
        }
    },
    {
        "page_content": "180 Chapter 5 Test management \nTABLE 5.1 Risk coverage by defects and tests \nUnresolved defects Test cases to be run \nProduct risk areas Number % Planned  Actual % \nPerformance, load, relabifity 304 28 3843 1,512 39 \nRobustness, operations, security 234 21 1,032 432 a2 \nFunctionality, data, dates 224 20 4,744 2,043 43 \nUse cases, user interfaces, locakization 160 15 498 318 64 \nInterfaces 93 8 193 153 79 \nCompatibiity n 6 1,787 939 53 \nOther 21 2 760 306 40 \n1,107 100 12,857 5,703 £ \nunresolved defects against the main product risk areas you identified in your risk \nanalysis. If you are doing requirements-based testing, you could measure coverage \nin terms of requirements or functional areas instead of risks. \nThe content of test reports \nTest progress report The Syllabus discusses two types of test report: a test progress report and a \n(test status report) test summary report. Both reports contain a lot of information in common; the \nA test report produced main difference is that test progress reports are used at regular intervals throughout \nat regular intervals the project, during test activities, and would result in test control actions; the test \nabout the progress of summary report is in a sense the final test progress report for the whole project, and \nf‘l m”g?:: is prepared at the end of a test activity or test level. For example, a test summary \nMW!.S requiring a report could also be used in a retrospective. \ndecision. Because test progress reports are reporting on more dynamic information, there \nare some things which are included in this report that are no longer relevant for a test \n:!; summary report summary report, including: \nA test report that ® The current status of the test activities and progress against the test plan. \nprovides an evaluation ® Factors that are currently impeding the progress of the testing, \nz:\" Wmﬁ ® The testing planned for the next period, 1o be including in the next test progress \nariteria. sl \n® The quality of the test object as currently assessed by the testing, \n‘When exit eriteria are met, the test manager issues the test summary report, Such \na report, created either at a key milestone or at the end of a test level, describes the \nresults of a given test level. In addition 1o including the kind of charts and tables \nshown carlier (but now at the end of the time period being monitored), you might \ndiscuss important events (especially problematic ones) that occurred during testing, \nthe objectives of testing and whether they were achieved, the test approach, strategies \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 b . 0 b 10 . o, (b 3 i, e, s sty conto ey b gy B e el snbs At e oo st s 8 cven earwny Conprge Lowmay muries e 14 10 ey akbacnd o o 14} o € whacgacm A (70Tt gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 193,
            "page_label": "194"
        }
    },
    {
        "page_content": "Section 4 Configuration Management 181 \nfollowed, how well they worked and the overall effectiveness of the test effort, The \ntest summary report draws on the test progress reports over the duration of the project \nor test level, particularly the last one. \nThe contents of both test progress reports and test summary reports may include \nthe following: \n® A summary of the testing performed. \no Information about what occurred during the period the report covers. \n® Deviations from plan, with regard to the schedule, duration or effort of test \nactivities. \n® The status of the testing and of product (test object) quality with respect 1o the \nexit criteria or definition of done. \n® The factors that have blocked or continue 1o block progress. \n® Relevant metrics, such as those relating 1o defects, test cases, test coverage, test \nactivity progress, and resource consumption, as described in Section 5.3.1. \n® Residual risks (see Section 5.5). \n® Reusable test work products produced. \nNot every test report will contain all this information (for example a quick software \nupdate), and some may include additional information (for example for a complex \nproject with many stakeholders or under legal and regulatory requirements). The con- \ntent of the report depends on the project, organizational requirements and the software \ndevelopment life cycle. For example, in Agile development, test progress reporting \nmay be incorporated into task boards, defect summaries and burndown charts, which \nmay be discussed during a daily stand-up meeting. For more about test reports in \nAgile projects, see the ISTQB-AT Foundation Level Agile Tester Extension Syllabus, \nThe audience for test reports and the effect on the report \nIn addition to the factors already mentioned that influence the content of a test report \n(either progress or summary), each test report should be tailored for the intended \naudience of the report. For example, a test summary report which is intended for the \nCEO and other high-level management should be short and concise, with overview \nsummaries of the main points. It may contain a summary of defects of high and \nother priority levels and the percentage of tests passed, but information about budget \nand schedule would be of more importance to them. A test progress report which is \nintended for testers and developers would include detailed information about defect \ntypes and trends, such as we saw in Figure 5.2. \nThe standard ISO/TEC/IEEE 291119-3 [2013] gives structures and examples of test \nprogress reports and what they call test completion reports (test summary reports). \n5.4 CONFIGURATION MANAGEMENT \nSYLLABUS LEARNING OBJECTIVE \nMANAGEMENT (K2) \nR 5.4 CONFIGURATION \nFL-54.1  Summarize how configuration management supports testing (K2) \nCopr N Crmpoge Loswming. A3 K Bt vt My et b o, s, o4 b, 0 s 18 . o, (b 3 i, g, s s sty comtos ey b gy B e el smbs At P e e dmd et sy oo st s 8 cvent barwny Conguge Lismng mrves e rgh b semeve o o sy e £ .",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 194,
            "page_label": "195"
        }
    },
    {
        "page_content": "182 Chapter 5 Test management \nIn this brief section, we'll look at how configuration management relates to and \nsupports testing. The only Glossary term is configuration management. \nConfiguration management is a topic that often perplexes new practitioners, but, \nif you ever have the bad luck to work as a tester on a project where this critical activity \nis handled poorly. you will never forget how important it is. Briefly put, configura- \ntion management is in part about determining cleardy what the items are that make \nup the software or system. These items include source code, test scripts, third-party \nsoftware (including tools that support testing), hardware, data and both development \nand test documentation. Configuration management is also about making sure that \nthese items are managed carefully, thoroughly and attentively throughout the entire \nproject and product life cycle. \nConfiguration management has a number of important implications for test- \ning. For one thing, it allows the testers to manage their testware and test results \nusing the same configuration management mechanisms, as if they were as valu- \nable as the source code and documentation for the system itself — which of course \nthey are. \nFor another thing. configuration management supports the build process, which is \nessential for delivery of a test release into the test environment, It is critical 1 have \na solid, reliable way of delivering test items that work and are the proper version. \nLast but not least, configuration management allows us to map what is being \ntested to the underlying files and components that make it up. This is absolutely \ncritical. For example, when we report defects, we need to report them against some- \nthing, something which is configuration controlled or version controlled. If it is \nnot clear what we found the defect in, the developers will have a very tough time \nof finding the defect in order 10 fix it. For the kind of test reports discussed earlier \nto have any meaning, we must be able to trace the test results back to what exactly \nwe tested, \nConfiguration management for testing may involve ensuring the following: \n® All test items of the test object are uniquely identified. version controlled, \ntracked for changes and related 10 each other, that is, what is being tested. \no Allitems of testware are uniquely identified, version controlled, tracked for \nchanges, related to each other and related to a version of the test item(s) so that \ntraceability can be maintained throughout the test process. \no Allidentified work products and software items are referenced unambigu- \nously in test documentation. \nIdeally, when testers receive an organized, version-controlled test release from a \nchange-managed source code repository, it is accompanied by release notes which \ncontain all the information shown. \nWhile our description was brief, configuration management is a topic that is as \ncomplex as test environment management. Advanced planning is critical to making \nthis work. During the project planning stage, and perhaps as part of your own test \nplan, make sure that configuration management procedures and tools are selected, \nAs the project proceeds, the configuration process and mechanisms must be imple- \nmented, and the key interfaces to the rest of the development process should be \ndocumented. Come test execution time, this will allow you and the rest of the project \nteam to avoid nasty surprises like testing the wrong software, receiving uninstallable \nbuilds and reporting unreproducible defects against versions of code that do not exist \nanywhere but in the test environment. \nG N Compoge Loswming. A3 Kt Bt vt My et b o, s, o4 B s, 0 s 10 . o, L 3 i g, s sty comtr ey b gy B e el smbs A P v e e s s & cven arwny Conpge Lewmay murses e 10 10 ey akoacnd o o} Vow € whacacs A (7 0TA s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 195,
            "page_label": "196"
        }
    },
    {
        "page_content": "Section 5 Risks and Testing 183 \n5.5 RISKS AND TESTING \nSYLLABUS LEARNING OBJECTIVES FOR 5.5 RISK \nAND TESTING (K2) \nFL-5.5.1 Define risk level by using likelihood and impact (K1) \nFL-55.2 Distinguish between project and product risks (K2) \nFL-5.5.3 Describe, by using examples, how product risk analysis may \ninfluence the thoroughness and scope of testing (K2) \nThis section covers a topic that we believe is eritical to testing: risk. Let’s look closely \nat risks, the possible problems that might endanger the objectives of the project stake- \nholders. We will discuss how to determine the kevel of risk using likelihood and \nimpact. We will see that there are risks related to the product and risks related to \nthe project, and look at typical risks in both categories. Finally, and most important, \nwe'll look at various ways that risk analysis and risk management can help us plot a \ncourse for solid testing \nAs you read this section, make sure to attend carefully to the Glossary terms \nproduct risk, project risk, risk, risk level and risk-based testing. \n5.5.1 Definition of risk \nRisk is a word we all use loosely, but what exactly is risk”? Simply put, it is the possi- \nbility of a negative or undesirable outcome. In the future, a risk has some likelihood \nbetween 0% and 100%; it is a possibility, not a certainty. In the past, however, cither \nthe risk has materialized and become an outcome or issue or it has not; the likelihood \nof arisk in the past is cither 0% or 100%. \nThe likelihood of a risk becoming an outcome is one factor to consider when \nthinking about the risk level associated with its possible negative consequences. The \nmore likely the outcome is. the worse the risk. However, likelihood is not the only \nconsideration. \nFor example, most people are likely to catch a cold in the course of their lives, usu- \nally more than once. The typical healthy individual suffers no serious consequences. \nTherefore the overall level of risk associated with colds is low for this person. But the \nrisk of a cold for an elderly person with breathing difficulties would be high. The poten- \ntial consequences or impact is an important consideration affecting the level of risk, two. \nRemember that in Chapter | we discussed how system context, and especially the \nrisk associated with failures, influences testing. Here, we'll get into more detail about \nthe concept of risks, how they influence testing and specific ways to manage risk. \n5.5.2 Product and project risks \nWe can classify risks into project risks (factors relating 1o the way the work is carried \nout, that is, the test project) and product risks (factors relating to what is produced by \nthe work, that is, the thing we are testing). We will look at product risks first, \no N Compogs Lonsming. A3 K Bt vt My et b o, s, o4 b, s 10 . o, U 3 s, g, s sty comto ey b sy B e el smbs At - \nRisk A factor that \ncould result in future \nnegative consequences. \na risk defined by impact \nand likelihood. \ne T oo st s & cven sy Conpe! s e e o e o sy e .",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 196,
            "page_label": "197"
        }
    },
    {
        "page_content": "184 Chapter 5 Test management \nProduct risks \nProduct risk A rsk You can think of a product risk as the possibility that the system or software might \nimpacting the quality of  fail to satisfy some reasonable customer, user or stakeholder expectation. Unsatisfac- \na product. tory software might omit some key function that the customers specified, the users \nrequired or the stakeholders were promised. It might be unreliable and frequently \nfail to behave normally. It might fail in ways that cause financial or other damage \n10 @ user or the company that user works for. It might have problems with data \nintegrity and data quality, such as data migration issues, data conversion problems, \ndata transport problems and violations of data standards such as field and referential \nintegrity. It might have problems related to a particular quality characteristic. The \nproblems might not involve functionality, but rather security, reliability, usability, \nmaintainability or performance. This type of quality attribute-related risk is some- \ntimes referred to as a ‘quality risk”. Generally, the software could fail to perform its \nintended functions, dissatisfying users and customers. If you can write a test for it, \nit's a product risk. \nHere are some example product risks: \n® Software might not perform its inended functions according to the \nspecification. \n@ Software might not perform its intended functions according to user. customer, \nand/or stakeholder needs or expectations (which is usually different from the firs!), \n® A particular computation may be performed incorrectly in some circumstances. \nA loop control structure may be coded incorrectly. \n® Response times may be inadequate for a high-performance transaction process- \ning system. \n® User experience (UX) feedback might not meet product expectations. \nProject risks \nWe just discussed risks to product quality. However, testing is an activity like the \nrest of the project and thus it is subject to risks that endanger the project. To deal \nProject risk A risk that  with the project risks that apply to testing. we can use the same concepts we apply \nImpacts project success. 1o identifying, prioritizing and managing product risks, which we will discuss in \nSection 5.3.3. \nRemembering that a risk is the possibility of a negative outcome, what project risks \naffect testing? There are direct risks such as the late delivery of the test items to the \ntest team or availability issues with the test environment. There are also indirect risks \nsuch as excessive delays in repairing defects found in testing or problems with getting \nprofessional system administration support for the test environment. \nOf course, these are merely four examples of project risks: many others can \napply to your testing effort. To discover these risks, ask yourself and other project \nparticipants and stakeholders: *“What could go wrong on the project to delay or \ninvalidate the test plan, the test strategy and the test estimate? What are unac- \nceptable outcomes of testing or in testing? What are the likelihoods and impacts \nof each of these risks?\" This process is very much like the risk analysis process \nfor products. Checklists and examples can help you identify test project risks \n[Black 2004]. \nThe Syllabus gives a good list of project risks in different categories. We list them \nhere along with some examples and ways to deal with them, \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 30 ottt g, s sty comto ey b gyt B e el b At o v e e e oo s et P Conpage Linmay murses e 1y 10wy akbacnd o o a1 W € whacgacm YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 197,
            "page_label": "198"
        }
    },
    {
        "page_content": "Section 5 Risks and Testing 185 \nProject issues: \n® Delays may occur in delivery, task completion or satisfaction of exit criteria or \ndefinition of done. For example, logistics or product quality problems may block \ntests: These can be mitigated through careful planning, good defect triage and \nmanagement, and robust test design. \n® Inaccurate estimates, reallocation of funds to higher priority projects, or general \ncost cutting across the organization may result in inadequate funding. \n® Late changes may result in substantial rework. For example, excessive change to the \nproduct may invalidate test results or require updates to test cases, expected results \nand environments. These can be mitigated through good change control processes, \nrobust test design and lightweight test documentation. When severe defects occur, \ntransference of the risk by escalation to management is often in onder. \nOrganizational issues: \n® Skills, training and staff may not be sufficient. For example, shortages of peo- \nple. skills or training may lead to problems with communicating and responding \n1o test results, unrealistic expectations of what testing can achieve, and need- \nlessly complex project or team organization. \n® Personnel issues may cause conflict and problems, affecting work. For example, \nif two people do not get along, working against each other rather than toward \na common goal (it happens), this is demoralizing to the whole team as well as \ndegrading the quality of everyone's work. \n® Users, business staff or subject matter experts may not be available due to \nconflicting business priorities. \nPolitical issues: \n® Testers may not communicate their needs and/or the test results adequately. For \nexample, high-level managers may not realize the need for ongoing support, \ntime and effort for test automation after an initial start, if testers and automators \ndo not communicate the need for this. \n® Developers and/or testers may fail 1o follow up on information found in testing \nand reviews (for example not improving development and testing practices). \n® There may be an improper attitude toward., or expectations of, testing (for \nexample not appreciating the value of finding defects during testing). \nTechnical issues: \n® Requirements or user stories may not be clear enough or well enough defined. \nFor example, ambiguous, conflicting or unprioritized requirements, an exces- \nsively large number of requirements given other project constraints, high system \ncomplexity and quality problems with the design, the code or the tests mean that \nthe system will take longer to develop and may not meet customer expectations. \nThe best cure for this is clear unambiguous requirements. \n® The requirements may not be met, given existing constraints, For example, a \nrequirement may want the app to be able to check the user’s bank balance to see \nif they can pay for what they are ordering, but the bank software does not allow it. \not S0 Crmpoge Loy N3 Bt Bt vl My et b oo s o4 bl 0 s 18 ot (o 3 o, . ot ety st ey b smpqresmed B e ol snbis Akt s el i s v gyt oot s e Sy S B Tl g eTicncs CEngage L BTG e (W 14 Y adiBicnd o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 198,
            "page_label": "199"
        }
    },
    {
        "page_content": "186 Chapter 5 Test management \n® The test environment may not be ready on time or may not be adequate, For \nexample, insufficient or unrealistic test environments may yield misleading \nresults, including false positives and false negatives, One option is to transfer \nthe risks 10 management by explaining the limits on test results obtained in lim- \nited environments, Mitigation, sometimes complete alleviation, can be achieved \nby outsourcing tests such as performance tests that are particularly sensitive to \nproper test environments, or by using virtualization. \n® Data conversion, migration planning and their tool support may be late, \no Weaknesses in the development process may impact the consistency or quality \nof project work products such as design, code, configurations, test data and \ntest cases. For example, test items may not install in the test environment. This \ncan be mitigated through smoke (or acceptance) testing prior to starting other \ntesting or as part of a nightly build or continuous integration, Having a defined \nuninstall process is a good contingency plan. \no Poor defect management and similar problems may result in accumulated \ndefects and other technical debt. \nSupplier issues: \n® A third party may fail to deliver a necessary product or service or go bankrupt. \no Contractual issues may cause problems to the project. For example, problems \nwith underlying platforms or hardware, failure to consider testing issues in \nthe contract, or failure to properdy respond to the issues when they arise can \nquickly add up to serious delays as well as time-consuming negotiations with a \nsupplier, The best way to mitigate this is to ensure that the contract is solid to \nbegin with, and have the technical details reviewed by testers or a test manager. \nProject risks do not just affect testing of course, they also affect development. In \nsome organizations, project managers are responsible for dealing with all project \nrisks, but sometimes test managers are given responsibility for test-related project \nrisks (as well as product risks). \nThere may be other risks that apply to your project and not all projects are subject \n10 the same risks. See Chapter 2 of Black [2009], Chapters 6 and 7 of Black [2004] \nand Chapter 3 of Craig and Jaskiel [2002] for a discussion on managing project risks \nduring testing and in the test plan. \nFinally, don’t forget that test items can also have risks associated with them. For \nexample, there is a risk that the test plan will omit tests for a functional area or that \nthe test cases do not exercise the critical areas of the system. \n5.5.3 Risk-based testing and product quality \nRisk management \nDealing with risks within an organization is known as risk management, and testing \nis one way of managing aspects of risk. For any risk, product or project, you have \nfour typical options: \n@ Mitigate: Take steps in advance to reduce the likelihood (and possibly the \nimpact) of the risk, \no Contingency: Have a plan in place to reduce the impact should the risk become \nan outcome. \nG N Compoge Loswming. A3 Kt Bt vt My et b o, s, o4 b i, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At e e v et oo st s & cven arwny Conpge Lewmay murses e 1y 10 ey akoacnd comwm o iy Vow € whacuacs A (7O ts Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 199,
            "page_label": "200"
        }
    },
    {
        "page_content": "Section 5 Risks and Testing 187 \n® Transfer: Convince some other member of the team or project stakeholder to \nreduce the likelihood or accept the impact of the risk. \n® Ignore: Do nothing about the risk, which is usually a smart option only \nwhen there’s little that can be done or when the likelihood and impact \nare low. \nThere is another typical risk management option, buying insurance, which is not \nusually pursued for project or product risks on software projects, though it is not \nunheard of. \nRisk management activities include: \no Identifying and analyzing (and re-evaluating on a regular basis) what can go \nwrong. \n® Determining the priority of risks, which risks are the most important to deal \nwith. \n® Implementing actions to mitigate those risks, to reduce their likelihood or \nimpact or both. \n® Making contingency plans to deal with risks if they do happen. \n‘We can deal with test-related risks to the project and product by applying some \nstraightforward, structured risk management techniques, The first step is to assess \nor analyze risks early in the project. Like a big ocean liner, projects, especially large \nprojects, require steering well before the iceberg is in plain sight. By using a test \nplan, you can remind yourself to consider and manage risks during the planning \nactivity. \nRisk-based testing \nRisk-based testing is the idea that we can organize our testing efforts in a way  Risk-based testing \nthat reduces the residual level of product risk when the system is delivered. Risk-  Testing in which the \nbased testing uses risk 1o prioritize and emphasize the appropriate tests during ~ Management, selection, \ntest execution, but it is about more than that. Risk-based testing starts carly in Ww.‘d use \nthe project, identifying risks 10 system quality and using that knowledge of risk  Of 1esting activities and \nto guide testing planning, specification, preparation and execution. Risk-based umummﬁdm \ntesting involves both mitigation (testing to provide opportunities to reduce the m types \nlikelihood of defects, especially high-impact defects) and contingency (testing \nto identify work-arounds 1o make the defects that do get past us less painful). \nRisk-based testing also involves measuring how well we are doing at finding and \nremoving defects in critical areas, as was shown in Table 5.1. Risk-based testing \ncan also involve using risk analysis to identify proactive opportunities to remove \nor prevent defects through non-testing activities and to help us select which test \nactivities to perform, \nMature test organizations use testing to reduce the risk associated with delivering \nthe software to an acceptable level [Beizer 1990, Hetzel 1988]. In the middle of the \n19905, a number of testers, including us, started 10 explore various techniques for \nrisk-based testing. In doing so, we adapted well-accepted risk management concepts \n1o software testing. Applying and refining risk assessment and management tech- \nnigues are discussed in Black [2004] and Black [2009]. For two alternative views, see \nChapter 11 of Pol, Teunissen and van Veenendaal [2002] and Chapter 2 of Craig and \nJaskiel [2002]. The origin of the risk-based testing concept can be found in Chapter | \nof Beizer [1990] and Chapter 2 of Hetzel [1988]. \nG N Crmpogs Lowming. A3 K Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 3 st g, s sty comtr ey b gy B e el b At v e e e oo s et e et Conpage Lonmay muries e 1y 10 ey kbacnd o o 0} Vo € hacacm A (74T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 200,
            "page_label": "201"
        }
    },
    {
        "page_content": "188 Chapter 5 Test management \nRisk analysis \nRisk-based testing starts with product risk analysis. One technique for risk analysis \nis a close reading of the requirements specification, user stories, design specifica- \ntions, user documentation and other items. Another technique is brainstorming with \nmany of the project stakeholders. Another is a sequence of one-to-one or small group \nsessions with the business and technology experts in the company. Some people use \nall these techniques when they can. To us, a team-based approach that involves the \nkey stakeholders and experts is preferable to a purely document-based approach, as \nteam approaches draw on the knowledge, wisdom and insight of the entire team to \ndetermine what to test and how much. \nWhile you could perform the risk analysis by asking, “What should we worry \nabout?\" usually more structure is required to avoid missing things. One way to pro- \nvide that structure is to look for specific risks in particular product risk catego- \nries. You could consider risks in the areas of functionality, localization, usability, \nreliability, performance and supportability. Alternatively, you could use the quality \ncharacteristics and sub-characteristics from ISO/IEC 25010 [2011] (introduced in \nChapter 2), as each sub-characteristic that matters is subject to risks that the system \nmight have troubles in that area. You might have a checklist of typical or past risks \nthat should be considered. You might also want to review the tests that failed and the \nbugs that you found in a previous release or a similar product. These lists and reflec- \ntions serve 1o jog the memory, forcing you to think about risks of particular types, as \nwell as helping you structure the documentation of the product risks. \nWhen we talk about specific risks, we mean a particular kind of defect or failure \nthat might oceur, For example, if you were testing the calculator utility that is bundled \nwith Microsoft Windows, you might identify incorrect calculation as a specific risk \nwithin the category of functionality. However, this is too broad, Consider incorrect \naddition. This is a high-impact kind of defect, as everyone who uses the calculator \nwill see it, It is unlikely, since addition is not a complex algorithm. Contrast that with \nan incorrect sine calculation. This is a low-impact kind of defect, since few people \nuse the sine function on the Windows calculator. It is more likely to have a defect, \nthough, since sine functions are hard to calculate. \n1t's worth making the point here that in early risk analysis, we are making edu- \ncated guesses. Some of those guesses will be wrong, Make sure that you plan to \nre-assess and adjust your risks at regular intervals in the project and make appropriate \ncourse corrections 1o the testing or the project itself. \nOne common problem people have when organizations first adopt risk-based test- \ning is a tendency to be excessively alarmed by some of the risks once they are clearly \narticulated, Don't confuse impact with likelihood or vice versa, You should manage \nrisks appropriately, based on likelihood and impact. Triage the risks by understanding \nhow much of your overall effort can be spent dealing with them. \nIt’s very important to maintain a sense of perspective, a focus on the point of \nthe exercise. As with life, the goal of risk-based testing should not be, but cannot \npractically be, a risk-free project. What we can accomplish with risk-based testing is \nthe marriage of testing with best practices in risk management to achieve a project \noutcome that balances risks with quality, features, budget and schedule. \nAssigning a risk level \nAfter identifying the risk items, you and, if applicable, the stakeholders, should \nreview the list to assign the likelihood of problems and the impact of problems asso- \nciated with cach one. There are many ways to go about this assignment of likelihood \not S04 Crmpoge Loy N3 Kt Btk My et b o s o4 B 0 b 18 o (b 3 o, . ot oty ot ey b smpqpemnd B e ol snbis A ket 11 v e e e oo e et e Tt Conpuge Lowmay murses e 1y 10 et kbacnd o o s} W € whbacacm A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 201,
            "page_label": "202"
        }
    },
    {
        "page_content": "Section 5 Risks and Testing 189 \nand impact. You can do this with all the stakeholders at once. You can have the \nbusiness people determine impact and the technical people determine likelihood. and \nthen merge the determinations, Either way, the reason for identifying risks first and \nthen assessing their level, is that the risks are relative to each other. \nThe scales used to rate likelihood and impact vary. Some people rate them high, \nmedium and low, Some use a 1-10 scale. The problem with a 1-10 scale is that it is \noften difficult to tell a 2 from a 3 or a 7 from an 8, unless the differences between \neach rating are clearly defined. A five-point scale (very high, high, medium, low and \nvery low) tends to work well. \nGiven two classifications of risk levels, likelihood and impact, we have a problem. \nWe need a single, aggregate risk rating to guide our testing effort. As with rating \nscales, practices vary. One approach is to convert each nsk classification into a num- \nber and then either multiply (or add) the numbers to calculate 4 risk priority number. \nFor example, suppose a particular risk has a high likelihood and a medium impact. \nThe risk priority number would then be 6 (2 times 3). \nMitigation options \nArmed with a risk priority number, we can now decide on the various risk \nmitigation options available to us. Do we use formal training for developers or \nanalysts, rely on cross-training and reviews or assume they know enough? Do we \nperform extensive testing, cursory testing or no testing at all? Should we ensure \nunit testing and system testing coverage of this risk? These options and more are \navailable to us. \nAs you go through this process. make sure you capture the key information in a \ndocument. We are not fond of excessive documentation but this quantity of informa- \ntion simply cannot be managed in your head. We recommend a lightweight table like \nthe one shown in Table 5.2, which we usually capture in a spreadsheet. \nTABLE 5.2 Arisk analysis template \nRisk priority Productrisk  Likelihood  Impact number Mitigation \nRisk category 1 \nRisk 1 \nRisk 2 \nRisk n \nConcluding thoughts on risk \nTo summarize, the results of product risk analysis are used for the following purposes: \n@ To determine the test technigues to be used. \n® To determine the particular levels and types of testing to be performed (for \nexample functional testing, security testing, performance testing), \n® To determine the extent of testing to be carried out for the different levels and \ntypes of testing. \nG N Crmpoge Lonwming. A3 Kights Bt vt My et b o, s, o4 b s, 0 s 16 . o, U 3 i, e, st s sty conto ey b sy B e el smbs At s i e b s oo st s & cvent warwny Conpuge Linmay marses e 1y 10 ey akbmcnd o o 141w € sbacgacm A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 202,
            "page_label": "203"
        }
    },
    {
        "page_content": "190 Chapter 5 Test management \no To prioritize testing in order to find the most critical defects as early as possible. \n® To determine whether any activities in addition to testing could be employed \nto reduce risk, such as providing training in design and testing to inexperi- \nenced developers. \nLet’s finish this section with two quick tips about product risk analysis. First, \nremember to consider both likelihood and impact. While it might make you feel like a \nhero to find lots of defects, testing is also about building confidence in key functions. \nWe need to test the things that probably will not break, but would be catastrophic if \nthey did. \nSecond, we re-iterate that in early risk analysis, we are making educated guesses. \nMake sure that you follow up and revisit the risk analysis at key project milestones, \nFor example, if you are following a V-model, you might perform the initial analysis \nduring the requirements phase, then review and revise it at the end of the design and \nimplementation phases, as well as prior to starting unit test, integration test and sys- \ntem test. We also recommend revisiting the risk analysis during testing. You might \nfind you have discovered new risks or found that some risks were not as risky as you \nthought and increased your confidence in the risk analysis. In Agile development, \nrevisiting the risk analysis at the start of every sprint is a good idea, \n5.6 DEFECT MANAGEMENT \nLLABUS LEARNING OBJECTIVES FOR 5.6 DEFECT \nMANAGEMENT (K3) \nFL-56.1  Write a defect report covering defects found during testing (K3) \nLet's wind down this chapter on test management with an important subject: how \nwe can document and manage the defects that occur during testing. One of the \nobjectives of testing is to find defects, which reveal themselves as discrepancies \nbetween actual and expected outcomes. When we observe a defect, we need to \nlog the details. Proper test management involves establishing appropriate actions \n10 investigate and dispose of defects, This includes tracking defects from initial \ndiscovery and classification through to resolution, confirmation testing and ulti- \nmate disposition, with the defects following a clearly established set of rules and \nprocesses for defect management and classification. We'll look at what topics we \nshould cover when reporting defects. At the end of this section. you will be ready \nto write a thorough defect report. \nThe only Syllabus term in this section is defect management. \nWhat are defect reports for? \nWhen running a test, you might observe actual results that vary from expected \nresults. This is not a bad thing ~ one of the major goals of testing is to find problems. \nDifferent organizations have different names to describe such situations. Commonly, \nthey are called incidents, bugs, defects, problems or issues. \nTo be precise, we sometimes draw a distinction between an incident or anomaly on \nthe one hand and defects or bugs on the other. An incident or anomaly is any situation \nG N Crmpoge Lowming. A3 g Bt vt My et b o, s, o4 Al i, 0 s 10 . o, U 3 i e, s sty comtr ey b gy B e el b At e oo s et s 8 cmeni oy Conpuge Lonmay muries e 1y 10 ey koo o o 0} o € whacacm YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 203,
            "page_label": "204"
        }
    },
    {
        "page_content": "Section 6 Defect Management 191 \nwhere the system exhibits questionable behaviour, and a defect is some problem, \nimperfection or deficiency in the item we are testing. \nCauses of defects include misconfiguration or failure of the test environment, \ncorrupted test data, bad tests, incorrect expected results and tester mistakes. In some \ncases, the policy is to classify as a defect anything that arises from a test design, \nthe test environment or anything else which is under formal configuration manage- \nment. Bear in mind the possibility that a questionable behaviour is not necessarily a \ntrue defect. We log these defects so that we have a record of what we observed and \ncan follow it up and track what is done to correct it, whether or not it turns out to \nbe a problem in the work product we are testing or something else. This is defect  Defect \nmanagement. The process of \nWhile it is most common to find defect logging or defect reporting processes and  [8C0gNIzing and \ntools in use during formal, independent testing, you can also log, report, track and  récording defects, \nmanage defects found during development and reviews. In fact, this is a good idea,  Ca55fYing them, \nbecause it gives useful information on the extent to which early (and cheaper) defect  TYeo0gating them, \ndetection and removal activities are happening. Sk \nOf course, we also need some way of reporting, tracking and managing defects mm\"md \nthat oceur in the field or after deployment of the system. While many of these \ndefects will be user error or some other behaviour not related to a problem in system \nbehaviour, some percentage of defects do escape from quality assurance and testing \nactivities. The defect detection percentage { DDP). which compares field defects with \ntest defects, can be a useful metric of the effectiveness of the test process, \nHere is an example of a DDP formula that would apply for calculating DDP for \nthe last level of testing prior to release to the field: \ndefects (testers) \ndefects (testers) + defects (fiekd) \nItis most common 1o find defects reported against the code or the system itself. \nHowever, we have also seen cases where defects are reported against requirements \nand design specifications, user and operator guides and tests. Often, it aids the effec- \ntiveness and efficiency of reporting. tracking and managing defects when the defect \ntracking tool provides an ability to vary some of the information captured depending \non what the defect was reported against. \nIn some projects, a very large number of defects are found. Even on smaller \nprojects where 100 or fewer defects are found, you can easily lose track of them \nunless you have a process for reporting, classifying. assigning and managing the \ndefects from discovery to final resolution. \nObjectives for defect reports \nA defect report contains a description of the misbehaviour that was observed and \nclassification of that misbehaviour. As with any written commaunication, it helps to \nhave clear goals in mind when writing. Typical objectives for such reports include \nthe following: \n® To provide developers, managers and others with detailed information \nabout the behaviour observed (the adverse event), that is, the defect. This \nwill enable them to identify specific effects, to isolate the problem with \na minimal reproducing test, and to correct the defect(s) as needed or to \notherwise resolve the problem. \nDDP = \not S04 Crmpoge Loy N3 Bt Bt vl My et b o s o4 B 4 s 18 o (b 3 s, . ot oty st ey b smpqpesmed B e ol snbis g 41 \nConpaet rview e e s oo s et e Tt s e (e 4 v o o o s o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 204,
            "page_label": "205"
        }
    },
    {
        "page_content": "192 Chapter 5 Test management \n® To support test managers in the analysis of trends in aggregate defect data, \ncither for understanding more about a particular set of problems or tests, or for \nunderstanding and reporting the overall level of system quality. This will enable \nthem to track the quality of the work product and the impact on the testing. For \nexample, if a lot of defects are being reported, the testers will do less testing, \nsince their time is taken with writing defect reports. This also means that more \nconfirmation tests will be needed. \n® To enable defect reports 1o be analyzed over a project, and even across \nprojects, to give information and ideas that can lead to development and test \nprocess improvements. \nWhen writing a defect report, it helps to have the readers in mind, too. The deve- \nlopers need the information in the report to find and fix the defects. Before that \nhappens. though, the defects should be reviewed and prioritized so that scarce testing \nand developer resources are spent fixing and confirmation testing the most important \ndefects. Since some defects may be deferred — perhaps to be fixed later or perhaps, \nultimately, not to be fixed at all - we should include work-arounds and other helpful \ninformation for help-desk or technical support teams. Finally, testers often need to \nknow what their colleagues are finding so that they can watch for similar behaviour \nelsewhere and avoid trying to run tests that will be blocked. \nHow to write a good defect report: some tips \nA good defect report is a technical document. In addition to being clear for its goals \nand audience, any good report grows out of a careful approach to researching and \nwriting the report. We have some rules of thumb that can help you write a better \ndefect report. \nUse a careful, attentive approach to running your tests. You never know when you \nare going to find a problem. If you are pounding on the keyboard while gossiping \nwith office mates or daydreaming about a movie you just saw, you might not notice \nstrange behaviours. Even if you see the defect, how much do you really know about \nit? What can you write in your defect \nIntermittent or sporadic symptoms are a fact of life for some defects and it is \nalways discouraging to have a defect report bounced back as irreproducible. It's a \ngood idea to try to reproduce symploms when you see them. We have found three \ntimes to be a good rule of thumb. If a defect has intermittent symptoms, we would \nstill report it, but we would be sure to include as much information as possible, \nespecially how many times we tried to reproduce it and how many times it did in \nfact occur. \nYou should also try to isolate the defect by making carefully chosen changes to \nthe steps used to reproduce it. In isolating the defect, you help guide the developer \n10 the problematic part of the system. You also increase your own knowledge of how \nthe system works, and how it fails. \nThink outside the box. Some test cases focus on boundary conditions, which may \nmake it appear that a defect is not likely to happen frequently in practice, We have \nfound that it’s a good idea to look for more generalized conditions that cause the \nfailure to occur, rather than simply relying on the test case. This helps prevent the \ninfamous defect report rejoinder, ‘No real user is ever going to do that”. It also cuts \ndown on the number of duplicate reports that get filed. \nAs there is often a lot of testing going on with the system during a test period, \nthere are lots of other test results available. Comparing an observed problem against \nom0 300 g Lo 8 s Rt Ay o b i e et 1 o 1 . s s e Gy s o e ey b e ot b et e e b et P Conpage Lonmay murses e 1y 10 ey kbacnd o o 0} S € whacgacs YIS (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 205,
            "page_label": "206"
        }
    },
    {
        "page_content": "Section 6 Defect Management 193 \nother test results and known defects is a good way to find and document additional \ninformation that the developer is likely to find very uscful. For example, you might \ncheck for similar symptoms observed with other defects, the same symptom observed \nwith defects that were fixed in previous versions or similar (or different) results seen \nin tests that cover similar parts of the system, \nMany readers of defect reports, managers especially, will need to understand the \npriority and severity of the defect. The impact of the problem on the users, customers \nand other stakeholders is important. Most defect tracking systems have a title or \nsummary field and that field should mention the impact, too. \nChoice of words definitely matters in defect reports. You should be clear and \nunambiguous. You should also be neutral, fact-focused and impartial, keeping in \nmind the testing-related interpersonal issues discussed in Chapter | and cardier in this \nchapter. Finally, keeping the report concise helps keep people’s attention and avoids \nthe problem of losing them in the details. \nAs a last rule of thumb for defect reports, we recommend that you use a review \nprocess for all reports filed. It works if you have the lead tester review reports and we \nhave also allowed testers, at least experienced ones, to review other testers’ reports. \nReviews are proven quality assurance techniques and defect reports are important \nproject deliverables. \nWhat goes in a defect report? \nA defect report describes some situation, behaviour or event that occurred during \ntesting that was not as it should be, or that requires further investigation. In many \ncases, a defect report consists of one of two screens full of information gathered by \na defect tracking tool and stored in a database. \nA defect report filed during dynamic testing typically includes the following: \n® An identifier. \nA title and short summary of the defect being reported. \n® Date of the defect report, issuing organization, the date and time of the failure, \nthe name of the tester, that is, the author of the defect report (and perhaps the \nreviewer of the test). \no Identification of the test item (configuration item being tested), the test environ- \nment and any additional information about the configuration of the software, \nsystem or environment. \n® The development life cycle activity(s) or sprint in which the defect was observed. \n® A description of the defect to enable reproduction and resolution, such as the \nsteps to reproduce and the isolation steps tried. \n® The expected and actual results of the test. \n® The scope or degree of impact (severity) of the defect on the interests of \nstakeholder(s). \n® Urgency/priority to fix. \n® State of the defect report (for example open, deferred, duplicate, waiting to be \nfixed, awaiting confirmation testing, re-opened, closed). \no Conclusions, recommendations and approvals. \no Global issues, such as other areas that may be affected by a change resulting \nfrom the defect. \no N Compoge Lossming. A3 K Bt vt My et b o, s, o4 b, s 10 . o, U 3 st g, s sty comto ey b sy B e el smbs At el e A e gyt oot s e Sy R G Tl g pericnce Cengage Ly BTG e (W 14 Y adibicnd o o o € g 4 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 206,
            "page_label": "207"
        }
    },
    {
        "page_content": "194 Chapter 5 Test management \n® Change history, such as the sequence of actions taken by project team members \nwith respect to the defect, to isolate, repair and confirm it as fixed. These fields \nshould mention the inputs given and outputs observed, the different ways you \ncould - and could not — make the problem recur, and the impact. \n® References, including the test case that revealed the problem, and references \nto specifications or other work products that provide information about cor- \nrect behaviour, \nSometimes testers classify the scope. severity and priority of the defect, though \nsometimes managers or 4 bug triage committee handle that role, \n1f you are using a defect management tool, some of the information will be auto- \nmatically generated, such as the time and date of the report. the 1D number, the report \nauthor and the initial state (open). \nAn example defect report can be found in ISO/IEC/IEEE 29119-3 [2013). \nAs the defect is managed to resolution, managers might assign a level of priority \nto the report. The change control board or bug triage committee might document the \nrisks, costs, opportunities and benefits associated with fixing or not fixing the defect, \nThe developer, when fixing the defect, can capture the root cause, the time or devel- \nopment stage where it was introduced and the time or testing activity that identified \nit, and removed it (which may be different 1o when it was identified). \nAfter the defect has been resolved, managers, developers or others may want to cap- \nture conclusions and recommendations. Throughout the life cycle of the defect report, \nfrom discovery to resolution, the defect tracking system should allow each person who \nwaorks on the defect report to enter status and history information. \nWhat happens to defect reports after you file them? \nAs we mentioned earlier, defect reports are managed through a life cycle from dis- \ncovery to resolution. The defect report life cycle is often shown as a state transition \ndiagram (see Figure 5.3). While your defect tracking system may use a different life \ncycle, let’s take this one as an example 1o illustrate how a defect report life cycle \nmight work. \nIn the defect report life cycle shown in Figure 5,3, all defect reports move through \na series of clearly identified states after being reported. Some of these state transi- \ntions occur when a member of the project team completes some assigned task related \n1o closing a defect report. Some of these state transitions occur when the project \nteam decides not to repair a defect during this project, leading to the deferral of the \ndefect report. Some of these state transitions oceur when a defect report is poorly \nwritten or describes behaviour which is actually correct, leading 1o the rejection of \nthat report. \nLet’s focus on the path taken by defect reports which are ultimately fixed. After \na defect is reported. a peer tester or test manager reviews the report. If successful in \nthe review, the defect report becomes opened, so now the project team must decide \nwhether or not to repair the defect. If the defect is to be repaired. a developer is \nassigned to repair it. \nOnce the developer believes the repairs are complete, the defect report returns 1o \nthe tester for confirmation testing. If the confirmation test fails, the defect report is \nre-opened and then re-assigned. Once the tester confirms a good repair, the defect \nreport is closed. No further work remains to be done on this defect. \nG N Compuge Lowming. A3 K Bt vt My et b o, s, o4 e s, 0 s 10 . o, U 3 st g, s e paty comto ey b gy B e el smbs At v e e e oo s et P Conpage Lonmay murves e 1y 10 ey akbacnd o o 0} wow € hacacs YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 207,
            "page_label": "208"
        }
    },
    {
        "page_content": "Section6 Defect Management 195 \nApproved \nRewritten N°1i o Approved for 10 be repaired \nre-repair oom'manon test \nGathered Problemn \nnew information returned \nFIGURE 5.3 Defect report Me cycle \nIn any state other than rejected, deferred or closed. further work is required on the \ndefect prioe to the end of this project, Tn such a state, the defect report has a clearly \nidentified owner. The owner is responsible for transitioning the defect into an allowed \nsubsequent state. The arrows in the diagram show these allowed transitions, \nIn a rejected, deferred or closed state, the defect report will not be assigned to an \nowner. However, certain real world events can cause a defect report 1o change state \neven if no active work is occurring on the defect report. Examples include the recur- \nrence of a failure associated with a closed defect report and the discovery of a more \nserious failure associated with a deferred defect report. \nIdeally, only the owner can transition the defect report from the current state to the \nnext state and ideally the owner can only transition the defect report to an allowed next \nstate. Most defect tracking systems support and enforce the defect life cycle and life \ncycle rules. Good defect tracking systems allow you to customize the set of states, the \nowners and the transitions allowed to match your actual workflows. And, while a good \ndefect tracking system is helpful, the actual defect work flow should be monitored and \nsupported by project and company management, \not S04 Crmpoge Loy N3 Bt Bt vl My et b oo s o4 bl 0 b 18 ot U 3 s, . ot ety st ey b st b e ol snbis Akt s el s M e gyt oot s et sy 4 el g pericncs Cengage Lty BTN e (W 14 Y adlbicnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 208,
            "page_label": "209"
        }
    },
    {
        "page_content": "196 Chapter 5 Test management \nCHAPTER REVIEW \nLet’s review what you have learned in this chapler. \nFrom Section 5.1, you should now be able to explain the basic ideas of test orga- \nnization. You should know why independent testing is important, but also be able to \nanalyze the potential benefits and problems associated with independent test teams. \nYou should be able 10 recall the tasks that a tester and a test manager will carry out. \nYou should know the Glossary terms tester and test manager. \nFrom Section 5.2, you should now understand the fundamentals of test planning \nand estimation. You shoukd know the reasons for writing test plans and be able 10 \nexplain how test plans relate to projects, test levels, test objectives and test activities, \nYou should be able to write a test execution schedule based on priority and depen- \ndencies. You should be able 1o explain the justification behind various entry and exit \ncriteria that might relate 1o projects, test levels and or test objectives. You should \nbe able to distinguish the purpose and content of test plans from that of test design \nspecifications, test cases and test procedures. You should know the factors that affect \nthe effort involved in testing, including especially test strategies (and appeoaches) \nand how they affect testing. You should be able to explain how metrics, expertise \nand negotiation are used for estimating. You should know the Glossary terms entry \neriteria, exit eriteria. test approach. test estimation. test plan, test planning and \ntest strategy. \nFrom Section 5.3, you should be able to explain the essentials of test progress \nmonitoring and control. You should know the common metrics that are captured, \nlogged and used for monitoring, as well as ways to present these metrics. You should \nbe able to explain a typical test progress report and test summary report. You should \nknow the Glossary terms test control, test monitoring. test progress report and \ntest summary report. \nFrom Section 5,4, you should now understand the bassics of configuration manage- \nment that relate 10 testing. and how it helps us do our testing work better. You should \nknow the Glossary term configuration management. \nFrom Section 5.5, you should now be able to explain how risk is related to testing, \nYou should know that a risk is a potential undesirable or negative outcome and that \nmost of the risks we are interested in relate to the achievement of project objectives. \nYou should know about likelihood and impact as factors that determine the impor- \ntance of a risk. You should be able to compare and contrast risks 1o the product (and \nits quality) and risks to the project itself and know typical risks to the product and \nproject. You should be able to describe how to use risk analysis and risk management \nfor testing and test planning. You should know the Glossary terms product risk, \nproject risk, risk, risk level and risk-based testing. \nFrom Section 5.6, you should now understand defect logging and be able to use \ndefect management on your projects, You should know the content of a defect report, \nbe able to write a high quality report based on test results and manage that report \nthrough its life cycle. You should know the Glossary term defect management. \ne 528 Cop Loy 48 g St e ot . i et 0 10 e, vt i e G s s P e et e et wpa rpe——— e Lramtmg st e gl b s bbb et o 7 b § bt o4l b gt &",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 209,
            "page_label": "210"
        }
    },
    {
        "page_content": "[Pa wpey \nSAMPLE EXAM QUESTIONS \nQuestion 1 What is a potential drawback of inde- \npendent testing? \na. Independent testing is likely to find the same \ndefects as developers, \nb. Developers may lose a sense of responsibility for \nquality. \n¢ Independent testing may find more defects. \nd. Independent testers may be too familiar with \nthe system. \nQuestion 2 Which of the following is among the \ntypical tasks of a test manager? \na. Develop system requirements, design specifica- \ntions and usage models. \nb. Handle all test automation duties. \nc. Keep tests and test coverage hidden from \ndevelopers. \nd. Gather and report test progress metrics. \nQuestion 3 According to the ISTQB Glossary, \nwhat do we mean when we call someone a test \nmanager? \nA A test manager manages a team of junior testers. \nb. A test manager plans and controls testing activities, \n€ A test manager sets up lest environments. \nd. A test manager creates a detailed test execution \nschedule, \nQuestion 4 What is the primary difference \nbetween the test plan, the test design specification \nand the test procedure specification? \n. The test plan describes one or more levels of \ntesting, the test design specification identifies the \nassociated high-level test cases and a test proce- \ndure specification describes the actions for execut- \ning atest. \nb, The test plan is for managers, the test design spec- \nification is for developers and the test procedure \nspecification is for testers who are auomating tests. \n. The test plan is the least thorough, the test \nprocedure specification is the most thorough and \nthe test design specification is midway between \nthe two, \nSamgle Exam Questions 197 \nd. The test plan is finished in the first third of the \nproject, the test design specification is finished in \nthe middle third of the project and the test proce- \ndure specification is finished in the last third of \nthe project. \nQuestion 5 Which of the following factors is an \ninfluence on the test effort involved in most projects? \na. Geographical separation of tester and developers. \nb. The departure of the test manager during the project. \n©. The quality of the information used to develop the \ntests \nd. Unexpected long-term illness by a member of \nthe project team. \nQuestion 6 A business domain expert has given \nadvice and guidance about the way the testing should \nbe carried out and a technology expert has recom- \nmended using a checklist of common types of defect. \nWhich two test strategies are being used? \na. Analytical and Process compliant. \nb. Reactive and Methodical. \n€. Directed and Methodical. \nd. Regression-averse and Analytical. \nQuestion 7 Consider the following exit criteria \nwhich might be found in a test plan: \n1. No known customer-critical defects. \n1L All interfaces between components tested. \n111 1006 code coverage of all units. \nIV. All specified requirements satisfied. \nV. System functionality matches legacy system for \nall business rules. \nWhich of the following statements is true about \nwhether these exit criteria belong in an acceptance \ntest plan? \na. All statements belong in an scceptance test plan, \nb. Only statement | belongs in an acceptance test plan. \nc. Only statements I, I and V belong in an accep- \ntance test plan. \nd. Only statements 1, IV and V belong in an accep- \ntance test plan. \na4 g 2wy, A s Mt ¥l Ay e 1 g s s i o i ey S P e Ak e g Iy y— Crmeae! e o g s ety o £ - —",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 210,
            "page_label": "211"
        }
    },
    {
        "page_content": "198 Chapter 5 Test management \nQuestion 8 According to the ISTQB Glossary, \nwhat is a test approach? \na. The implementation of the test strategy for a spe- \ncific project. \nb. Documentation that expresses the generic require- \nments for testing one or more projects within an \norganization providing detail on how testing is to \nbe performed, \n¢, Documentation describing the test objectives to \nbe achieved and the means and the schedule for \nachieving them, organized to coordinate testing \nactivities, \nd. The set of interrelated activitics comprising of test \nplanning, test monitoring and control, test analy- \nsis, test design, test implementation, test exccution \nand test completion, \nQuestion 9 Which of the following metrics would \nbe most useful to monitor during test execution? \na. Percentage of test cases written, \nb. Number of test environments remaining to be \nconfigured. \nc. Number of defects found and fixed. \nd. Percentage of requirements for which a test has \nbeen written. \nQuestion 10 What would appear in a test progress \nreport that would not be included in a test summary \nreport? \na. Summary of testing performed. \nb. The status of the test activities against the test plan. \n¢, Factors that have blocked or continue to block \nprogress. \nd. Reusable test products produced, \nQuestion 11 In a defect report, the tester makes \nthe following statement: “The payment processing \nsubsystem fails to accept payments from American \nExpress cardholders, which is considered a must- \nwork feature for this release.” This statement is likely \n10 be found in which of the following sections? \na. Scope or degree of impact of the defect. \nb. Identification of the test item, \n¢ Summary of the defect. \nd. Description of the defect. \nQuestion 12 During an early period of test exe- \ncution, a defect is located, resolved and passed its \nconfirmation test, but is seen again later during sub- \nsequent test execution. Which of the following is a \ntesting-related aspect of configuration management \nthat is most likely to have broken down? \na. Traceability. \nb. Configuration item identification, \n<. Configuration control. \nd. Test documentation management. \nQuestion 13 You are working as a tester on a \nproject to develop a point-of-sales system for grocery \nstores and other similar retail outlets. Which of the \nfollowing is a product risk for such a project? \na. The arrival of a more reliable competing product \non the market. \nb. Delivery of an incomplete test release to the first \ncycle of system test, \n¢. An excessively high number of defect fixes fail \nduring confirmation testing. \nd. Failure to accept allowed credit cands. \nQuestion 14 What is a risk? \na. A bad thing that has happened to the product or \nthe project. \nb. A bad thing that might happen. \n<. Something that costs a lot to put right. \nd. Something that may happen in the future, \nQuestion 15 You are writing a test plan and are \ncurrently completing a section on risk. Which of the \nfollowing is most likely to be listed as a project risk? \na. Unexpected illness of a key team member. \nb. Excessively slow transaction processing time. \n<. Data corruption under network congestion, \nd. Failure to handle a key use case. \nQuestion 16 You and the project stakeholders \ndevelop a list of product risks and project risks during \nthe planning stage of a project. What else should you \ndo with those lists of risks during test planning? \na. Determine the extent of testing required for the \nproduct risks and the mitigation and contingency \nactions required for the project risks, \nG N Crmpogs Lonwming. A3 K Bt vt My et b o, s, o4 e i, s 10 . e, U 30 ot g, s sty comto ey b gy B e el b At \nJe—p—— Conpaet e e b ety s 8 cmen oy s e (e 4 v ol o o s o € g 30 (F s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 211,
            "page_label": "212"
        }
    },
    {
        "page_content": "b. Obtain the resources needed to completely cover \neach product risk with tests and transfer responsi- \nbility for the project risks to the project manager. \n<. Execute sufficient tests for the product risks, based \non the likelihood and impact of each product risk \nand execute mitigation actions for all project risks. \nd. No further risk management action is required at \nthe test planning stage. \nQuestion 17 The following are ways to estimate \ntest effort: \nBurndown charts. \n. Wideband Delphi. \n. Expert opinion. \nPrevious experience. \n. Planning poker. \n. Similar project data. = \ne \nWhich of these are an example of a metrics-based \ntechnique? \na. 3, 4and5. \nb. 1.5and 6. \n¢ 2,3and4. \nd. 1, 2and 5. \nQuestion 18 In a defect report, the tester makes \nthe following statement: “At this point, I expect to \nreceive an error message explaining the rejection \nof this invalid input and asking me to enter a valid \ninput. Instead the system accepts the input, displays \nan hourglass for between one and five seconds and \nfinally terminates abnormally, giving the message, \n“Unexpected data type: 15. Click 1o continue™” This \nstatement is likely 1o be found in which of the follow- \ning sections of a defect report? \na. Summary. \nb. Impact. \nSample Exam Questions 199 \n¢. Ttem pass/fail criteria. \nd. Defect description. \nQuestion 19 There are five tests 1o be executed. A \nhigh-priority test has been scheduled as the third in \nthe sequence of five, rather than the first. Which of \nthe following would NOT be a good reason for this? \na. The high priority test has a logical dependency on \nthe two tests run before it \nb. All tests are the same priority and this is the most \nefficient sequence. \n¢. The tests scheduled before it are more likely to \npass so this will take less time and give feedback \nmore quickly. \nd. The first three tests have the same priority so it \ndoes not matter what order they are run in. \nQuestion 20 Exit criteria have been agreed at the \nstart of the project, but it is now clear that they will \nnot all be met before the release date next week. \nWhat should the test manager NOT say to those who \nwant to release next week anyway? \na. These exit criteria were agreed for a very good \nreason, to avoid this situation. It is essential that \nall exit eriteria are met before we release. \nb. Itis not my role to make that decision, so if you go \nahead and release next week anyway, I will care- \nfully monitor the effects and will report the results \n10 high-level management. \n¢. Have all project stakeholders, business owners \nand the product owner been informed of the \nrisks of releasing next week, and do they all \nunderstand and accept them? (Sign on this paper \nstating this.) \nd. What are the most critical exit criteria, and can we \nget agreement from all stakeholders about which \nare absolutely necessary and which could be com- \npromised this time? \no N Compoge Lossming. A3 Kt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 ot g, s s paty comtos ey b gy B e el smbs At el s A e 1 gyt oo s o sy 4 el g pericnce Cengage Lty BTG e (W 14 WY adibicnd o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 212,
            "page_label": "213"
        }
    },
    {
        "page_content": "200 Chapter 5 Test management \nEXERCISES \nTest execution schedule exercise \nYou have been asked to develop a test execution schedule for the second release of a customer data management \napplication. The first release only allowed basic customer records 1o be added. This second one contains 5 new \nfeatures and 2 bug fixes, together with the new features” priorities (1 is highest) and the defect severity levels, in \nthe table below. \nTABLE 5.3 Exercise: Test executon schedule \nPriority Feature/Fix Test order \n1 Delete inactive customer record \n2 De-activate customer \n3 Edit extra data items \n4 Edit basic data \n5 Link customer records (for exampie in the same company) \n5 Provide extra data items when record added \nWhat would be the BEST test execution sequence for this release? \nDefect report exercise \nAssume you are testing a maintenance release which will add a new supported model to a browser-based appli- \ncation that allows car dealers to order custom-configured cars from the maker. You are working with a selected \nnumber of dealers who are performing beta testing. When they find problems, they send you an email describing \nthe failure. You use that email to create a defect report in your defect management system. \nYou receive the following email from one of the dealers: \n1 entered an order for a Racinax 917X in midnight violet, During the upload. I got an ‘unexpected return \nvalue' error message. I then checked the order and it was in the system. | think the internet connection \nmight have gone down for a moment as we found out later that there was a power surge elsewhere in the \nbuilding at around that time. \nWrite a defect report based on this email and note any elements of such a report which might not be available \nbased on this email alone. \nNote that this scenario, including the name of the car model, is entirely fictitious. \nG N Crmpuge Loy A3 Kt Bt vt Moy et b o, s, o4 b . 0 b 16 . o, (b 3 i, e, s sty scmtoa ey b gyt B e el smbs AChagit s e s dmd sy commem s et s 8 cvent barwny Conprge Lowmay murses e 1y 10wy akbmcnd o o e} € sbacgacm A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 213,
            "page_label": "214"
        }
    },
    {
        "page_content": "Exercise Solutons 201 \nEXERCISE SOLUTIONS \nTest execution schedule exercise \nBelow we show the solution, with the tests listed in the test order. The justifications are shown below the table. \nTABLE 5.4 Solution: Test execution schedule \nPriority \n2 \nFeature/Fix Test order \nDe-activate customer 1 \nDelete inactive customer record 2 \nProvide extra data items when record added 3 \nEdit extra data items 4 \nEdit bask data 5 \nLink customer records {for example in the same company) 6 \nThe highest priority is to Delete an inactive customer, but first we must have an inactive customer, s0 we run \nDe-activate customer first. This is a functional dependency. \nThe next highest priority is to Edit extra data items, but again, we need 1o have some extra data items in order \nto do this, another functional dependency. \nThe final two are independent, so the priority 4 test is run before the priority 5 test. \nG N Compuge Lowming A3 gt Bt vt My et b copi, s, o4 b . 0 s 18 . . (b 3 i e, s sty comto ey b gy B e el b At s e e dmd ety JR—— s o cven barwny Conpge Lonmay murses e 1y 10wy akbacnd o o 1y Vo € whocacs A (70T s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 214,
            "page_label": "215"
        }
    },
    {
        "page_content": "202 Chapter 5 Test management \nDefect report exercise \nHere is an example of a defect report: \nTest defect report identifier: This would be assigned by the defect tracking tool. \nSummary: System returns confusing error message if internet connectivity is interrupted. \nDefect description: \nSteps to reproduce \n1. Entered an order for a Racinax 917X in midnight violet. \n2. During the order upload, the system displayed an ‘unexpected return value” error message. \n3. Verified that, in spite of the error message, the order was in the system. \nSuspected cause Given that a brief interruption in the internet connection may have occurred during order \ntransmission due to the power surge, the suspected cause of the failure is a lack of robust handling of slow \nor unreliable internet connections. \nImpact assessment The message in step 2 is not helpful to the user, because it gives the user no clues about \nwhat to do next. A user encountering the message described in step 2 might decide to re-enter the order, \nwhich in this case would result in a redundant order. \nSome car dealerships will have unreliable internet connections, with the frequency of connection loss \ndepending on location, wireless infrastructure available in the dealership, type of internet connection hard- \nware used in the dealers’ computers, and other such factors. Therefore we can expect that some number of \ndefects such as this will occur in widespread use. (Indeed, this beta test defect proves that this is a likely \nevent in the real world.) Since a careless or rushed dealer might decide 1o resubmit the order based on the \nerror message, this will result in redundant orders, which will cause significant loss of profitability for the \ndealerships and the salespeople themselves. \nThis report addresses the inputs, the expected results, the actual results, the difference between the expected and \nactual results, and the procedure steps in which the tester identified the unexpected results. Here is some additional \ninformation needed to improve the report: \n@ The failure needs to be reproduced at least two more times. By doing the failure replication in the test envi- \nronment, the tester will be able to control whether or not the internet connection goes down during the order \nand for how long the connection is down. \n@ s the problem in any way specific to the Racinax 917X model and/or the colour? Isolation of the failure is \nneeded, and then the report can be updated. \n@ Is the problem in any way specific to power surges? We would guess not but checking with different types \nof connections (during the isolation and replication discussed above) would make sense. If it proves indepen- \ndent of the type of connection, we would know that the problem is more general than just for power surges. \n® The defect report should also include information about the date and time of the test, the beta test envi- \nronment and the name of the beta tester and the tester entering the defect. \noo N Compogs Lossming. A3 K Bt vt My et b o, s, o4 b . 0 b 10 . o, U 3 i, g, st sty comto ey b sy B e el smbs AChapiti s \nJe——— Conpae s i e b s s & cvent barwny s e (e 4 e o o o o € g 0 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 215,
            "page_label": "216"
        }
    },
    {
        "page_content": "CHAPTER SIX \nTool support for testing \nou may be wishing that you had a magic tool that would automate all of the \ntesting for you. If so, you will be disappointed. However, there are a number of \nvery useful tools that can bring significant benefits, In this chapter, we will see that \nthere is tool support for many different aspects of software testing. We will see that \nsuccess with tools is not guaranteed, even if an appropriate tool is acquired ~ there \nare also risks in using tools. There are some special considerations mentioned in the \nSyllabus for test execution tools and test management tools, \n6.1 TEST TOOL CONSIDERATIONS \nSYLLABUS LEARNING OBJECTIV FOR 6.1 ST TOOL \nCONSIDERATIONS ) \nFL-6.1.1 Classify test tools according to their purpose and the test \nactivities they support (K2) \nFL-6.1.2  Identify benefits and risks of test automation (K1) \nFL-6.1.3 Remember special considerations for test execution and test \nmanagement tools (K1) \nAs we go through this section, watch for the Syllabus terms data-driven testing, \nkeyword-driven testing, performance testing tool. test automation. test execution \ntool and test management tool. You will find these terms and their definitions, taken \nfrom the ISTQB Glossary, in this chapter, in the Glossary and online. \nIn this section, we will describe the various tool types in terms of their general \nfunctionality, rather than going into lots of detail. The reason for this is that, in gen- \neral, the types of tool will be fairdy stable over a longer period, even though there \nwill be new vendors in the market, new and improved tools, and even new types of \ntool in the coming years, \nWe will also discuss both the benefits and the risks of using tools to run or exe- \ncute tests. We will outline special considerations for test execution tools and for test \nmanagement tools, the most popular types of test tools. \nWe will not mention any open source or commercial twols in this chapter. If we \ndid, this book would date very quickly! Tool vendors are acquired by other vendors, \nchange their names, and change the names of the tools quite frequently, so we will \nnot mention the names of any tools or vendors. \n203 \noy N Crmpogs Loseming. NS Rt Roerrnd Moy et b copi, wamnd, o0 et o s 10 e, U 3 vt g, acmms ety commvs sy b sppoesad B e el snbhe gt T mview s devmed sy Je——— s & cvent barwny 1 Conpae Lowmay mures e 1 1wy alomcnd o o 1 S € wharacs A 17aTa s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 216,
            "page_label": "217"
        }
    },
    {
        "page_content": "204 Chapter 6 Tool support for testing \nThere are a number of ways in which tools can be used 1o support testing activities: \n® To start with the most visible use, we can use tools directly in testing. This \nincludes test exccution tools and test data preparation tools, \n® We can use tols 10 help us manage the testing process, This includes wols \nthat manage requirements, test cases, test procedures, automate test scripts, \ntest results, test data and defects, as well as tools that assist with reporting and \nmonitoring test execution progress. \n® We can use tools as part of exploration, investigation and evaluation. For exam- \nple, we can use tools to monitor file activity for an application. \no We can use 10ols in a number of other ways, in the form of any tool that aids \nin testing. This would include spreadsheets when used to manage test assets \nor progress, or as a way to document manual or automated tests. \n6.1.1 Test tool classification \nIn this section, we will look first at how tools can be classified, starting with tool \nobjectives or purposes. Then we will look at tools that support various aspects of \ntesting: management of testing and testware, static testing, test design and implemen- \ntation, test execution and logging, performance measurement and dynamic analysis, \nand finally tool support for specialized testing needs. \nTool classification \nWhat are we trying to achieve with tools? What are the motivations? As you can \nimagine, these vary depending on the activity, but common objectives or purposes \nfor the use of tools include the following: \no We might want to improve the efficiency of our test activities by automating \nrepetitive tasks or by automating activitics that would otherwise require signif- \nicant resources 1o do manually. For example, the execution of regression tests is \nrepetitive and takes significant time and effort to do manually, \no We might want to improve the efficiency of our testing by providing support for \nmanual test activities throughout the test process. For example, tools can help in \nnoting our findings while doing exploratory testing. See Chapter 1, Section 1.1.4 \nfor more on the test process. \no We might want to improve the quality of test activities by achieving more \nconsistency and reproducibility. For example, tools can help to reproduce \nfailures more accurately and with more information. When a defect is \nfixed, the exact same test should be run again, otherwise you cannot be \nsure that the defect was correctly fixed. This requires both consistency and \nreproducibility. \n® We might need to carry out activities that simply cannot be done manually, but \nwhich can be done via automated tools. For example, large-scale performance \ntesting can only be achicved using tools: it is highly unfeasible to get thousands \nof users 10 test your system manually. \no We might want to increase the reliability of our testing. For example, auto- \nmation of large data comparisons, simulating user behaviour, or repeating \nlarge numbers of tests (which can lead to mistakes due to tedium of the work, \nif it is done by people). \nG N Crmpogs Lowming. A3 Kighs Bt vt My et b o, s, o4 A i, 0 s 10 . e, U 3 st e, s sty comto ey b gy B e el smbs At s v e e e oo s et e Conpuge Linmay murses e 1y 10wy koo o o a1 wow € hacgacs A (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 217,
            "page_label": "218"
        }
    },
    {
        "page_content": "Section 1 Test Tool Considerations 205 \nHow are tools classified? \nTools can be classified in different ways. One is by their purpose (as above), or they \ncan be classified by licencing model (commescial or open source), by price and/or \nby technology used. In this chapter, tools are classified by the testing activities or \nareas that are supported by a set of tools, for example, tools that support management \nactivities, tools to support static testing, efc. \nThere is not necessarily a one-to-one relationship between a type of tool described \nhere and a tool offered by a commercial tool vendor or an open source tool. Some \ntools perform a very specific and limited function and support only one test activity \n(sometimes called a point solution), but many of the tools provide support for a number \nof different functions (tool suites or familics of tols), For example, a test management \nool may provide support for managing testing (progress monitoring), configuration \nmanagement of testware, defect management, and requirements management and \ntraceability; another tool may provide both coverage measurement and test design \nsupport. \nTools versus people \nThere are some things that people can do much better or easier than a computer can \ndo. For example, when you see a friend in an unexpected place, say in an airport, you \ncan immediately recognize their face. This is an example of pattern recognition that \npeople are very good at, but it is not easy to write software that can recognize a face. \nThere are other things that computers can do much better or more quickly than \npeople can do. For example, can you add up 20 three-digit numbers quickly? This is \nnot easy for most people to do, so you are likely to make some mistakes even if the \nnumbers are written down. A computer does this accurately and very quickly. As \nanother example, if people are asked to do exactly the same task over and over, they \nsoon get bored and then start making mistakes. \nThe point is that it is a good idea to use computers to do things that computers are \nreally good at and that people are not very good at. Tool support is very useful for \nrepetitive tasks — the computer doesn't get bored and will be able 1o exactly repeat \nwhat was done before. Because the tool will be fast, this can make those activities \nmuch more efficient and more reliable. The tools can also do things that might over- \nload a person, such as comparing the contents of a large data file or simulating how \nthe system would behave. \nTools that affect their own results \nA tool that measures some aspect of software may be intrusive. This means that the \ntool may have unexpected side effects in the software under test, For example, a tool \nthat measures timings for performance testing needs to interact very closely with that \nsoftware in order to measure it. A performance tool will set a start time and a stop \ntime for a given transaction in order to measure the response time, for example. But \nthe act of taking that measurement, that is, storing the time at those two points, could \nactually make the whole transaction take slightly longer than it would do if the tool \nwas not measuring the response time. Of course, the extra time is very small, but it \nis still there. This effect is called the “probe effect’, \nAnother example of the probe effect occurs with coverage tools. In order \nto measure coverage, the tool must first identify all of the structural clements \nthat might be exercised to see whether a test exercises it or not. This is called \ninstrumenting the code. The tests are then run through the instrumented code \nso that the tool can tell (through the instrumentation) whether or not a given \nG N Crmpogs Lowming. A3 K Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 3 i, e, s sty comtr ey b gy B e el b At v e e e oo s et e Conpage Linmay murses e 1y 10 ey koo o o ) Vo € whacacs A (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 218,
            "page_label": "219"
        }
    },
    {
        "page_content": "206 Chapter 6 Tool support for testing \nbranch (for example) has been exercised. But the instrumented code is not the \nsame as the real code — it also includes the instrumentation code. In theory. the \ncode is the same, but in practice, it isn't, Because different coverage tools work in \nslightly different ways, you may get a slightly different coverage measure on the \nsame program because of the probe effect. For example, different tools may count \nbranches in different ways, so the percentage of coverage would be compared to \na different total number of branches. The response time of the instrumented code \nmay also be significantly worse than the code without instrumentation, (There are \nalso non-intrusive coverage tools that observe the blocks of memory containing \nthe object code to get a rough measurement without instrumentation, for example \nfor embedded software.) \nOne further example of the probe effect is when a debugging tool is used to try o \nfind a particular defect. If the code is run with the debugger, then the bug disappears; \nit only reappears when the debugger is turned off, thereby making it much more \ndifficult to find, These are sometimes known as “Heisen-bugs” (after Heisenberg's \nuncertainty principle). \nTool classification categories \nIn the descriptions of the tools below, we will indicate the tools that are more likely \nto be used by developers, for example during component testing and component \nintegration testing, by showing (D) (for Developer) after the tool type. For example, \ncoverage measurement tools are most often used in component testing, but perfor- \nmance testing tools are more often used at system testing, system integration testing \nand acceptance testing. \nNote that for the Foundation Certificate exam, you only need to recognize the \ndifferent types of tools; you do not need to understand in any detail what they do (or \nknow how to use them). We will briefly describe what the tools do (more detail than \nin the Syllabus) to help you identify the different types of tool. \nTool support for management of testing and testware \nWhat does test management mean? It could be the management of tests or it could \nTest management be managing the testing process. The tools in this broad category provide support \ntool A tool that for either or both of these. The management of testing applies over the whole of the \nprovides support to software development life cycle, so a test management tool could be among the first 1o \n::m::\"'*\"\"“ be used in & project. A test management tool may also manage the tests, which would \nd.::\"ﬂ At begin early in the project and would then continue to be used throughout the project \n1t often m and also after the system had been released. In practice, test management tools are \ncapabilities, such as typically used by specialist testers or test managers at system or acceplance test level, \ntestware ST \nmdmmm Test management tools and application lifecycle management (ALM) tools \nthe logging of results, Test management tools provide features that cover both the management of testing, \nprogress tracking, such as progress reports and keeping track of testing activities, and the management \nincident management of testware, such as logging of test results and keeping track of test environments \nand test reporting. needed for tests. There are some tools that provide support for only one activity (for \n(Note that the example defect management tools); other tools or tool suites may provide support for \nGb’..y dtﬂnkbn all test management activities. Special considerations for test management tools are \nbut the discussed in Section 6.1.3. \nm ekt Application lifecycle management (ALM) tools manage not only testing but also \nmanagement ) development and deployment. With a focus on communication, collaboration and task \ntracking, they are popular in Agile development. \nG N Crmpugs Lowming. A3 Kighs Bt vt My et b o, s, o4 e, 0 s 18 . o, U 3 i e, s sty commo ey b gyt B e el smbs At s e e b ety oo s et P Conpuge Linmay murses e 1y 10wy kbmcnd o o a1 wow € whacacs YA (7T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 219,
            "page_label": "220"
        }
    },
    {
        "page_content": "Section 1 Test Tool Considerations 207 \nThe information from test management tools (and ALM tools) can be used to \nmonitor the testing process and decide what actions to take (test control), as described \nin Chapter 5. The tool also gives information about the component or system being \ntested (the test object). Test management tools help to gather, organize and commu- \nnicate information about the testing on a project. \nRequirements management tools \nAre requirements management tools really testing tools? Some people may say they \nare not, but they do provide some features that are very helpful to testing. Because \ntests are based on requirements, the better the quality of the requirements, the casier \nit will be to write tests from them. It is also important to be able to trace tests to \nrequirements and requirements 10 tests, as we saw in Chapter 2. Note that require- \nments means any source of what the system should do, such as user stories, \nDefect management tools \nThis type of ool is also known as a defect tracking tool, an incident management \ntool, a bug tracking tool or a bug management tool. However not all of the things \ntracked are actually defects or bugs, There may also be perceived problems, anom- \nalies (that aren’t necessarily defects) or enhancement requests:; this is why they are \nsometimes referred to as incidents and incident management tols. Also, what is \nnormally recorded is information about the failure (not the defect) that was generated \nduring testing; information about the defect that caused that failure would come to \nlight when someone (for example a developer) begins to investigate the failure. \nDefect reports go through a number of stages, from initial identification and \nrecording of the details, through analysis, classification, assignment for fixing. fixed, \nre-tested and closed, as described in Chapter 5. Defect management tools make it \nmuch easier 1o keep track of the defects and their status over time. \nConfiguration management tools \nAnexample: A test group began testing the software, expecting to find the usual fairly \nhigh number of problems. But to their surprise, the software seemed to be much better \nthan usual this time, Very few defects were found. Before they celebrated the great \nquality of this release, they just made an additional check to see if they had the right \nversion and discovered that they were actually testing the version from two months \ncarlier (which had been debugged) with the tests for that eardier version. It was nice \nto know that this was still OK, but they were not actually testing what they thought \nthey were testing or what they should have been testing. \nConfiguration management tools are not strictly testing tools either, but good \nconfiguration management is critical for controlled testing, as was described in \nChapter 5. We need to know exactly what it is that we are supposed to test, such as \nthe exact version of all of the things that belong in a system, It is possible to perform \nconfiguration management activities without the use of tools, but the tools make \nlife a lot easier, especially in complex environments. The same tool may be used for \ntestware as well as for software items. Testware also has different versions and is \nchanged over time. It is important to run the correct version of the tests as well, as \nour example shows, \nContinuous integration tools (D) \nThe (D) after this (and other types of tools) indicates that these tools are more likely \nto be used by developers. \nv A o i, 4 s e My v e s o7 A1 bk o Do e s Bk s oo s ppmd b i e gl \nConpaet e e b et e s e (e 4 e sl o o o € g 0 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 220,
            "page_label": "221"
        }
    },
    {
        "page_content": "208 Chapter 6 Tool support for testing \nIncremental and iterative development life cycles require frequent builds, and \ncontinuous integration tools are an essential part of the Agile toolkit. Continuous \nintegration is the practice of integrating new or changed code with the existing code \nrepository very frequently, at least daily but sometimes dozens of times a day or \nmore. Unit tests are most often automatically run when a new build is made (when a \ndeveloper commits his or her change), so any defects found can be corrected imme- \ndiately. The result of the integration is a system that is tested and could in principle \nbe deployed to users at any time. \nIf the deployment is automated, then it is called continuous delivery, but there \nmight still be some final human approval before it is released. If deployment is \nautomatic (with no final human approval). then it is called continuous deployment, \nContinuous integration, delivery and deployment are the basis for DevOps, where \ncode changes are delivered 1o users in an automated delivery pipeline. \nThese tols are continuous because the integration of the code components hap- \npens so often, and is sutomatic (without human intervention), whenever the new build \nis triggered by a developer. This type of tool is included here because it includes tests, \nwhich are managed in the continuous integration tool, \nTool support for static testing \nThe tools described in this section support the testing activities described in Chapter 3, \nTools that support reviews \nThe value of different types of review was discussed in Chapter 3. For a very informal \nreview, where one person looks at another's work product and gives a few comments \nabout it a tool such as this might just get in the way. However, when the review pro- \ncess is more formal, when many people are involved, or when the people involved are \nin different geographical locations, then tool support becomes far more beneficial. \nIt is possible 1o keep track of all the information for a review process using spread- \nsheets and text documents, but a review tool that is designed for the purpose is \nmore likely to do a better job. For example, one thing that should be monitored for \neach review is that the reviewers have not gone over the work product too quickly, \nthat is, that the checking rate (number of pages checked per hour) was close to that \nrecommended for that review cycle. A review tool could automatically calculate the \nchecking rate and flag exceptions. The review tools can normally be tailored for the \nparticular review process or type of review being done. \nStatic analysis tools (D) \nStatic analysis tools are normally used by developers as part of the development and \ncomponent testing process. The key aspect is that the code (or other artefact) is not \nexecuted or run. Of course, the tool itself is executed, but the source code we are \ninterested in is the input data to the tool. \nStatic analysis tools are an extension of compiler technology, in fact some com- \npilers do offer static analysis features. It's worth checking what is available from \nexisting compilers or development environments before looking at acquiring a more \nsophisticated static analysis tool. \nStatic analysis can also be carried out on things other than software code, for \nexample, static analysis of requirements or static analysis of websites to assess for \nproper use of accessibility tags or the following of HTML standards, \nStatic analysis tools for code can help the developers to understand the structure \nof the code and can also be used 10 enforce coding standards, \not S0 Crmpoge Loy N3 B Bt vl My et b o s o4 B 0 b 18 ot (b 3 s, . ot ety st ey b st B e ol b At s el e i e gyt oot s et Sy 48 ool ey ericncs Cengage Liwing TGS e (W 14 Y ol o o e o € g 430 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 221,
            "page_label": "222"
        }
    },
    {
        "page_content": "Section 1 Test Tool Considerations 209 \nTool support for test design and specification \nThe tools described in this section support the testing activities described in \nChapter 4. These tools help to create maintainable test designs, test cases, test pro- \ncedures and test data. \nTest design tools \nTest design tools help to construct test cases, or at least test inputs (part of a test case). \nIf an automated oracle is available, then the tool can also construct the expected \nresult, so it can actually generate test cases, rather than just test inputs, \nTests that are designed using a tool need 1o have a starting point (o derive the tests \nfrom. This could be requirements from a requirements management tool, which may \nidentify valid and invalid boundary values for an input field, for example. Tests can \nalso be derived from elements on a screen, to ensure that each element is exercised by \na test, for example buttons, input fields or pull-down lists. Tests can also be derived \nfrom a model of the system, as in model-based testing (see below). Coverage tools \nmay identify the tests needed to extend white-box coverage. \nThere are two problems with test design tools, First, although it is relatively easy \n1o generate test inputs, automatically generating the correct expected result is more \nchallenging. Sometimes an oracle is available (for example valid boundary condi- \ntions), other times only a partial oracle (generating some aspect but not all detail of \nan expected result), and most often no oracle other than “The system is still running”. \nThe second problem is having oo many tests and needing to find a way of iden- \ntifying the most important tests to run. Cutting down an unmanageable number of \ntests can be done by risk analysis (see Chapter 5). \nModel-based testing tools \nTools that generate test inputs or test cases from stored information that represents \nsome kind of model of the system or software are called model-based testing tools. \nSuch tools may be based on a state diagram, to implement state transition testing. \nfor example. \nModel-based testing tools work by generating tests (inputs or test cases) automati- \ncally based on what is stored about the model used to describe the system behaviour. If \nthe system is changed, only the model is updated, and the new tests are then generated \nautomatically, \nMore information about Model-Based Testing (MBT) is available in the ISTQB \nFoundation Level Model-Based Tester Extension Syllabus and qualification. \nTest data preparation tools \nSetting up test data can be a significant effort, especially if an extensive range or \nvolume of data is needed for testing. Test data preparation tools help in this area, They \nmay be used by developers, but they may also be used during system or acceptance \ntesting. They are particularly useful for performance and reliability testing, where & \nlarge amount of realistic data is needed. \nThe most sophisticated tools can deal with a range of files and database formats. \nThese tools are also useful for anonymizing data to conform to data protection rules. \nTest-driven development (TDD) tools (D) \nTest-driven development (TDD) is a software development approach started as part \nof EXtreme Programming, which had a great influence on Agile development, The \nidea behind TDD is that tests are written first, before code, because if you think about \not S04 Crmpoge Loy N3 K Bt vl My et b o s o4 A 0 b 18 ot (b 3 s, . ot kot omtrs ey b smpqpesmed B e ol snbis Akt o1 eview e e e Je—p—— e Congage Lisming mares e (i 1 vy adibmcnd comem o ey o € shocgaes 430 (0maSns s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 222,
            "page_label": "223"
        }
    },
    {
        "page_content": "210 Chapter 6 Tool support for testing \nhow you will test something, you are anticipating what could go wrong; when you \nthen write the code, it is more likely to cope with problems. \nTDD tools provide a framework for writing and running tests. The tests that are \ndeveloped in TDD are basically unit tests; these are not sufficient testing for the \ncomponent being developed, but they are a good starting point, Other levels of testing \nare still needed, that is, integration, system and acceptance testing. \nAcceptance test-driven development (ATDD) and behaviour-driven devel \nopment (BDD) tools \nTools to develop system-level tests before developing the system itself are becoming \nincreasingly popular. ATDD is a way of capturing requirements by writing accep- \ntance tests in conjunction with users, This approach is also called Specification \nby Example, as in Adzic [2011]. BDD is focused on system behaviour and func- \ntionality, and is normally done by the development team. Tools to support ATDD \n/BDD usually have syntax (rules), like a natural language, that need to be followed. \nAn example of this syntax is the ‘Given/When/Then format: ‘Given <some con- \ndition>, When <something is done>, Then <some result should happen>’. Another \nformat is ‘As a <role, for example customer>, In order to <achieve something, for \nexample buy a product>, | want to <do something, for example place an order from \nmy Y. \nThe ATDIVBDD tools support this domain language for specifying the tests, and \nalso enable those tests 1o be generated and run. \nTools may cover both test design/implementation and test execution/logging. Both \nTDD and ATDD/BDD tools enable the tests specified in the tools 1o be run, so they \ncover test execution as well as test design and implementation. In addition, some other \ntest design and integration tools, for example MBT tools, provide outputs directly to \ntest execution and logging tools. These tools were described in this section, but also \nprovide support for the test activities described below. \nTool support for test execution and logging \nTest execution tools \nWhen people think of a testing tool, it is usually a test execution tool that they have \nin mind, a tool that can run tests. This type of tool is also referred to as a test running \ntool. Some tools of this type offer a way to get started by capturing or recording \nmanual tests; hence they are also known as capture/playback tools, capture/replay \ntools or record/playback tools. The analogy is with recording a television programme \nand playing it back. However, the tests are not something which is played back just \nfor someone to watch! \nTest execution tool Test execution tools usc a scripting language to drive the tool. The scripting lan- \nA test tool that guage is actually a programming language. Testers who wish 1o use a test execution \nexecutes tests against tool directly may need to use programming skills to create and modify the scripts, \na designated test although some newer tools provide a higher-level interface for testers. The advantage \n:\" and evaluates of programmable scripting is that tests can repeat actions (in loops) for different data \nvesilts dnd values (test inputs), they can take different routes depending on the outcome of a test \nm’. (for example if a test fails, go 10 a different set of tests) and they can be called from \nother scripts giving modular structure to the set of tests. \nWhen people first encounter a test execution tool, they tend to use it to capture \n(or record) tests, which sounds really good when you first hear about it. However, the \napproach breaks down when you try to replay the captured tests. This approach does \nG N Crmpogs Lowming. A3 K Bt vt My et b o, s, o4 e, 0 s 18 . o, U 3 i, e, s sty comto ey b gy B e el snbis At Je—p—— e e b et P Conpage Linmay muries e 1y 10 ey kbacnd o o a1 o € whacacs YA (7T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 223,
            "page_label": "224"
        }
    },
    {
        "page_content": "Section 1 Test Tool Considerations 211 \nnot scale up for large numbers of tests, as described in Section 6.1.3. The main reason \nfor this is that a captured script is very difficult to maintain because: \n@ Itis closely tied to the flow and interface presented by the GUI (graphical user \ninterface). \n® It may rely on the circumstances, state and context of the system at the time \nthe script was recorded. For example. a script will capture a new order number \nassigned by the system when a test is recorded. When that test is played back, \nthe system will assign a different order number and reject subsequent requests \nthat contain the previously captured order number. \n® The test input information is hard-coded, that is, it is embedded in the indi- \nvidual script for each test. \nAny of these things can be overcome by modifying the scripts, but then we are \nno longer just recording and playing back! If it takes more time to update a captured \ntest than it would take to run the same test again manually, the scripts tend to be \nabandoned and the tool becomes ‘shelf-ware’. \nThere are better ways to use test execution tools to make them work well and \nactually deliver the benefits of unattended automated test running. There are at least \nfive levels of scripting and also different comparison techniques. Data-driven testing \nis an advance over captured scripts, but keyword-driven testing gives significantly \nmore benefits. See Fewster and Graham [1999], Buwalda er al. [2001], Graham and \nFewster [2012), Gamba and Graham [2018] and Section 6.2.3. \nThere are many different ways 10 use a test execution tool and the tools themselves \nare continuing to gain new useful features. Test execution tools are often used for \nregression testing, where tests are run that have been run before, such as in continu- \nous integration. One of the most significant benefits of using this type of tool is that \nwhenever an existing system is changed (for example for a defect fix or an enhance- \nment), the tests that were run earlier can be run again, to make sure that the changes \nhave not disturbed the existing system. \nCoverage tools (for example requirements coverage, code coverage (D)) \nHow thoroughly have you tested? Coverage tools can help answer this question. \nRequirements coverage tools measure how many requirements have been exercised by \naset of tests; code coverage tools (D), normally used by developers, measure how many \ncode elements, such as statements or branches, have been exercised by a set of tests, \nA coverage tool first identifies the elements or coverage items that can be counted, \nand where the tool can identify when a test has exercised that coverage item. At \ncomponent testing level, the coverage items could be lines of code or code statements \nor decision outcomes (for example the True or False exit from an IF statement). At \ncomponent integration level, the coverage item may be a call to a function or module. \nAt system or acceptance level, the coverage item may be a user story. a requirement, \na function or a feature, \nThe coverage tool counts the number of coverage items that have been executed \nby the test suite and reports the percentage of coverage items that have been exer- \ncised and may also identify the items that have not yet been exercised (that is, not yet \ntested). Additional tests can then be run to increase coverage. \nNote that the coverage tools only measure the coverage of the items that they can \nidentify. Just because your tests have achieved 100% coverage, this does not mean \nthat your software is 100% tested! See also Chapter 4 Section 4.3, \nG N Crmpoge Lowming A3 g Bt vt My et b o, s, o4 . s 10 . o, (b 3 o, e, s s pacty csto ey b gy B e el b At s el e i e gyt oo s et sy o el ey ericncs Cengge Liwing AT e (W 14 Y adlbmcnd o o e o € g 30 (s e",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 224,
            "page_label": "225"
        }
    },
    {
        "page_content": "212 Chapter 6 Tool support for testing \nTest harnesses (D) \nSome software components cannot be tested independently, for example when they \ndepend on receiving data from other components, or they need to send a message \n1o a different component or system and receive a reply. Waiting until such compo- \nnents are integrated with the whole system is asking for trouble. It is much better 10 \ntest on as small a scale as possible. In this and similar cases, the component can be \ntested if the data it needs can be supplied 10 it in another way. A driver is another \ncomponent or tool that will call or invoke the component we want to test and supply \nit with the data it needs, A stub is a small component that interacts with the tested \ncomponent in the way it expects, but just for that purpose, so it pretends to be the \npart that receives the message and returns a reply. Stubs may also be referred to as \nmock objects. \nA test harness is the test environment that provides these drivers and stubs so that \nour component can be tested as much as possible on its own, \nUnit test framework tools (D) \nA unit test framework is another tool which is used by developers when testing indi- \nvidual components (or sets of integrated components). Unit test frameworks can be \nused in Agile development to automate tests in parallel with development. Both unit \ntest frameworks and test harnesses enable the developer to test, identify and localize \nany defects. The framework can also act as a driver and/or as a mock object to supply \nany information needed by the software being tested (for example an input that would \nhave come from a user) and also receive any information sent by the software (for \nexample a value 1o be displayed on a screen), \nTest harnesses or unit test frameworks may be developed in-house for particular \nsystems. \nThere are a large number of xUnit tools for different programming languages, \nfor example JUnit for Java, NUnit for .Net applications, etc. There are both com- \nmercial tools and also open source tools. Unit test framework tools are very similar \n1o test execution tools, since they include facilities such as the ability to store test \ncases and monitor whether tests pass or fail, for example. The main difference is \nthat there is no capture/playback facility and they tend to be used at a lower level, \nthat is, for component or component integration testing, rather than for system or \nacceptance testing. Unit test frameworks may also provide support for measuring \ncoverage. \nTool support for performance measurement and dynamic analysis \nThe tools described in this section support testing that can be carried out on a system \nwhen it is operational, that is, while it is running. This can be during testing or could \nbe after a system is released into live operation. These tools support activities that \ncannot reasonably be done manually. \nPerformance testing tools \nPerformance testing  Performance testing tools are concerned with testing at system level 10 see whether \ntool A test tool that or not the system will stand up to a high volume of usage. A load test checks that the \nmw';: system can cope with its expected number of transactions. A volume test checks that \nmm the system can cope with a large amount of data, for example many fields in a record, \nrecords its performance many records in a file, etc. A stress test is one that goes beyond the normal expected \nduring test execution. usage of the system (1o see what would happen outside its design expectations), with \nrespect to load or volume. \nG N Crmpoge Loswming. A3 g Bt vt My et b o, s, o4 b s, 0 s 18 . o, U 3 i, e, s sty comto ey b gy B e el smbs At e e b ety oo s et T Conpuge Lowmay muries e 1y 10wy kbacnd o o 1} wow € whacacs YA (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 225,
            "page_label": "226"
        }
    },
    {
        "page_content": "Section 1 Test Tool Considerations 213 \nIn performance testing, many test inputs may be sent to the software or system where \nthe individual results may not be checked in detail. The purpose of the test is to measure \ncharacteristics, such as response times, throughput or the mean time between failures \n(for reliability testing). Load generation can simulate multiple users or high volumes of \ninput data. More sophisticated tols can generate different user profiles, different types \nof activity, timing delays and other parameters. Adequately replicating the end-user \nenvironments or user profiles is usually key to realistic results. Performance testing tools \nnormally provide reports based on test logs and graphs of load against response times, \nAnalyzing the output of a performance testing tool is not always straightfor- \nward and it requires time and expertise, If the performance is not up to the standard \nexpected, then some analysis needs to be performed to see where the problem is and \nto know what can be done to improve the performance. \nMonitoring tools \nMonitoring tools are used to continuously keep track of the status of the system in \nuse, in order to have the earliest warning of problems and to improve service. There \nare monitoring tools for servers, networks, databases, security, performance, website \nand internet usage, and applications, \nMonitoring tools may help to identify performance problems and network prob- \nlems, and give information about the use of the system, such as the number of users. \nDynamic analysis tools (D) \nDynamic analysis tools are so named because they require the code to be running. \nThey are analysis rather than testing tools because they analyze what is happening \nbehind the scenes while the software is running (whether being executed with test \ncases or being used in operation). \nAn analogy with a car may be useful here. If you go to look at a car to buy, you \nmight sit in it to see if is comfortable and see what sound the doors make, This would \nbe static analysis because the car is not being driven. If you take a test drive, then you \nwould check that the car performs as you expect (for example, it turns right when you \nturn the steering wheel clockwise). This would be a test. While the car is running, if \nyou were to check the oil pressure or the brake fluid, this would be dynamic analysis. \nIt can only be done while the engine is running, but it is not a test case. \n‘When your PC’s response time gets skower and slower over time, but is much improved \nafter you re-boot it, this may well be due to a memory leak, where the programs do not \ncorrectly release blocks of memory back to the operating system. Eventually the system \nwill run out of memory completely and stop. Reboating restores all of the memory that \nwas lost, so the performance of the system is now restored 1o its normal state. \nDynamic analysis tools would typically be used by developers in component test- \ning and component integration testing. \nTool support for specialized testing needs \nIn addition to the tools that support specific testing activities, there are tools that \nsupport other testing needs and more specific testing issues. Tool support is available \n1o support these other testing needs. \nData quality assessment \nData quality is very important, and there are tools to assess it. In many IT-centric \norganizations, systems and ‘systems of systems’ manage very large volumes of \ncomplex, interrelated data. This puts data at the centre of many projects in these \nom0 300 o L 8 s Rt Ay b i et o .t dmes s e Gy s e e ey o e ot b et e e b et e Conpage Linmay murses e 1y 10wy akbacnd o o 10} W € hocacs YA (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 226,
            "page_label": "227"
        }
    },
    {
        "page_content": "214 Chapter 6 Tool support for testing \norganizations, These tools can check data according to given validation rules, for \nexample, checking that a particular field is numeric or of a given length. Any data \nthat fails a check is reported for someone to look at and fix. \nData conversion and migration \nWhen technology moves on, it often becomes necessary to have to transfer large sets \nof data from one type of storage to another. The data involved can vary in terms of \ncriticality and volume. Data conversion and migration tools can review and verify data \nconversion and check that data has been migrated according to the migration rules. For \nexample, a field in the new database may be larger than the equivalent field in the old \none, so the value stored may need to have extra characters or digits added to it. These \ntools can help ensure that the processed data is correct and complete, and that it com- \nplies to pre-defined, context-specific standards, even when the volume of data is large. \nUsability testing \nTools to support usability testing can help to assess user experience in using the \nsystem, for example, surveys after using a website or mobile app. Tools can check to \nmake sure there are no broken links on a web page. Tools can also monitor usage, \nsuch as which links are clicked most often. Video recorders and screen capture utili- \nties can also be used to help assess usability. Major companies that sell to the public \nmay also have a usability lab, where beta versions of software are used by volunteers \nfrom the target market; this could include video, analysis of key presses or even eye \nmovement tracking. \nAccessibility testing \nRegulations now require that systems including websites are usable by people with \nvarious disabilities. For example, it should be possible to increase the font size of text, \nFor blind users, each visual object on a screen should have alternative text associated \nwith it which can be read by a screen reader. For example, a field with a magnifying \nglass on the left would have the alternative text ‘Search’; an image of a horse gallop- \ning would have the text ‘Galloping horse’. Alternative text is particulardy important \nfor buttons and interactive images, such as the “Submit’ button. See www.w3.org for \nmore information about accessibility for websites, \nLocalization testing \nThis is also called internationalization, Localization is making sure that systems are \nunderstood in different countries and in different cultures. This involves translation \nof text for information, menu items, buttons, error messages, field names: in fact any- \nthing that someone will read on a page. Although tools can help in some ways with \ntranslations, the twols often get it wrong, particularly with colloquial expressions, \nTools can be of more help in other ways, for example if the same translation is used \nin many different environments or platforms (such as mobile devices). This is one \narea where human intelligence is still needed. \nSecurity testing \nThere are a number of tools that protect systems from external attack, for example \nfirewalls, which are important for any system. \nSecurity testing tools can be used 1o test security by trying to break into a system, \nwhether or not it is protected by a security tool. The attacks may focus on the net- \nwaork, the support software, the application code or the underlying database, \not S04 Crmpoge Loy N3 K Bt vl My et b o s o4 A 0 s 18 ot (b 3 s, e, st kot omtrs ey b smpppesmed B e ol sndis Akt 11 el e M e gyt oo s et Sy S el g pericncs Cengage Ly TS e (W 14 Y ol o o e o € g 30 (s s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 227,
            "page_label": "228"
        }
    },
    {
        "page_content": "Section 1 Test Tool Considerations 215 \nSecurity testing tools can provide support in identifying viruses, detecting intru- \nsions such as denial of service attacks, simulating various types of external attacks, \nprobing for open ports or other externally visible points of attack, and identifying \nweaknesses in password files and passwords. In addition, these tools can perform \nsecurity checks during operation, for example checking the integrity of files, intru- \nsion detection and the results of test attacks. \nPortability testing \nPortability testing tools help to support testing on different platforms and in different \nenvironments, You may have an application that needs to perform the same functions \non desktop computers, laptops, tablets and mobile phones. Although the function- \nality is the same, the way in which the information is displayed would be different. \nPortability testing tools can help to run the same tests on the different devices, even \nthough the look and feel of each interface is different. \nAbout tools in general \nIn this chapter, we have described tools according to their general functional classi- \nfications. There are also further specializations of tools within these classifications. \nFor example, there are web-based performance testing tools as well as performance \ntesting tools for back-office systems. There are static analysis tools for specific devel- \nopment platforms and programming languages, since each programming language \nand every platform has distinct characteristics. There are dynamic analysis tools that \nfocus on security issues, as well as dynamic analysis tools for embedded systems. \nTool sets may be bundled for specific application areas such as web-based, \nembedded systems or mobile applications. \n6.1.2 Benefits and risks of test automation \nThe reason for acquiring tools to support testing is to gain benefits, by using software \nto do certain tasks that are better done by a computer than by a person. \nThe term test automation is most often used to refer to tools that exccute tests  Test automation The \nand compare results, that is, test execution tools. This type of tool is one which can  use of software to \nprovide great benefits, but there are also significant risks. Test automation is also  perform or support test \nmuch broader than just supporting execution; tools are available 10 support many activities, for example, \naspects of testing. :s‘hm \nPotential benefits of using tools i \nThere are many benefits that can be gained by using tools to support testing, whatever \nthe specific type of tool. Benefits include: \nReduction in repetitive manual work \nRepetitive work is tedious to do manually. People become bored and make mistakes \nwhen doing the same task over and over. Examples of this type of repetitive work \ninclude running regression tests, entering the same test data over and over again \n(both of which can be done by a test execution tool), checking against coding stan- \ndards (which can be done by a static analysis tool) or running a large number of tests \nthrough the system in a short time (which can be done by a performance testing tool). \nTools can also help to set up or tear down test environments. Using tools to help with \nrepetitive work can save time (us well as frustration). \nG N Compuge Lonwming. A3 g Bt vt My et b o, s, o4 e i, 0 s 18 . o, U 3 st e, s sty commr ey b gy B e el snbs At Je—p—— e e b et P Conpage Lonmay murves e 1y 10 ey akbacnd o o 0} wow € hacacs YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 228,
            "page_label": "229"
        }
    },
    {
        "page_content": "216 Chapter 6 Tool support for testing \nGreater consistency and repeatability \nPeople tend to do the same task in a slightly different way even when they think they \nare repeating something exactly. A tool will exactly reproduce what it did before, so \neach time it is run, the result is consistent. Examples of where this aspect is beneficial \ninchude checking to confirm the correctness of a fix to a defect (which can be done by \na debugging tool or test execution tool), entering test inputs (which can be done by a \ntest execution tool) and generating tests from requirements (which can be done by a test \ndesign tool, MBT tool or possibly a requirements management tool). \nMore objective assessment \nIf a person calculates a value from the software or defect reports, they may inad- \nvertently omit something, or their own subjective prejudices may bead them to inter- \npret that data incorrectly. Using a tool means that subjective bias is removed and \nthe assessment is more repeatable and consistently calculated, Examples include \nassessing the structure of a component (which can be done by a static analysis tool), \nmeasuring coverage of the test item by a set of tests (coverage measurement tool), \nassessing system behaviour (monitoring tools) and defect statistics (fest management \ntool or defect management tool). \nEasier access to information about testing \nHaving lots of data does not mean that information is communicated. Information \npresented visually is much easier for the human mind to take in and interpret. For \nexample, a chart or graph is a better way to show information than a long list of num- \nbers. This is why charts and graphs in spreadsheets are so useful. Special-purpose \ntools give these features directly for the information they process. Examples include \nstatistics and graphs about test progress (test execution tool or test management tool), \ndefect rates (defect management or test management tool) and performance (perfor- \nmance testing tool). \nIn addition to these general benefits, each type of tool has specific benefits relating \nto the aspect of testing that the particular tool supports. These benefits are normally \nprominently featured in the information available for the type of tool. It is worth \ninvestigating a number of different tools to get a general view of the benefits. \nRisks of using tools \nAlthough there are significant benefits that can be achieved using tools to support \ntesting activities, there are many organizations that have not achieved the benefits \nthey expected. \nSimply purchasing a tool is no guarantee of achieving benefits, just as buying \nmembership in a gym does not guarantee that you will be fitter. Each type of tool \nrequires investment of effort and time in order to achieve the potential benefits. \nThere are many risks that are present when tool support for testing is introduced \nand used, whatever the specific type of tool. Risks include: \nExpectations for the tool may be unrealistic (including functionality and \nease of use) \nUnrealistic expectations may be one of the greatest risks to success with tools, The \ntools are only software and we all know that there are many problems with any kind \nof software! It is important to have clear objectives for what the tool can do and that \nthose objectives are realistic. Unrealistic expectations may be both for functionality \nand for case of use. \nGt N Crmpogs Loswming. A3 K Bt vt My et b o, s, o4 e . 0 s 18 . o, L 30 et e, ot sty conto ey b gyt B e el b At v e e e oo s et P Conpuge Lonmay muries e 1y 10 ey kbacnd o o s} Vo € whacacm YA (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 229,
            "page_label": "230"
        }
    },
    {
        "page_content": "Section 1 Test Tool Considerations 217 \nThe time, cost and effort for the initial introduction of a tool may be \nunder-estimated (including training and external expertise) \nIntroducing something new into an organization is seldom straightforward. Having \npurchased a tool, you will want to move from downloading the tool to having a num- \nber of people being able to use the tool in a way that will bring benefits. There will be \ntechnical problems to overcome, but there will also be resistance from other people — \nboth need to be addressed in order to succeed in introducing a tool. Often forgotten \nare the time, cost and effort or external expertise to help get things onto the right \ntrack, and for training, not just in the use of the tool itself, but internal training after \na pilot project so that everyone is using the tool and agreed internal conventions in a \nconsistent way for maximum benefit. Simply acquiring a tool and getting it started \nis not enough either. When a tool is introduced, it will affect the way testing is done, \n50 test processes and test documentation will need to change. \nThe time and effort needed to achieve significant and continuing benefits \nfrom the tool may be under-estimated \nThink back to the last time you did something new for the very first time (learning \nto drive, riding a bike, skiing). Your first attempts were unlikely to be very good, but \nwith more experience you became much better. Using a testing tool for the first time \nwill not be your best use of the tool either. It takes time to develop ways of using the \ntool in order to achicve what's possible. In the excitement of acquiring a new tool, it \nis easy to forget about things like the need for changes in the test process as the 1ol \nis implemented. These changes should be planned from the start. \nBut it doesn't stop there, It is critical for continuing success that there is continuous \nimprovement in the way the tool is used. This may involve re-factoring automation \nassets from time to time or changing the way those assets are organized, Fortunately, \nthere are some short cuts (for example reading books and articles about other peo- \nple’s experiences and learning from them). See also Section 6.2.1 for more detail on \nintroducing a tool into an organization, \nThe effort required to maintain the test assets generated by the tool may \nbe under-estimated \nInsufficient planning for maintenance of the assets that the wol produces and uses \nis a strong contributor to tools that end up as shelf-ware, along with the previously \nlisted risks, Although particularly relevant for test execution tools, planning for main- \ntenance is also a factor with other types of tool. \nThe tool may be relied on too much (seen as a replacement for test design \nor execution, or the use of automated testing where manual testing would \nbe better) \nTools are definitely not magic! They can do very well what they have been designed \nto do (at least a good quality tool can), but they cannot do everything. A tool can \ncertainly help, but it does not replace the intelligence needed to know how best 1o use \nit, and how to evaluate current and future uses of the tool. A test execution tool does \nnot replace the need for good test design and should not be used for every test. Some \ntests are still better executed manually. For example, a test that takes a very long time \nto automate and will not be run very often is better done manually. \nVersion control of test assets or interoperability may be neglected \nConfiguration management is important for test assets, and that definitely includes auto- \nmation assets. If this is neglected, it can lead to significant and unnecessary problems. \nG N Compoge Lowming. A3 K Bt vt My et b o, s, o4 e . 0 s 16 . o, U 3 i e, s sty comto ey b gy B e el b At o v e e e oo s et P Conpuge Lonmay muries e 1y 10 ey koo o o a1 wow € whacaacm IS (74T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 230,
            "page_label": "231"
        }
    },
    {
        "page_content": "218 Chapter 6 Tool support for testing \nRelationships and interoperability issues between critical tools may be neglected, \nsuch as requirements management tools, configuration management tools, defect man- \nagement tools, and tools from multiple vendors. Most organizations have a number of \ndifferent tools, often to support different aspects of development and testing. Some oega- \nnizations may have a tool sute with many different tool types bundled together, others \nmay have a number of tools that are sourced from a variety of sources. The relationships \nand interoperability between the various tools can cause problems. These should be \ntaken into account when acquiring a new tool, but when tools change, this may also \ncause interoperability problems to change. This is particularly important for continuous. \nintegration tools, but the problems there will become obvious quickly. Problems with \nintegrating other tools may not be so obvious, but may store up problems for fater. \nThe tool vendor may go out of business, retire the tool, or sell the tool \nto a different vendor \nA commercial tool vendor may not be able to continue to support a tool that you have \ncome to rely on. If a tool is acquired by a different organization, they may not have \nthe same priorities. Sometimes tols that have been very popular are phased out. Open \nsource tools also change over time and may become less suitable for you. When you \nchoose a tool, always think about how you would cope if you had to change to a different \ntool, and structure your automation to cope with this, even if it seems very unlikely now. \nThe vendor may provide a poor response for support, upgrades and \ndefect fixes \nCommercial vendors, or those supporting open source tools, may not respond quickly \nor adequately to questions, problems or bugs found in the tools (yes it does happen!). \nUpgrades may cause problems for you because of the way that you have used the tool, \nbut the vendor may not be interested in helping you if you are in a small minority of \ntheir customers, \nAn open source project may be suspended \nEven open source projects can be affected. Since they are very much supported by \na community, if the people involved are no longer active, it may be difficult 10 get \nhelp with tool problems. \nA new platform or technology may not be supported by the tool \nYour own applications are changing over time as well, moving onto new platforms \nand using new technology. If you are in the forefront of your industry. your testing \ntools may not yet be able to deal with these innovations, \nThere may be no clear ownership of the tool (for example for mentoring, \nupdates etc.) \nThere should be one person who is responsible for technical aspects of the tool. This \nperson may be an enthusiastic champion for automation in general, or they may have \namore technical role and be the person who deals with licences, reporting problems, \ninstalling updates, etc. If no one takes ownership of the tool, there will be problems. \nin the future, and the automation will begin to decay. \nThis list of risks is not exhaustive. Two other important factors are: \n@ The skill needed to create good tests. \n® The skill needed to use the tools well, depending on the type of tool. \nG N Crmpogs Lowming. A3 K Bt vt My et b o, s, o4 e, 0 s 18 . o, U 3 i, e, s sty comto ey b gy B e el snbis At eview e e e oo s et P Conpage Linmay muries e 1y 10 ey kbacnd o o a1 o € whacacs YA (7T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 231,
            "page_label": "232"
        }
    },
    {
        "page_content": "Section 1 Test Tool Considerations 219 \nThe skills of a tester are not the same as the skills of the tool user. The tester \nconcentrates on what should be tested, what the test cases should be and how to \nprioritize the testing. The tool user concentrates on how best to get the tool to do its \njob effectively and how to give increasing benefit from tool use. \n6.1.3 Special considerations for test execution and test \nmanagement tools \nTest execution tools \nIn order to know what tests to execute and how to run them, the test execution tool \nmust have some way of knowing what to do ~ this is the script for the tool, But since \nthe tool is only software, the script must be completely exact and unambiguous to \nthe computer, which has no commeon sense. This means that the seript is a program, \nwritten in a programming language. The scripting language may be specific to a par- \nticular tool, or it may be a more general language. Scripting languages are not used \nJust by test execution tools, but the scripts used by the tool are stored electronically \nto be run when the tests are executed under the tool’s control. \nThere are tols that can generate scripts by identifying what is on the screen rather \nthan by capturing a manual test, but they still generate scripts to be used in execution: \nthey are not “script-free’ or “code-free’, This later generation of tools may use smart \nimage-capturing technology and they can help to generate initial scripts quickly, so \nmay be more useful than the traditional capture/replay tools. However, maintenance \nwill still be needed on the scripts over time as the user interface evolves, for example. \nThis will need someone to be able to work directly with the scripting language. The \nsales pitches for these tools may imply that this is not necessary, but this is miskading. \nThere are different levels of scripting. Five are described in Fewster and Graham \n[1999): \n® Linear scripts, which could be created manually or captured by recording a \nmanual test. \n® Structured scripts, using selection and iteration programming structures. \n® Shared scripts, where a script can be called by other scripts so can be re-used: \nshared scripts also require a formal script library under configuration management. \n@ Data-driven scripts, where test data is in a file or spreadsheet to be read by a \ncontrol script. \no Keyword-driven scripts, where all of the information about the test is stored \nin a file or spreadsheet, with a single control script that implements the tests \ndescribed in the file using shared and keyword scripts. \nCapturing a manual test seems like a good idea to start with, particularly if you are \ncurrently running tests manually anyway. But a captured test (a linear script) is not \na good solution, for a number of reasons, including: \n® The script does not know what the expected result is until you program it in. It \nonly stores inputs that have been recorded. not test cases. \n® A small change to the software may invalidate dozens or hundreds of scripts, \n® The recorded script can only cope with exactly the same conditions as when \nit was recorded. Unexpected events (for example a file that already exists) \nwill not be interpreted correctly by the tool. \nGt N Crmpoge Lossming. A3 Kgh Bt vt My et b o, s, o4 B i, 0 s 10 . o, U 3 e, e, s sty comto ey b gy B e el smbs AChagiti s Je——— i e b s s & cvent barwny Conpage Lowmay maries e 1 10 ey akbacnd o o a1 o € whacgacm A (7 0Tv s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 232,
            "page_label": "233"
        }
    },
    {
        "page_content": "220 Chapter 6 Tool support for testing \nHowever, there are some times when capturing test inputs (that is, recording a \nmanual test) is useful. For example, if you are doing exploratory testing or if you \nare running unscripted tests with experienced business users, it can be very helpful \nsimply to record everything that is done, as an audit trail. This serves as a form of \ndocumentation of what was tested (although analyzing it may not be easy). This audit \ntrail can also be very useful if a failure occurs which cannot be easily reproduced. \nThe recording of the specific failure can be played to the developer to see exactly \nwhat sequence caused the problem. \nCaptured test inputs can be useful in the short term, where the context remains \nvalid. Just don't expect to replay them as regression tests (when the context of the \ntest may be different). Captured tests may be acceptable for a few dozen tests, where \nthe effort 1o update them when the software changes is not very large. Don't expect \na linear scripting approach to scale to hundreds or thousands of tests, \nCapturing tests does have a place, but it is not a large place in terms of automating \ntest execution, \nData-driven testing uses scripts that allow the data, that is, the test inputs and \nexpected outcomes, 1o be stored separately from the script, This can include situations. \nwhere, instead of the tester putting hard-coded data combinations into a spreadsheet, \na tool generates and supplies data in real time, based on configurable parameters. \nFor example, a tool may generate a random user 1D for sccounts and can take advan- \ntage of random number seeding to ensuring repeatability in the pattern of user IDs. \nWhichever way data-driven testing is implemented, it has the advantage that a tester \nwho does not know how to use a seripting language can populate a file or spreadsheet \nwith the data for a specific test. This is particularly useful when there are a large \nnumber of data values that need to be tested using the same control seript, \nKeyword-driven testing uses scripts that include not just data but also key- \nwords in the data fike or spreadsheet. In other words, the spreadsheet or data file \ncontains keywords that describe the actions to be taken and the test data. Action \nwords is another term used for keywords as described in Buwalda et al. [2001), \nKeyword-driven (or action word) automation enables a tester (who is not a script \nprogrammer) to devise a great variety of tests, not just varied input data for essen- \ntially the same test, as in data-driven scripts. The tester needs 1o know what key- \nwords are currently available to use (by someone having written a script for it) and \nwhat data the keyword is expecting, but the tester can then write tests, not just enter \nspecific test data. The tester can also request additional keywords to be added 10 \nthe available programmed set of scripts as needed. Keywords can deal with both \ntest inputs and expected outcomes, \nSomeone still needs 1o be able to use the tool directly and be able to program in \nthe tool's scripting language, in order to write and debug the keyword scripts that \nwill use the data tables or keyword tables. A small number of automation specialists \ncan support a larger number of testers. who then do not need to learn to be script \nprogrammers (unless they want to). \nThe data files (data-driven or keyword-driven) include the expected results for the \ntests. The actual results from each test run also need to be stored, at least until they \nare compared 1o the expected results and any differences are logged. \nWhatever form of scripting is used, there are two important points to remember: \n® Someone needs 1o have expertise in the scripting language of the tool, \n® The expected results of a test still need to be compared to the actual result \nproduced by the application when the (automated) test is run. \nG N Compogs Loswming. A3 gt Bt My ek b o, s, o4 b . s 10 . e, U 3 i g, s sty cosmo ey b gy B e el snbis At \nJe—p—— Conpget e e v et s 8 cven arwny s e e 4 vy sl o o o € g 30 (FT s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 233,
            "page_label": "234"
        }
    },
    {
        "page_content": "Section 1 Test Tool Considerations 221 \nThere are two types of result comparison: dynamic, which is done while the test \nis running, or post-execution, which is done after the test completes. Dynamic com- \nparison is best for things like pop-ups or error messages during a test, and can help \nto determine the subsequent progress of the test. Post-execution comparison is best \nfor comparing large volumes of data, for example in a database, after tests have made \nmany changes. \nModel-based testing tools include not only test execution capability, but can also \ngenerate test inputs and expected results. The tool stores a functional specification \nsuch as an activity diagram or finite state machine. This is generally specified by a \nsystem designer. The MBT tool uses the stored model to generate inputs and expected \nresults. For example, a state diagram shows the type of input that would generate a \nstate transition. The tool selects a sample value from the input space, and the expected \nresult would include the end state after the transition. The generated test cases can be \nstored in a test management tool or run by a test execution tool. Note that there is an \nISTQB qualification for MBT: Foundation Level Model-Based Testing (an extension \nto the Foundation level). \nMore information on data-driven and keyword-driven scripting can be found \nin Fewster and Graham [1999], Buwalda et al [2001], Janssen and Pinkster [2001], \nGraham and Fewster [2012] and Gamba and Graham [2018]. Note that there is an \nISTQB qualification in test sutomation: Advanced Level Test Automation Engineer. \nTest management tools \nTest management tools can provide a lot of useful information, but the information as \nproduced by the tool may not be in the form that will be most effective within your \nown context. Some additional work may be needed to produce interfaces to other \ntools or a spreadsheet in order to ensure that the information is communicated in the \nmost effective way. \nTest management tools need to interface with other tools (including spreadsheets \nfor example) for various reasons, including: \no To produce useful information in a format that fits the needs of the organization. \no To maintain consistent traceability 1o requirements in a requirements manage- \nment tool. \n@ To link with test object version information in the configuration management \ntool. \nTools such as application lifecycle management (ALM) tools may have aspects \nthat are used by different people within the organization. For example, a high-level \nmanager wants to see trends and graphs about test progress, application quality \nschedules and budgets, but a developer wants to see detailed information about how \na defect occurred, \nA report produced by a test management tool (either directly or indirectly through \nanother tool or spreadsheet) may be a very useful report at the moment, but the same \ninformation may not be useful in three or six months. It is important to monitor the \ninformation produced to ensure it is the most relevant now. \nIt is important 1o have a defined test process before test management tools are \nintroduced. If the testing process is working well manually, then a test management \ntool can help to support the process and make it more efficient. If you adopt a test \nmanagement tool when your own testing processes are immature, one option is to \nfollow the standards and processes that are assumed by the way the tool works. \nG N Crmpogs Loswming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At s v e e s oo st P Conpge Lewmay mures e 13 10 ey akoacd o o sy Vo € whacacs A (7O s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 234,
            "page_label": "235"
        }
    },
    {
        "page_content": "222 Chapter 6 Tool support for testing \nThis can be helpful, but it is not necessary to follow the processes specific to the \ntool. The best approach is to define your own processes, taking into account the \n100l you will be using. and then adapt the tool to provide the greatest benefit 10 \nyour organization. \n6.2 EFFECTIVE USE OF TOOLS \nS FOR 6.2 EFFECTIVE USE LLABUS LEARNING OBJEC \nOF TOOLS (K1) \nFL-62.1 Identify the main principles for selecting a tool (K1) \nFL-622 Recall the objectives for using pilot projects to introduce \ntools (K1) \nFL-6.23 Identify the success factors for evaluation, implementation, \ndeployment, and on-going support of test tools in an organi- \nzation (K1) \nIn this section, we discuss the principles and process of introducing tools into your \nonganization. We look at the process of tool selection. We'll talk about how to carry \nout tool pilot projects, We'll conclude with thoughts on what makes tool introduction \nsuccessful. There are no Glossary terms for this section. \nLots of tool advice is online, and good advice on automation can be found in Few- \nster and Graham (1999 and 2012), Gamba and Graham [2018] and Axelrod [2018]. \n6.2.1 Main principles for tool selection \n‘The place to start when introducing a tool into an organization is not with the tool: it \nis with the organization. In order for a tool to provide benefit, it must match a need \nwithin the organization, and solve that need in a way that is both effective and effi- \ncient. The tool should help to build on the strengths of the organization and address its \nweaknesses, The organization needs 1o be ready for the changes that will come with \nthe new tool. If the current testing practices are not good and the organization is not \nmature, then it is generally more cost-effective to improve testing practices rather than \ntotry to find tools to support poor practices. Automating chaos just gives faster chaos! \nOf course, we can sometimes improve our own processes in parallel with introduc- \ning a tool to support those practices. We can pick up some good ideas for improve- \nment from the ways that the tools work. However, be aware that the tool should not \ntake the lead, but should provide support to what your organization defines. \nThe following factors are important in selecting a tool: \n® Assessment of the organization’s matunity (for example strengths and weak- \nnesses, readiness for change). \no ldentification of the areas within the organization where tool support will help \nto improve lesting processes. \no Understanding the technologies used by the test object(s), so that a tool will be \nselected that is compatible with those technologies. \no N Crmpoge Lossming. A3 Ksghs Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 i, g, s sty comto ey b sy B e el smbs At s e s dmd ety oo st s & cvent barwny Conpe! s e e o e o sy e € - .",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 235,
            "page_label": "236"
        }
    },
    {
        "page_content": "Section 2 Effective use of Tools 223 \n® Knowledge of any build and continuous integration tools already being used \nwithin the organization, to make sure that the new tool(s) will integrate with \nthem and be compatible. \n@ Evaluation of tools against clear requirements and objective eriteria, \n@ Consideration of any free trial period for the tool (for commercial tools) to \nensure that this gives adequate time to evaluate the wol. \no Evaluation of the vendor (including training, support and other commercial \naspects) or support for non-commercial tools (open source). \no Identification of internal requirements for coaching and mentoring in the use of \nthe tool. \no Evaluation of training needs for those who will use the tools directly and indi- \nrectly (for example without technical detail), taking into account testing skills \nand test automation skills (for those working directly with the tools). \no Consideration of pros and cons of different licencing models (for example com- \nmercial or open source). \no Estimation of a cost-benefit ratio based on a concrete and realistic business \ncase (if required). \nA final step would be to do a proof-of-concept evaluation. The purpose of this is \nto see whether or not the tool performs as expected, both with the software under \ntest and with any other tools or aspects of your own infrastructure. For example, you \nwant to find out now if your proposed tool cannot communicate with your configu- \nration management system. You may also need to identify changes to your current \ninfrastructure to enable the new 100l(s) to be used effectively. \n6.2.2 Pilot project \nAfter selecting a tool (or tools) and a successful proof-of-concept, the next step is to \nhave a pilot project as the first step in using the tool(s) for real. The pilot project will use \nthe tool in carnest but on a small scale, with sufficient time to explore different ways \nof using the tool. Objectives should be set for the pilot in order to assess whether or \nnot the tool can accomplish what is needed within the current organizational context. \nA pilot tool project should expect to encounter problems. They should be solved \nin ways that can be used by everyone later on. The pilot project should experiment \nwith different ways of using the tool. For example, different reports from a test man- \nagement tool, different scripting and comparison techniques for a test execution tool \nor different load profiles for a performance testing tool. \nThe objectives for a pilot project for a newly acquired tool are: \no To gain in-depth knowledge about the tool (more detail, more ways of using it) \nand to understand more fully both its strengths and its weaknesses. \n@ To see how the tool would fit with existing processes and practices, determining \nhow those would need 1o adapt and change to work well with the tool, and how \n10 use the tool to streamline and improve existing processes. \n® To decide on standard ways of using the tool that will work for all potential \nusers (for example naming conventions for files and tests, ereation of libraries, \ndefining modularity, where different elements will be stored, how they and the \ntool itself will be maintained, and coding standards for test scripts), \noyt N Crmpogs Lo N3 B Bt vl Moy et b o, . o4 B 0 b 18 ot (b 3 o, . ot st ey b smppresmed B e ol snbis Akt s el e A e gyt oot s e Sy o el ey ericncs CEngage L BTG e (W 14 Y adlbnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 236,
            "page_label": "237"
        }
    },
    {
        "page_content": "224 Chapter 6 Tool support for testing \n® To assess whether or not the benefits will be achieved at reasonable cost. \no To understand (and experiment with) metrics that you want the tool(s) to \ncollect and 1o report, and configuring the 10ol(s) to ensure that your goals for \nthese metrics can be achieved. \n6.2.3 Success factors for tools \nSuccess is not guaranteed or automatic when acquiring a testing tool, but many orga- \nnizations have succeeded. After a successful selection process and a pilot project, \ntwo other things are also important to get the greatest benefit from the tools: the way \nin which the tool(s) is deployed within the wider organization, and the way in which \nongoing support is organized. Here are some of the factors that contribute to success: \n@ Rolling out the tool incrementally (after the pilot) to the rest of the organization \n(@ gradual uptake of the tool, not trying to get the whole organization to use it at \nonce immediately). \n® Adapting and improving processes, testware and tool artefacts 1o get the best fit \nand balance between them and the use of the tool. \no Providing adequate support, training (for example for those using the tool \ndirectly), coaching (for example from external specialist automation consultants) \nand mentoring for tool users. \n® Defining and communicating guidelines for the use of the tool, based on what \nwas learned in the pilot (for example internal standards for automation). \no Implementing a way to gather information about the use of the tool, to enable \ncontinuous improvement as tool use spreads through more of the organization, \n® Monitoring the use of the tool and the benefits achieved, and adapting the use of \nthe tool to take account of what is learned. \no Providing continuing support for anyone using test tools, such as the test team \n(for example, technical expertise is needed 1o help non-programmer testers who \nuse keyword-driven testing). \n@ Gathering lessons learned, based on information gathered from all teams \nwho are using test tools. \nAny tools also need to be integrated both technically and organizationally into \nthe software development life cycle. This may involve separate organizations that \nare responsible for different aspects, such as operations and/for third-party suppliers. \nFailure in tool use can come from any one factor: success needs all factors to be \nworking together. \nMore information and advice about experiences in using tools can be found in \nGraham and Fewster [2012] and Gamba and Graham [2018]. \nG N Crmpoge Lossming. A3 Kt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i g, st sty conto ey b sy B e el smbs At el e A e 5 gyt oot s o Sy w8 el g pericncs Cengage Ly BTG e (W 14 Y adiBicnd o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 237,
            "page_label": "238"
        }
    },
    {
        "page_content": "Chapter Review 225 \nCHAPTER REVIEW \nLet’s review what you have learned in this chapter. \nFrom Section 6.1.1, you should now be able to classify different types of test tools \naccording to the test process activities that they support. You should also recognize \nthe tools that may help developers in their testing (shown by (D) below). In addition \nto the list below, you should recognize that there are tools that support specific appli- \ncation areas and that general purpose tools can also be used 1o support testing, The \ntools you should now recognize are: \n‘Tools that support the management of testing and tests: \nTest management tools and application lifecycle management (ALM) tools, \nRequirements management tools, \nDefect management tools. \nConfiguration management tools, \nContinuous integration tools (D), \nTools that support static testing: \nTools that support reviews. \nStatic analysis tools (D). \n“Tools that support test design and implementation: \nTest design tools. \nModel-Based Testing (MBT) wols. \nTest data preparation tools. \nAcceptance test-driven development (ATDD) and behaviour-driven \ndevelopment (BDD) tools. \nTools that support test execution and logging: \nTest execution tools. \nCoverage tools, for example requirements coverage, code coverage (D). \nTest harnesses (D). \nUnit test framework tools (D). \nTools that support performance measurement and dynamic analysis: \nPerformance testing tools. \n@ Monitoring tools. \n® Dynamic analysis tools (D). \nYou should know the Glossary terms performance testing tool, test execution \ntool, and test management tool. \nFrom Section 6.1.2, you should be able o summarize the potential benefits and \npotential risks of test automation. \nFrom Section 6.1.3, you should recognize the special considerations of test execu- \ntion tools and test management tools. \nG N Compoge Lossming. A3 K Bt vt My ek b o, s, o4 b . 0 b 10 . o, U 3 i, g, s s paty conto ey b smpppered B e el smbis AChagat s el s M e s gyt oo s et Sy 4 ool ey pericnce Cengage Ly BTN e (W 14 Y adibicnd o o e o € b 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 238,
            "page_label": "239"
        }
    },
    {
        "page_content": "226 Chapter 6 Tool support for testing \nFrom these two sections, you should know the Glossary terms data-driven \ntesting, keyword-driven testing and test automation. \nFrom Section 6.2.1, you should be able to state the main principles of introducing \na tool into an organization. \nFrom Section 6.2.2, you should be able to state the objectives of a pilot project as \na starting point to introduce a tool into an organization, \nFrom Section 6.2.3, you should recognize that simply acquiring a tool is not the \nonly factor in achieving success in using test tools: there are many other factors that \nare important for success. \nThere are no specific definitions for these three sections. \nG N Compuge Lowming A3 gt Bt My et b i, s, o4 B i, s 18 . o, U 3 i e, ot sty comto ey b gy B e el b At el s M e 0 syt oot s et Sy o ol g pericnce CEngage LR BTG e (W 4 S adiBicnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 239,
            "page_label": "240"
        }
    },
    {
        "page_content": "SAMPLE EXAM QUESTIONS \nQuestion 1 Which twols belp to support static \ntesting? \na. Static analysis tools and test execution tools. \nb. Review tools, static analysis tools and coverage \nmeasurement tools. \n<. Dynamic analysis wols and modelling tools, \nd. Review tools and static analysis tools. \nQuestion 2 Which test activities are supported by \ntest harnesses? \na. Test management and control, \nb. Test design and implementation. \n<. Test execution and logging. \nd. Performance measurement and dynamic analysis. \nQuestion 3 What are the potential benefits from \nusing test automation? \na. Greater quality of code, reduction in the number \nof testers needed, better objectives for testing. \nb. Greater repeatability of tests, reduction in repeti- \ntive work, objective assessment. \n<. Greater responsiveness of users, reduction of tests \nrun, objectives not necessary. \nd. Greater quality of code, reduction in paperwork, \nfewer objections to the tests, \nQuestion 4 Which of the following are advanced \nscripting techniques for test exccution tools” \na. Data-driven and keyword-driven, \nb. Data-driven and capture-driven. \n<. Capture-driven and keyhole-driven. \nd. Playback-driven and keyword-driven. \nSample Exam Questions 227 \nQuestion 5 Which of the following would NOT be \ndone as part of selecting a tool for an organization? \na. Assess organizational maturity, strengths and \nweaknesses. \nb. Roll out the ol to as many users as possible \nwithin the organization. \nc. Evaluate the tool features against clear require- \nments and objective criteria. \nd. Identify internal requirements for coaching and \nmentoring in the use of the tool. \nQuestion 6 Which of the following is a goal \nfor a pilot project for introducing a tool into an \norganization? \na. Decide which tool to acquire. \nb. Decide on the main objectives and requirements \nfor this type of tol. \nc. Evaluate the tool vendor including training, sup- \nport, and commercial aspects. \nd. Decide on standard ways of using, managing, \nstoring and maintaining the tool and the test \nassets, \nQuestion 7 What is a success factor for the use of \ntools within an organization? \na. Implement a way to gather usage information \nfrom the actual use of the tool. \nb. Evaluate how the tool fits with existing processes \nand adopt the tol’s approach. \nc. Roll out the tool to all of the rest of the organiza- \ntion at once, as quickly as possible. \nd. Assess whether the benefits will be achieved at \nreasonable cost. \no N Crmpoge Lossming. A3 K Bt vt My et b o, s, o b, s 18 . o, U 3 ot e, st s sty comto ey b gy B e el smbs At s \nJe——— Conpget e e b et s & cvent barwny s e (4 vy o o o o € g 0 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 240,
            "page_label": "241"
        }
    },
    {
        "page_content": "CHAPTER SEVEN \nISTQB Foundation Exam \n¢ wrote (and re-wrote and re-wrote) this book specifically to cover the Inter- \nnational Software Testing Qualification Board (ISTQB) Foundation Syllabus \n2018. Because of this, mastery of the previous six chapters should ensure that you \nmaster the exam 100, Here are some thoughts on how to make sure you show how \nmuch you know when taking the exam, \n7.1 PREPARING FOR THE EXAM \n7.1.1 Studying for the exam \nWhether you have taken a preparatory course along with reading this book or just \nread the book, we have some study tips for certification candidates who intend \n1o take an exam under one of the ISTQB-recognized National Boards or Exam \nBoards. \nOf course, you should carefully study the Syllabus. If you encounter any state- \nments or concepts you do not completely understand, refer back to this book and w0 \nany course materials used, to prepare for the exam. Exam questions often tum on \nprecise understanding of a concept or the wording. \nWhile you should study the whole Syllabus, scrutinize the learning objectives in \nthe Syllabus one by one. Ask yourself if you have achieved that learning objective to \nthe given level of knowledge. If not, go back and review the appropriate material in \nthe section corresponding to that learning objective. Note that the sections in both the \nSyllabus and the book often have the same numbering, and this is usually the same as \nthe leaming objective in the Syllabus. \nGoing beyond the Syllabus, notice that a number of international standards are \nreferred 1o in the Syllabus, Some exam questions may be about the standards and \nthere will also be questions about Glossary terms, and often even experienced testers \nare unfamiliar with them. \nWe recommend that you try to answer all of the sample exam questions in this \nbook. If you have understood the matenial in a chapter, you should have no trouble \nwith the questions. Make sure you understand why the right answer is the right \nanswer and why the wrong answers are wrong. If you cannot answer 4 question and \ndon't understand why, review that section. \nWe also recommend that you try to do all of the exercises in the book. If you can \ncomplete them correctly, you probably understand the concepts. If you do the exer- \ncises well, you are more likely to pass the exam, \nFinally, be sure to take the mock exam we have provided at the end of this chap- \nter. If you have understood the material in the book, you should have no trouble \n228 \noot N Crmpoge Lonwming A3 Bt Bt vt My et b o, s, o bl . 0 b 18 o U 3 ot g, s sty comra oy b syt B e Pl smbhs et T mview s devmed sy Je——— s & cvent barwny ¥ Conpae Lowmay mures e (g 1wy albacnd comem o iy S € shorqaes i (Farans gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 241,
            "page_label": "242"
        }
    },
    {
        "page_content": "Section 1 Prepanng for the Exam 229 \nwith most, if not all, of the questions on the mock exam. Make sure you carefully \nreview the book sections corresponding to any questions that you haven't answered \ncorrectly, When you take the mock exam, try to simulate exam conditions as much \nas possible. Set aside a full hour to do the exam, and make sure that you are not \ndisturbed during the hour: no interruptions! Turn off email and leave your phone \nin another room. \n7.1.2 The exam and the Syllabus \nThe ISTQB Foundation exam is designed to assess your knowledge and understand- \ning of basic testing ideas and terms, which provides i solid starting point for your \nfuture testing efforts. These exams are not perfect. Even well-qualified candidates \noften fail a few questions or disagree with some answers. Study hard, so you can take \nthe exam in a relaxed and confident way, \nIf you take the exam unprepared, you should expect your lack of preparation to \nshow in your score. Since you will pay to take the exam, consider preparation time \nan investment in protecting your exam fee. \nThe ISTQB Foundation Syllabus 2018 is the current Syllabus, It replaces the 2011 \nversion, which in turn replaced the 2007 version, which in turn replaced the 2005 \nversion. Do not refer to the old Syllabi, as the exam covers the new Syllabus. \n7.1.3 Where should you take the exam? \nIf you have taken an accredited training course, it is quite likely the exam will \noccur on the last day of the course. The exam fee may have been included in the \ncourse fee. If so, you should plan to study each evening during the course, includ- \ning material that will be covered on the last day. Any topic covered in the Syllabus \nmay be in the exam. You might even want to read this book before starting the \ncourse. \nYou might be taking an online course. Again, we recommend that you study the \nmaterials as you cover each section. Consider reading the book during or even before \ntaking the course. \nIf you have studied on your own, without taking a course, then you will need to \nfind a place open to the general public where the exam is offered. Many conferences \naround the world offer the exam this way. Otherwise the website for the testing board \nfor your country (the National Board) should be able 1o help you find the right venue \nfor a public exam. \nIn addition, ISTQB-recognized exam providers such as ASTQB, BCS (British \nComputer Society), Brightest, Cert-IT, Certlnstitute, GASQ, iSQI and Kryterion \n(list adapted from ISTQB website at the time of publication) will provide you with \nISTQB-branded certificates, as they work in close cooperation with the ISTQB and its \nNational Boards. The ISTQB and each ISTQB-recognized National Board and Exam \nBoard are constitutionally obliged to support a single, universally accepted, inter- \nnational qualification scheme. Therefore, you are free 1o choose ISTQB-accredited \ntrainers, if desired, and ISTQB-accredited exam providers based on factors like con- \nvenience, competence, suitability of the training to your immediate professional and \nbusiness needs and price. \nG N Crmpogs Loswming. A3 gt Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st ey b gy B e el snbs At s Je—p—— e e v et P Conpge Lewmay mures e 13 10 ey akoacd o o sy Vo € whacacs A (7O s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 242,
            "page_label": "243"
        }
    },
    {
        "page_content": "230 Chapter 7 I1STQB Foundation Exam \n7.2 TAKING THE EXAM \nAll the National Boards offer the same type of Foundation exam. It is a multiple \nchoice exam that conforms 1o the guidelines established by the ISTQB. The exam in \nthis book also follows the new guidelines from ISTQB for the balance of questions \nfrom the different sections/chapters. For some of us, it's been a while since we last \ntook an exam and you may not remember all the tricks and techniques for taking a \nmultiple choice exam. Here are a few paragraphs that will give you some ideas on \nhow to take the exam. \n7.2.1 How to approach multiple choice questions \nRemember that there are two aspects to correctly answering a multiple choice exam \nquestion, First, you must understand the question and decide what the right answer \nis. Second, you must correctly communicate your answer by selecting the correct \noption from the choices listed. \nEach question has one correct answer. This answer is alwuays or most frequently \ntrue, given the stated context and conditions in the question. The other answers are \nintended to mislead people who do not completely understand the concept and are \ncalled ‘distracters’ in the examination business, \nRead the question carefully and make sure you understand it before you decide on \nyour answer. It may help to highlight or underline the keywords or concepts stated \nin the question. Some questions may be ambiguous or ask which is the best or worst \nalternative. You should choose the best answer based on what you have learned in this \nbook, Remember that your own situation may be different from the most common \nsituation or the ideal situation. \nSome of the distructers may confuse you. In other words, you might be unsure about \nthe correct answers (o some of the questions. So make two passes through the exam. On \nyour first pass, answer all of the questions you are sure of. Come back to the others later. \n1f a question will take a long time 1o answer, come back to it later, when you feel \nconfident that you have enough time. Each correct answer is worth one point, whether \nit takes 30 seconds or § minutes to decide on it. \nIt's important to pace yourself. We suggest that you spend 45-60 seconds on each \nquestion. That way your first pass will take less than 40 minutes. You can then use \nthe remaining time to have a one-minute break, then double-check your answers, and \nthen finally go back to any questions you haven't answered yet. \n1f you use an answer sheet for the exam, they may vary slightly from one National \nBoard or Exam Board to another. You should make sure that each answer corresponds \n1o the correct question number. Especially if you have skipped a question: it is casy \n10 get confused and mark the answer option above or below the question you are \ntrying to answer. \nAlso, make sure you have an answer for every question. Double-check that you \nhave selected the answer you want, as it is easy to select the wrong answer. \n7.2.2 On trick questions \nThere are no deliberately designed trick questions in the ISTQB exams, but some can \nseem quite difficult if you are not completely sure of the right answer. Remember, just \nbecause it is hard to answer does not make it a trick. Here are some ideas for dealing \nwith the tough questions. \nG N Crmpoge Lowming. A3 Kigh Bt vt My et b o, s, o4 A, 0 s 18 . o, (b 3 o g, s sty conto ey b gy B e el b At s e e b s oo st s vl g Conpge Lomay murses e 1y 10 ey koo o o e} o € whacacs A (7 0Tv s gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 243,
            "page_label": "244"
        }
    },
    {
        "page_content": "Section 2 Taking the Exam 231 \nFirst, read the question and each of the options very thoroughly. It is easy to read \nwhat you expect to be there rather than what is there. \n1f you are tempted to change your answer, do so only if you are quite sure. When \nyou are undecided between two options, it’s usually best to go with your first instinet. \nRemember that the correct answer is the one always or most often true. Simply \nbecause you can imagine a circumstance under which an answer might not be true \ndoes not make it wrong, just as knowing a 95-year-old cigarette smoker does not \nprove that cigarette smoking is not risky. \nFor most exams, you are allowed to write on the question paper or make notes on \npaper (which would need to be handed in). In addition, there is typically no penalty \nfor the wrong answer. So feel free to guess! \nWhile the ISTQB guidelines call for straightforward questions, some topics are \ndifficult to cover without some amount of complexity. Be especially careful with \nnegative questions and complex questions. If the question asks which is false (or true), \nmark them T or F first if you can. This will make it easier to see which is the odd \none out. If the question requires matching a list of definitions, you might be able to \ndraw lines between the definitions and the words they define. And remember to use \nthe process of elimination to work for you whenever possible. Use what you know to \ncliminate wrong answers and cross them out up front to avoid mistakes. \nFinally, remember that the ISTQB exam is about the theory of good practice, not \nwhat you typically do in practice. The ISTQB Syllabus was developed by an interna- \ntional working group of testing experts based on the good practices they have seen in \nthe real world. In testing, good practices may lead typical practices by about 20 years’ \ntime. Some organizations are struggling to implement even basic testing principles, \nsuch as removing the misconception that “complete” testing is possible, while other \norganizations may already be doing most of the things discussed in the Syllabus. Still \nother organizations may do things well but differently from what is described in the \nSyllabus. So remember to apply what you have learned in this book to the exam. When \nan exam question calls into conflict what you have kearned here and what you typically \nhave done in the past, rely on what you have learned to answer the exam question. \nAnd then go back to your workplace and put the good ideas that you have leamed into \npractice! \n7.2.3 Last but not least \nISTQB has sample exam papers available to download, as well as the correct answers \nand justifications for those answers. It is well worth working through these exams \nas well! Search under ‘Exams’, “Sample exams', and “Foundation level’ for CTFL \n2018 exams. \nWe wish you good luck with your certification exam and best of success in your \ncareer as a test professional! We stand poised on the brink of great forward progress \nin the field of testing and are happy that you will be a part of it \nG N Crmpogs Lowming A3 gt Bt vt My et b o, s, o4 b s, 0 s 10 . o, U 3 s e, ot sty st ey b gy B e el snbs At v e e s Je—p—— s & cven arwny Conpge Lewmay murses e 1y 10 ey akbacnd comwm o iy Vow € whacacs A (7 OTA s Mt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 244,
            "page_label": "245"
        }
    },
    {
        "page_content": "232 Chapter 7 ISTQB Foundation Exam \n7.3 MOCK EXAM \nIn the real exam, you will have 60 minutes (75 if the exam is not in your first language) to work through 40 ques- \ntions of approximately the same difficulty mix and Syllabus distribution as shown in the following mock exam, \nAfter you have taken this mock exam, check your answers with the answer key. The answers to all exam questions \nin this book are in the section after the Glossary. \nQuestion 1 Assume postal rates for light letiers \nare: \n$0.25 up 1o 10 grams. \n$0.35 up to 50 grams. \n$0.45 up to 75 grams. \n$0.55 up to 100 grams. \nWhich test inputs (in grams) could be selected using \nequivalence partitioning? \na. 0,9, 10,49, 50,74, 75, 99, 100, \nb. 5,35, 65, 95, 115. \ne 0.1, 10, 11, 50, 51, 75, 76, 100, 101, \nd. 5,25,35,45,55. \nQuestion 2 Consider the following statements \nabout testing and debugging and identify which \nare True: \n1. Debugging is a testing activity. \n2. Testers may be involved in debugging and compo- \nnent testing in Agile development. \n3. Testing finds, analyzes and fixes defects. \n4. Debugging executes tests to show failures caused \nby defects. \n5. Checking whether fixes resolved the defects found \nis a form of testing. \na 2and 5. \nb. 2,3and 4. \ne 1 2and 5. \nd. Only 5 is True. \nQuestion 3 Consider the following statements. \nabout the reasons to adapt life cycle models for spe- \ncific projects or products: \n1 Different projects have different goals. \n11 The life cycle model should be adapled to suit \nall types of development within the company for \nconsistency. \n1 Different types of product have different product \nrisks. \nIV Business priorities are different depending on the \ncontext of the project or product. \nV Different test environments may be necessary. \nWhich of the statements are true? \na L Uland IV. \nb. [ Mand IV. \n< 1L IVand V. \ndILIV and V. \nQuestion 4 Which of the following statements \nabout use case testing are True? \nL. A use case actor may be a business user: a use \ncase subject s the system. \nError messages are tested in the main behavioural \nflow of the use case. \nInteractions miy be represented by activity diagrams, \n. Coverage cannot be measured for use case testing. \nInteractions may change the state of the subject. \n1,2and 3. \n1. 4and 5. \n2. 3and 4. \n. 1,3and 5. \n~ \nw \no en e \nQuestion 5 Consider the following list of either \nproduct or project risks: \n1 An incorrect caleulation of fees might short- \nchange the organization, \n11 A vendor might fail to deliver a system component \non time. \nTIA defect might allow hackers to gain administra- \ntive privileges. \nIV A skills gap might occur in a new technology used \nin the system, \nV' A defect-prioritization process might overload the \ndevelopment team. \nG N4 Compoge Lossming. A3 K Bt vt My et b o, s, o b s, 0 s 18 . o, U 3 ot e, s sty comto ey b sy B e el smbs AChapati s el s M v syt oo s et Sy S el g pericncs Cengage L BTN e (W 14 Y ol o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 245,
            "page_label": "246"
        }
    },
    {
        "page_content": "Which of the following statements is true? \na. Lis primarily a product risk and 11, 111, IV and V \nare primarily project risks. \nb. 11 and V are primarily product risks and 1, Il and \nV are primarily project risks. \n<. | and 11l are primarily product risks, while 11, IV \nand V are primarily project risks. \nd. l1land V are primarily product risks, while I, 11 \nand IV are primarily project risks. \nQuestion 6 Consider the following statements \nabout regression tests: \n1 They may usefully be automated if they are well \ndesigned, \n1l They are the same as confirmation tests \n(re-tests). \n111 They are a way to reduce the risk of a change \nhaving an adverse affect elsewhere in the \nsystem. \nIV They are only effective if automated. \nWhich pair of statements is true? \no Tand IL \nb. Fand 11 \n< land 1L \nd. and IV, \nQuestion 7 According to the ISTQB Glossary, \nwhat is a black-box test technique? \na. A sequence of test cases in execution order, and \nany associated actions that may be required to \nset up the initial preconditions and any wrap-up \nactivities post-execution. \nb. A procedure to derive and/or select test cases \nbased on the tester's experience, knowledge and \nintuition. \n<. A procedure to derive and/or select test cases \nbased on an analysis of the specification, either \nfunctional or non-functional, of a component \nor system without reference to its internal \nstructure. \nd. A procedure 1o derive and/or select test cases \nbased on an analysis of the internal structure of a \ncomponent or system. \nSection3 Mock Exam 233 \nQuestion 8 Review the following portion of a \ndefect report, which occurs after the defect 1D, time \nand date, and author. \n1. 1 place any item in the shopping cart. \n2. I place any other (different) item in the shopping cart. \n3. | remove the first item from the shopping cart, but \nleave the second item in the cart. \n4. I click the <Checkout> button. \n5. T expect the system to display the first checkout \nscreen. Instead, it gives the pop-up error message, \n“No items in shopping cart. Click <Okay> to con- \ntinue shopping”. \n6. I click <Okay>. \n7. Texpect the system to return to the main window \nto allow me to continue adding and removing items \nfrom the cart. Instead, the browser terminates. \n8. The failure described in steps 5 and 7 occurred in \ncach of three attempts to perform steps 1,2, 3, 4 \nand 6, \nAssume that no other narrative information is included \nin the report. Which of the following important aspects \nof a good defect report is missing from this report? \na. The steps to reproduce the failure. \nb. The summary. \nc. The check for intermittence. \nd. The use of an objective tone. \nQuestion 9 Which of the tasks below is typically \ndone by a tester (not a test manager)? \n@, Support setting up the defect management system. \nb. Share testing perspectives with other project activ- \nities such as integration planning. \n¢. Design, set up and verify test environment(s). \nd. Initiate the analysis, design, implementation and \nexecution of tests. \nQuestion 10 Consider the following factors: \n1. The test levels and test types being considered. \n2. The best practices implemented in other companies. \n3. The business domain. \n4. Required internal and external standards. \n5. The way in which the previous system was developed. \nG N Crmpogs Lowming. A3 g Bt vt My et b o, s, o4 e i, 0 s 10 . o, U 30 g, s sty conto ey b gy B e el b At s \no Conpaet v e e e Je—p—— s 8 cven oy s e (e 4 e ol o o s o € g 0 (FT s s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 246,
            "page_label": "247"
        }
    },
    {
        "page_content": "234 Chapter 7 ISTQB Foundation Exam \nWhich of these factors may influence the test process \nin context? \na 1, 2andS. \nb. 2,4and 5. \nc L 3andd. \nd. 1, 3and 5. \nQuestion 11 Of the following statements \nabout reviews of specifications, which statement \nis true? \na. Reviews are not generally cost-effective as the \nmeetings are time consuming and require prepara- \ntion and follow up. \nb. There is no need to prepare for or follow up on \nreviews, \n¢. Reviews must be controlled by the author. \n. Reviews are i cost-effective early static test on the \nsystem. \nQuestion 12 Consider the following list of test pro- \ncess activities: \n1 Analysis and design. \n11 Test closure activities. \n111 Evaluating exit criteria and reporting. \nIV Planning and control. \nV Implementation and execution, \n‘Which of the following places these in their logical \nsequence? \na LILTILIV and V. \nb IV LV I and 11, \ne IV.L V. Iland 1L \n& LIV, V. T and 11, \nQuestion 13 Test objectives vary between \nprojects and so should be stated in the test plan. \nWhich one of the following test objectives \nmight conflict with the proper tester \nmindset? \na. Show that the system works before \nwe ship it. \nb. Find as many defects as possible. \nc. Reduce the overall level of product risk. \nd. Prevent defects through early involvement. \nQuestion 14 Which of the following is a test \nmetric? \n. Percentage of planned functionality that has \ncompleted development. \nb. Confirmation test results (pass/fail). \n<. Cost of development, including time and effort, \nd. Effort spent in fixing defects, not including \nconfirmation testing or regression testing, \nQuestion 15 If you are flying with an economy \nticket, there is a possibility that you may get \nupgraded to business class, especially if you hold a \ngold card in the airdine’s frequent flyer programme. \nIf you do not hold a gold card, there is a possibility \nthat you will get bumped off the flight if it is full \nand you check in late. This is shown in Figure 7.1. \nNote that each box (i.c. statement) has been \nnumbered. \nThree tests have already been run: \nTest 1: Gold card holder who gets upgraded to \nbusiness class, \nTest 2: Non-gold card holder who stays in economy. \nTest 3: A person who is bumped from the flight. \n¥ (T s 4?7 N \n; ‘ \ns \n=) = o \nv \nFIGURE 7.1 Control flow diagram for flight checkin \nWhat additional tests would be needed to achieve \n100% decision coverage? \n. A gold cand holder who stays in economy and a non- \ngold card holder who gets upgraded to business class. \nb. A gold card holder and a non-gold card holder \nwho are both upgraded to business class. \no N Crmpoge Lossming. A3 K Bt vt My et b o, s, o b, s 10 . o, U 3 s, e, s sty comtoss ey b gy B e el smbs AChagiti s el e A v s, gyt oot s e Sy S G el g pericncs Cengge Ly BTG e (W 14 Y ol o o o € g 3 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 247,
            "page_label": "248"
        }
    },
    {
        "page_content": "© A gold card holder and a non-gold card holder \nwho both stay in economy class. \nd. A gold card holder who is upgraded to business \nclass and a non-gold card holder who stays in \neconomy class. \nQuestion 16 Consider the following types of \ntools: \nV Test management tools. \nW Static analysis tools. \nX Continuous integration tools, \nY Dynamic analysis tools, \nZ Performance testing tools. \nWhich of the following of these tools is most likely \nto be used by developers? \na W, Xand Y. \nb. V.Y and Z. \n© V. Wand Z. \nd. X, Yand Z, \nQuestion 17 Which of the following statements \nabout error guessing are True? \nP. Error guessing uses a test charter and test session \nsheets. \nQ. Error guessing is based on tester knowledge. \nR. Error guessing always uses a checklist, \nS. Error guessing is a black-box test technique. \nT. Error guessing can be based on mistakes \ndevelopers may make. \na Pand T. \nb. Qand R. \nc QandT. \nd. Qand S. \nQuestion 18 Which of the following is the most \nimportant difference between the metrics-based \napproach and the expert-based approach to test \nestimation? \n4. The metrics-based approach is more accurate than \nthe expert-based approach. \nb. The metrics-based approach uses calculations \nfrom historical data while the expert-based \napproach relies on team wisdom. \nSection 3 Mock Exam 235 \n¢. The metrics-based approach can be used to ver- \nify an estimate created using the expert-based \napproach, but not vice versa. \nd. The expert-based approach takes longer than the \nmetrics-based approach. \nQuestion 19 If the temperature falls below \n18 degrees, the heating is switched on. When the \ntemperature reaches 21 degrees, the heating is \nswitched off. What is the minimum set of test input \nvalues to cover all valid equivalence partitions? \na. 15. 19 and 25 degrees. \nb. 17, 18, 20 and 21 degrees, \nc. 18, 20 and 22 degrees. \nd. 16 and 26 degrees. \nQuestion 20 According to the ISTQB Glossary, \nwhat is coverage? \na. An attribute or combination of attributes that is \nderived from one or more test conditions by using \na test technique that enables the measurement of \nthe thoroughness of the test execution. \nb. The degree to which specified coverage items \nhave been determined to have been exercised by a \ntest suite, expressed as A percentage. \n€. The degree to which specified tests have been run, \ncompared to the full set of tests that could have \nbeen run. \nd. The degree to which tests have been automated, \ncompared to tests that could have been automated. \nQuestion 21 Which of the following statements \nabout static testing is False? \na Reviews are applicable to web pages and user guides. \nb. Static analysis is applicable 10 executing test seripts. \n€, Static analysis can be applied to work products \nwritten in natural language (e.g. English). \nd. Reviews can be applied to security requirements \nand project budgets. \nQuestion 22 Which success factors are required \nfor good tool support within an organization? \na. Acquiring the best tool and ensuring that all tes- \nters use it. \nb. Adapting processes to fit with the use of the tool \nand monitoring tool use and benefits, \nG N Crmpoge Lossming. A3 Kt Bt vt My et b o, s, o4 b . 0 s 10 . o, U 3 s, e, s sty comto rế b sy B n rồnnà smbs At s bl e M e s gyt oo s e Sy S B el g pericnce CEngage Ly BT e (W 14 Y i ee o o o € b ngh e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 248,
            "page_label": "249"
        }
    },
    {
        "page_content": "236 Chapter 7 1STQ8 Foundation Exam \n© Seting ambitious objectives for tool benefits and \naggressive deadlines for achieving them. \nd. Adopting practices from other successful organi- \nzations and ensuring that initial ways of using the \ntool are maintained. \nQuestion 23 Maich the keyword terms to the cor- \nrect definitions: \nx. Something incorrect in a work product, \ny. A person’s mistake. \nZ. An event where the system does NOT perform \ncorrectly, \na. x.is a defect, y. is an error, z. is a failure. \nb. x.is an error, y. is a defect, z. is a failure. \n€. x.1s a failure, y. is an error, z. is a defect. \nd, x.isadefedt, y. Ís a failure, z. is an error. \nQuestion 24 Which of the following would be a \ntypical defect found in component testing? \na. Incorrect sequencing or timing of interface calls. \nb. Incorrect code and logic. \n© Business rules not implemented correctly. \nd. Unhandled or improperly handled communication \nbetween components. \nQuestion 25 Which of the following could be a \nroot cause of a defect in financial software in which \nan incorrect interest rate is calculated? \na. Insufficient funds were available to pay the inter- \nest rate calculated. \nb. Insufficient calculations of compound interest \nwere included. \nc. Insufficient training was given to the developers \nconcerning compound interest calculation rules. \nd. Inaccurate calculators were used to calculate the \nexpected results. \nQuestion 26 Assume postal rates for light \nletters are: \n$0.25 up to 10 grams. \n$0.35 up to 50 grams. \n$0.45 up to 75 grams, \n$0.55 up to 100 grams. \nWhich test inputs (in grams) would be selected using \nboundary value analysis? \na. 0.9, 19, 49, 50, 14, 75, 99, 100, \nb. 10, 50, 75, 100, 250, 1000, \n<. 0, 1,10, 11, 50, 51,75, 76, 100, 101. \nd. 25,26, 35, 36, 45. 46, 55. 56. \nQuestion 27 Consider the following decision table. \nTABLE 7.1 Decision table for car rental \nConditions  Rule 1 Rule2 Rule3 Rule4 \nOver 237 F i T T \nClean drving Dontcare F T T \nrecord? \nOn business? Don‘tcare Don'tcare F T \nActions \nSupply rental i F ì T \ncar? \nPremium F F F T \ncharge? \nGiven this decision table, what is the expected result \nfor the following test cases? \nTCL: A 26-year-old on business but with violations or \naccidents on his driving record. \nTC2: A 62-year-old tourist with a clean driving record. \na. TCL: Do not supply car: \nTC2: Supply car with premium charge. \nb. TCL: Supply car with premium charge: \nTC2: Supply car with no premium charge. \n<. TCL: Do not supply cár, \nTC2: Supply car with no premium charge. \nd. TCI: Supply car with premium charge: \nTC2: Do not supply car. \nQuestion 28 What is exploratory testing? \na. The process of anticipating or guessing where \ndefects might occur. \nb. A systematic approach to identifying specific \nequivalent classes of input. \n<. The testing carried out by a chartered engineer. \nd. Concurrent test design, test execution, test logging \nand learning. \nG N Compoge Lossming. A3 K Bt vt My et b o, s, o4 b, 0 s 18 . o, U 3 ot g, s sty comto rế b gy B n el b At s \nn H €mpert i e b s - S N s e N t N t t N n n N N g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 249,
            "page_label": "250"
        }
    },
    {
        "page_content": "Question 29 Which statement about functional, \nnon-functional and white-box testing is True? \na. Functional testing evaluates characteristics such \nas reliability. security or usability: non-functional \ntesting evaluates characteristics such as system \narchitecture and the thoroughness of testing: \nwhite-box testing evaluates characteristics such as \ncompleteness and correctness. \nb. Functional testing evaluates characteristics such \nas completeness and correctness: non-functional \ntesting evaluates characteristics such as reliability, \nsecurity or usability: white-box testing evaluates \ncharacteristics such as system architecture and the \nthoroughness of testing. \n©. Functional testing evaluates characteristics such \nas completeness and correctness: non-functional \ntesting evaluates characteristics such as system \narchitecture and the thoroughness of testing: \nwhite-box testing evaluates characteristics such as \nreliability, security or usability. \nd. Functional testing evaluates characteristics such \nas system architecture and the thoroughness of \ntesting; non-functional testing evaluates charac- \nteristics such as reliability, security or usability; \nwhite-box testing evaluates characteristics such as \ncompleteness and correctncss. \nQuestion 30 A test plan is written specifically \nto describe a level of testing where the primary \ngoal is establishing confidence in the system. \nWhich of the following is a likely name for this \ndocument? \na Master test plan, \nb. System test plan. \n<. Acceptance test plan. \nd. Project plan. \nQuestion 31 How do experience-based test tech- \nniques differ from black-box test techniques? \na They depend on the tester's understanding of \nthe way the system is structured rather than on a \ndocumented record of what the system should do, \nb. They depend on an individual’s domain knowledge \nand expertise rather than on a documented record \nof what the system should do. \nSection 3 Mock Exam 237 \n¢. They depend on a documented record of what the \nsystem should do rather than on an individual’s \npersonal view, \nd. They depend on having older testers rather than \nyounger testers. \nQuestion 32 Which of the following statements \nare characteristics of static testing. and which are \ncharacteristics of dynamic testing? \n1. Finds defects ín work products directly. \n2. Better for ensuring internal quality (e,g, standards \nare followed). \n3. Finds defects through failures in execution. \n4. Easier and cheaper to find and fix security vulner- \nabilities (e.g. buffer overflow). \n5. Focuses on externally visible behaviours. \na. Static testing: 2 and 4, Dynamic testing: 1, 3 and 5. \nb. Static testing: 3 and 5, Dynamic testing: 1, 2 and 4. \n¢, Static testing: 1, 2 and 4, Dynamic testing: 3 and 5. \nd. Static testing: 1, 2and 3, Dynamic testing: 4 and 5. \nQuestion 33 System test execution on a project is \nplanned for eight weeks. After a week of testing, a \ntester suggests that the test objective stated in the test \nplan of *finding as many defects as possible during \nsystem test” might be more closely met by redirecting \nthe test effort ín what way? \na By asking a selection of users what is most \nimportant for them, and testing that. \nb. By testing the main workflows of the business. \nc. By repeating the unit and integration tests, \nd. By testing in areas where the most defects have \nalready been found. \nQuestion 34 Consider the following activities that \nmight relate to configuration management: \n1 Identify and document the characteristics of a \ntest item, \n11 Control changes to the characteristics of a \ntest item. \n11I Check a test item for defects introduced by a change. \n1V Record and report the status of changes to test items. \nV Confirm that changes to a test item fixed a defect. \nG N Crmpogs Lowming. A3 K Bt vt My et b o, s, o4 A . 0 s 10 . o, U 3 i e, rn sty comto rế b n ee B y el smbs At s \nm €mpert v e e e H s 8 cven oy s e (4 t đ o o € N N e",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 250,
            "page_label": "251"
        }
    },
    {
        "page_content": "238 Chapter 7 1STQ8 Foundation Exam \nWhich of the following statements is True? \na. Only [ is a configuration management task. \nb. All are configuration management tasks. \n© 1, ITand H] are configuration management tasks, \nd. 1, I and IV are configuration management tasks. \nQuestion 35 Consider the following state transi- \ntion diagram. \nFIGURE 7.2 State transition diagram \nGiven this diagram, which test case below covers \nevery valid transition? \na SS-SI-S2-S4-SI~-S1-ES \nb. SS-SI-S2-S3-S4-S3-S4-ES \nc. S§S-SI-S2-S4-SI-S3-S4-SI-S3-ES \nd. S§S~-SI~-S4=S2-SI~-S3~-ES \nQuestion 36 \nYou are the test manager for a user acceptance test \nproject. You have decided to use a requirements-based \nand risk-based test strategy. Test suites have been \ndesigned to cover all of the requirements. \nTable 7.2 (below) shows the priority and the \ndesigned test suites associated with the requirements \n(a lower priority number means higher risk). \nThe table also shows the configuration of the \ntest environment in which the requirements can be \ntested. Thus two different configurations, ABC and \nXYZ, are required to execute all test suites. \nAssume that the test environment is delivered ini- \ntially t the test team under the configuration ABC; \nXYZ will be available from the start of week 4. Each \ntest suite will take approximately 1 week to execute. \nThe execution of TS4 depends on TS3 to be exe- \ncuted before, and the execution of TS2 depends on \nTSI to be executed before. \nBased only on the given information, which of \nthe following test execution schedules would be pre- \nferred given the constraints of the test environment \nand the test strategy chosen? \na TS5, TS6, TS4, TS3, TSI, TS2, TST. \nb. TS5, TS4, TSI, TS6, TS3, TS2, TS7. \n<. TS5, TS6, TSI, TS3, TS4, TS2. TS7. \n. TS5, TS6, TSI, TS2, TS7, TS4, TS3, \nTABLE 7.2 Priority and dependency table for Question 36 \nRequirement  Priority  Test Suites  Environment  Dependency \nLAI 4 ™ \nR2 4 152 \nR3 3 153 \nR4 2 154 \nRS 1 T55, 156 \nR6 5 157 \n5 \nš k& R KK \nG N Crmpuge 129926 A3 Kin Bt vt My et b o, s, o4 b . 0 s 18 . o, U 3 st e, ot s sty scmto rế b E ee B y el snbs /Ch N \n€mpert i e b s n st - G N T e (0 0 đ 3 3 € N 3 33 G",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 251,
            "page_label": "252"
        }
    },
    {
        "page_content": "Question 37 What is the most important factor for \nsuccessful performance of reviews? \n#. A separate scribe during the logging meeting. \nb. Trained participants and review leaders. \n© The availability of tools to support the review \nprocess, \nd. A reviewed test plan. \nQuestion 38 How is impact analysis used in main- \ntenance testing? \n. H evaluates intended consequences and possible \nside effects of a change to the system, in order to \nplan what testing to do. \nb. It evaluates the consequences of an intended \nchange on the test environment. \n< It evaluates the ease of maintaining the system \nwhen there are changes to the system or to the tests. \nd. It evaluates the existing regression tests to assess \nwhether a planned change to the system should go \nahead. \nQuestion 39 When you sign up, you must give \nyour first and last name, address and postcode or zip \ncode, your mobile/cell phone number, your email \naddress and set yourself a password. \nWhen you log in, you must give the following \ndetails: last name, phone number and password. \nYou are logged in until you select ‘Logout” followed \nby answering *Yes' to 'Are you sure?' \nYou can update your details once you are logged \nin, but you need to confirm the change by entering \nSection 3 Mock Exam 239 \na code sent to your phone. You then need to log in \nagain. \nIn reviewing the specification above, which of the \nfollowing are potential defects likely to be found by a \nuser perspective review? \n1, Cannot log back in Íf not logged yourself out (e.g. \nwhen changing details). \n2. Incorrect format of the code sent to the phone. \n3. Buffer overflow for a postal address that is too long. \n4. Cannot change the phone number, as the code is \nsent to the old phone. \n5. When details are changed, they create a separate \nnew record in the database. \na. land2 \nb. 3and 5, \n¢ landd, \nd. 4 and 5. \nQuestion 40 Which of the following is an advan- \ntage of independent testing? \na. Independent testers do NOT have to spend time \ncommunicating with the project team, \nb. Developers can stop worrying about the quality of \ntheir work and focus on producing more code. \n€, The others on a project can pressure the indepen- \ndent testers to accelerate testing at the end of the \nschedule. \nd. Independent testers sometimes question the \nassumptions behind requirements, designs and \nimplementations. \nG N Crmpoge Lossming. A3 K Bt vt My et b o, s, o4 b i, 0 s 10 . o, U 3 s, g, s sty comtos rế b sy B n el smbs At s bl s M e gyt oo s o Sy 4 B el g ericncs CEngage L BTG e (W 14 s n kh ee o o o € g 30 e n g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 252,
            "page_label": "253"
        }
    },
    {
        "page_content": "",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 253,
            "page_label": "254"
        }
    },
    {
        "page_content": "GLOSSARY \nThis glossary provides the complete definition set of software testing terms, as defined by ISTQB. \nThe glossary has been arranged in a single section of definitions ordered alphabetically. Some terms are \npreferred to other synonymous ones, in which case the definition of the preferred term appears, with the synonyms \nlisted afterwards, For example, a synonym of white-box festing is structural resting. \n‘See also` cross-references are also used. They assist the user to quickly navigate to the right index term. ‘See \naÌso' cross-references are constructed for relationships such as broader term to a narrower term and overlapping \nmeanings between two terms, \nFinally. note that the terms that are underlined are those that are specifically mentioned as keywords in the \nSyllabus at the beginning of Chapters | to 6, These are the terms that you should know for the exam. \nAcceptance criteria Thecriteria that a component \nor system must satisly in order to be accepted by a \nuser, customer or other authorized entity. \nAcceptance testing  Formal testing with respect to \nuser needs, requirements, and business processes \nconducted to determine whether or not a system \nsatisfies the acceptance criteria and to enable \nthe user, customers or other authorized entity to \ndetermine whether or not to accept the system. \nSee also: user acceptance testing. \nAccessibility The degree to which a component \nor system can be used by people with the widest \nrange of characteristics and capabilities to achieve \na specified goal in a specified context of use, \nAccessibility testing  Testing to determine the \ncase by which users with disabilities can use a \ncomponent of system, \nActual result The behavior produced/observed \nwhen a component or system is tested. \nSynonym: actual outcome \nAd hoc reviewing Areview technique carried out \nby independent reviewers informally, without a \nstructured process. \nAlpha testing  Simulated or actual operational testing \nconducted in the developer’s test environment, by \nroles outside the development organization, \nAnomaly _ Any condition that deviates from \nexpectation based on requirements specifications, \ndesign documents, user documents, standards, \nete.. or from someone's perception or experience. \nAnomalies may be found during, but not limited \n1o, reviewing, testing, analysis, compilation, or use \nof software products or applicable documentation. \nAudit  An independent cxamination of a work \nproduct, process or set of processes that is \nperformed by a third party to assess compliance \nwith specifications, standards, contractual \nagreements or other criteria. \nAvailability The degree to which a component or \nsystem is operational and accessible when required \nfor use. \nBehavior (behaviour) Theresponse Of a \ncomponent or system to a set of input values and \npreconditions. \nBeta testing  Simulated or actual operational testing \nconducted at an external site, by roles outside the \ndevelopment organization. \nSynonym: field testing \nBlack-box test technique A procedure to derive \nand/or select test cases based on an analysis of the \nspecification, either functional or non-functional, \nof a component or system without reference to its \ninternal structure. \nSynonyms: black-box technique, specification- \nbased technique, specification-based test technique \nBoundary value A minimum or maximum value of \nan ordered equivalence parttion. \nBoundary value analysis A black-box test \ntechnique in which test cases are designed \nbased on boundary values. See also: boundary \nvalue. \nBurndown chart A publicly displayed chart \nthat depicts the outstanding effort versus time \nin an iteration. It shows the status and trend of \ncompleting the tasks of the iteration. The X-axis \n241 \nyy N Crmpogs Lowming. A3 Kiện Bt vt My et b copi, s, o4 Al n 0 s 18 . o, U 3 o, g, s sty comtr rế b gy B y el b At \nm €mpert v e e e n s et - G N s e (e t t t N € N N g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 254,
            "page_label": "255"
        }
    },
    {
        "page_content": "242 Glossary \ntypically represents days in the sprint, while the \nY-axis Ís the remaining effort (usually cither in \nideal engineering hours or story points). \nChecklist-based reviewing A review technigue \nguided by a list of questions or required attributes.. \nAn experience-based test \ntechnique whereby the experienced tester uses a \nhigh-level list of items to be noted, checked, or \nremembered, or a set of rules or criteria against \nwhich a product has to be verified. \nCode coverage An analysis method that \ndetermines which parts of the software have \nbeen executed (covered) by the test suite and \nwhich parts have not been executed, for example, \nstatement coverage, decision coverage or condition \ncoverage, \nCommercial off-the-shelf (COTS) A software \nproduct that is developed for the general market, \ni.e. for a large number of customers, and that is \ndelivered to many customers in identical format. \nSynonym: off-the-shelf software \nCompatibility The degree to which a component \nor system can exchange information with other \ncomponents or systems. \nComplexity The degree to which a component \nor system has a design andfor internal structure \nthat is difficult to understand, maintain and \nverify. \nCompliance “The capability of the software product \nto adhere to standards, conventions or regulations \nin laws and similar prescriptions, \nComponent A minimal part of a system that can be \ntested in isolation. \nSynonyms: module, unit \nComponentintegrationtesting Tesling performed \nto expose defects in the interfaces and interactions \nbetween integraled components. \nSynonym: link testing \nComponentspecification A description of a \ncomponent’s function in terms of its output \nvalues for specified input values under specified \nconditions, and required non-functional behavior \n(for example, resource utilization). \nComponent testing Thetesting of individual \nhardware or software componerts, \nSynonyms: module testing, unit testing \nCondition A logical expression that can be \nevaluated as True or False, for example, A>B. \nSynonym: branch condition \nConfiguration “The composition of a component \nor system as defined by the number, nature and \ninterconnections of ils constituent parts, \nConfiguration item  An aggregation Of work \nproducts that is designated for configuration \nmanagement and treated as a single entity in the \nconfiguration management process. \nConfiguration management A discipline \napplying technical and administrative direction \nand surveillance to identify and document \nthe functional and physical characteristics \nof a configuration item, control changes to \nthose characteristics, record and report change \nprocessing and implementation status, and verify \ncompliance with specified requirements, \nConfiguration management tool - A tool that \nprovides support for the identification and \ncontrol of configuration items, their status over \nchanges and versions, and the release of baselines \nconsisting of configuration items, \nDynamic testing conducted \nafter fixing defects with the objective to confirm \nthat failures caused by those defects do not occur \nanymore. \nSynonym: re-testing \nContractual acceptance testing Acceptancetesting \nconducted to verify whether a system salisfies its \ncontractual requirements. \nControl flow “The sequence in which operations are \nperformed during the execution of a test item. \nCost of quality The total costs incurred on quality \nactivities and issues and often split into prevention \ncosts, appraisal costs, internal failure costs and \nexternal failure costs. \nCoverage The degree to which specified coverage \nitems have been determined to have been exercised \nby a test suite expressed as a percentage, \nSynonym: test coverage \nCoverage item  An attribute or combination of \nattributes that is derived from one or more test \nconditions by using a test technique that enables \nthe measurement of the thoroughness of the test \nexecution. \nCoverage ol - A toôi that provides objective measures \nof what structural elements, for example, statements, \nbranches have been exercised by a test suite. \nSynonym: coverage measurement tool \nData flow  An abstract representation of the \nsequence and possible changes of the state of \nG N Crmpogs Lowming. A3 K Bt vt My et b o, s, o4 e . 0 s 18 . o, U 3 st e, rn sty comto ey b gy B n el b At \nH €mpert n a n - G u s e (e t đ N t N € n N N U",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 255,
            "page_label": "256"
        }
    },
    {
        "page_content": "data objects, where the state of an object is any of \ncreation, usage or destruction. \nData-driven testing A scripting technique that \nstores test input and expected results in a table \nor spreadsheet, so that a single control script can \nexecute all of the tests in the table. Data-driven \ntesting is often used to support the application of \ntest execution tools such as capture/playback tools, \nSee also: keyword-driven testing. \nDebugging The process of finding, analyzing and \nremoving the causes of failures in software. \nDecision - A type of statement in which a choice \nbetween two or more possible outcomes controls \nwhich set of actions will result. \nDecision covernge The coverage of decision \noutcomes. \nDecision outcome “The result of a decision that \ndetermines the next statement 10 be executed. \nDecision table A table used to show sets of \nconditions and the actions resulting from them. \nSynonym: cause-effect decision table \nDecision table testing A black-box test techniguce \nin which test cases are designed to execute the \ncombinations of inputs and/or stimuli (causes) \nshown ín a decision table, See also: decision table, \nDecision testing A white-box test technique in \nwhich test cases are designed to execute decision \noutcomes. \nDefect An imperfection or deficiency in a work \nproduct where it does not meet its requirements or \nspecifications. \nSynonyms: bug, fault \nDefect density The number of defects per unit size \nof u work product, \nSynonym: fault density \nDefect management  The process of recognizing \nand recording defects, classifying them, \ninvestigating them, taking action to resolve them \nand disposing of them when resolved. \nDefect management tool A tool that facilitates the \nrecording and status tracking of defects. \nSynonyms: bug tracking tool, defect tracking tool \nDefect report  Documentation of the occurrence, \nnature and status of a defect. See alsoc incident \nreport. \nSynonym: bug report \nDriver A software component or test tool that \nreplaces a component that takes care of the control \nand/or the calling of a component or system. \nSynonym: test driver \nGlossary 243 \nDynamic analysis The process of evaluating \nbehavior, for example, memory performance, CPU \nUsage, Of a system or component during execution. \nDynamic analysis tool A tool that provides run- \ntime information on the state of the software code. \nThese tools are most commonly used to identify \nunassigned pointers, check pointer arithmetic and \n1o monitor the allocation, use and de-allocation of \nmemory and to flag memory leaks. \nDynamic testing Testing that involves the execution \nof the software of a component Of system. \nEffectiveness  Extent to which correct and complete \ngoals are achieved. See also: efficiency. \nEfficiency Resources expended in relation to the \nextent with which users achieve specified goals. \nSee also: effectiveness. \nEntry criteria The set of conditions for officially \nstarting a defined task. \nSynonym: definition of ready \nEquivalence partition A portion of the value \ndomain of a data element related to the test object \nfor which all values are expected to be treated the \nsame based on the specification. \nSynonym: equivalence class \nEquivalence partitioning A black-box test \ntechnique in which test cases are designed to \nexercise equivalence partitions by using one \nrepresentative member of cach partition. \nSynonym: partition testing \nError A human action that produces an incorrect \nresult. \nSynonym: mistake \nError guessing A testtechnique in which tests \nare derived on the basis of the tester's knowledge \nof past failures, or general knowledge of failure \nmodes. \nExecutable statement A statement which, when \ncompiled, is translated into object code, and \nwhich will be executed procedurally when the \nprogram is running and may perform an action \non data. \nExercised A program clement is said to be \nexercised by ạ test case when the input value \ncauses the execution of that element, such as a \nstatement, decision or other structural element, \nExhaustive testing A test approach in which the \ntest suite comprises all combinations of input \nvalues and preconditions. \nSynonym: complete testing \nG N Compogs Lowming. A3 K Bt vt My et b o, . o4 e n 0 s 18 . o, U 3 st e, s sty comto rế b gy B y el snbs /Ch v Ll e i e 8y appresact oo dres o vy o8 e ID IƯ SỰ Liwmay BuTCs e NI N Ng o o 1 € U N YA G s g Hữ",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 256,
            "page_label": "257"
        }
    },
    {
        "page_content": "244 Glossary \nExitcriteria The set of conditions for officially \ncompleting a defined task. \nSynonyms: completion criteria, test completion \ncriteria, definition of done \nExpected result The predicted observable behavior \nof a component or system executing under \nspecified conditions, based on its specification or \nanother source, \nSynonyms: expected outcome, predicted outcome \nA procedure 10 \nderive and/or select test cases based on the tester’s \nexperience, knowledge and intuition. \nSynonym: experience-based technique \nExperience-based testing Testing based on the \ntester's experience, knowledge and intuition. \nExploratory testing An approach to testing \nwhereby the testers dynamically design and \nexecute tests based on their knowledge, exploration \nof the test item and the results of previous tests. \nExtreme Programming (XP) A software \nengineering methodology used within Agile \nsoftware development where core practices are \nprogramming in pairs, doing extensive code \nreview, unit testing of all code. and simplicity \nand clarity in code. See also: Agile software \ndevelopment. \nFacilitator The leader and main person responsible \nfor an inspection or review process, See also: \nmaoderator. \nFail A testis deemed to fail ¡f its actual result does \nnot match its expected result. \nFailure An cvent in which a component or system \ndoes not perform a required function within \nspecified limits. \nFailure rate The ratio of the number of failures of a \ngiven category 10 a given unit Of measure. \nFeature An attribute of a component or \nsystem specified or implied by requirements \ndocumentation (for example, reliability, usability or \ndesign constraints). \nSynonym: software feature \nFinding A result of an evaluation that identifies \nsome important issue, problem or opportunity. \nFormal review A form of review that follows a \ndefined process with a formally documented \noutput. \nFunctional integration  An integration approach \nthat combines the components or systems for the \npurpose of getting a basic functionality working \ncarly. See also: integration testing. \nFunctional requirement - A requirement that \nspecifies a function that a component or system \nmust be able to perform. \nFunctional suitability The degree to which a \ncomponent or system provides functions that \nmeet stated and implied needs when used under \nspecified conditions. \nSynonym: functionality \nFunctional testing Testing conducted to evaluate \nthe compliance of a component or system with \nfunctional requirements, See also: black-box rest \ntechnique. \nGUI  Acronym for Graphical User Interface, \nHigh-level test case A test case without concrete \nvalues for input data and expected results. See also: \nlonw-level 1est case. \nSynonyms: abstract test case, logical test case \nIDEAL An organizational improvement model that \nserves as a roadmap for initiating, planning and \nimplementing improvement actions. The IDEAL \nmodel is named for the five phases it describes: \ninitiating, điagnosing, establishing, acting and \nlearning. \nImpactanalysis “The identification of all work \nproducts affected by a change, including an \nestimate of the resources needed to accomplish the \nchange, \nIncident report Documemtation of the occurrence, \nnature and status of an incident, \nSynonyms: deviation report, software test incident \nreport, test incident report. See also: defect report \nIncremental development model A development \nlife cycle model in which the project scope is \ngenerally determined early in the project life \ncycle, but time and cost estimates are routinely \nmodified as the project team understanding of \nthe product increases. The product is developed \nthrough a series of repeated cycles, cach delivering \nan increment which successively adds to the \nfunctionality of the product. See also: iterative \ndevelopment model. \nIndependence of testing  Separation of \nresponsibilities, which encourages the \naccomplishment of objective testing. \nG N Crmpogs Lowming. A3 Khện Bt vt My et b o, s, o4 e i, 0 s 10 . o, U 3 i, g, rn sty comtr rế b gyt B n el snbs At \nH €mpert n a n s 8 ree oy s e (e t t o N € n N g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 257,
            "page_label": "258"
        }
    },
    {
        "page_content": "Informal group review - An informal review \nperformed by three or more persons. See also: \ninformal review. \nInformal review A type of review without a formal \n(documented) procedure. \nInput Daia received by a component or system from \nan external source. \nInspection A type of formal review to identify \nissues in a work product, which provides \nmeasurement to improve the review process and \nthe software development process. \nInstallation guide Supplicd instructions on \nany suitable media, which guides the installer \nthrough the installation process. This may \nbe a manual guide, step-by-step procedure, \ninstallation wizard or any other similar process \ndescription. \nIntegration The process of combining components \nor systems into larger assemblics, \nTesting performed to expose \ndefects in the interfaces and in the interactions \nbetween integrated components or systems. \nee alsoc component integration lesting, system \nintegration testing. \nInteroperability “The degree to which two or \nmore components or systems can exchange \ninformation and use the information that has \nbeen exchanged. \nInteroperability testing Testing to determine the \ninteroperability of a software product. See also: \nJunctionality testing. \nSynonym: compatibility testing \nIterative development model A development life \ncycle where a project is broken into a usually large \nnumber of iterations. An iteration is a complete \ndevelopment loop resulting ín a release (internal \nor external) of an executable product, a subset of \nthe final product under development, which grows \nfrom iteration to iteration to become the final \nproduct. \nA scripting technigue \nthat uses data files to contain not only test data \nand expected results but also keywords related to \nthe application being tested. The keywords are \ninterpreted by special supporting scripts that are \ncalled by the control script for the test. See also: \ndata-driven testing. \nSynonym: action word-driven testing \nGlossary 245 \nLife cycle model A description of the processes, \nworkflows and activities used ín the development, \ndelivery, maintenance and retirement of a system, \nSee also software life cvcle. \nLoad testing A type of performance testing \nconducted to evaluate the behavior of a \ncomponent or system under varying loads, usually \nbetween anticipated conditions of low, typical and \npeak usage. See also: performance testing, stress \nlesting. \nLow-level test case A test case with concrete values \nfor input data and expected results. See also: frigh- \nlevel test case. \nSynonym: concrete test case \nMaintainability The degree to which a component \nor system can be modified by the intended \nmaintainers. \nMaintenance The process of modifying a \ncomponent or system after delivery to cogrect \ndefects, improve quality attributes or adapt to a \nchanged environment. \nMaintenance testing Testing the changes to an \noperational system or the impact of a changed \nenvironment to an operational system. \nMaster test plan - A test plan that is used to \ncoordinate multiple test levels or test types. \nSee also: test plan. \nMaturity (1) The capability of an organization \nwith respect to the effectiveness and efficiency of \nits processes and work practices. (2) The degree \n1o which a component or system meets needs for \nreliability under normal operation. \nMeasure The number or category assigned to an \nattribute of an entity by making a measurement. \nMeasurement  The process of assigning a number \no category to an entity to describe an attribute of \nthat entity. \nMemory leak A memory access failure due to a \ndefect in a program's dynamic store allocation \nlogic that causes it to fail to release memory after \nit has finished using it, eventually causing the \nprogram and/or other concurrent processes to fail \ndue to lack of memory. \nMetric A measurement scale and the method used \nfor measurement. \nMilestone A point in time in a project at which \ndefined (intermediate) deliverables and results \nshould be ready. \nG N Crmpogs Lowming. A3 K Bt vt My ek b o, s, o4 e i, 0 s 18 . o, U 3 ot g, s sty comto rế b gy B n el b At n s et €mpert n a n - G N s e (e t đ N t N € N N U",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 258,
            "page_label": "259"
        }
    },
    {
        "page_content": "246 Glossary \nModel-based testing (MBT) Testing based on or \ninvolving models. \nModerator - A neutral person who conducts a \nusability test session. See also: facilitator. \nSynonym: inspection leader \nMonitoring tool A software tool or hardware \ndevice that runs concurrently with the component \nor system under test and supervises, records andfor \nanalyzes the behavior of the component or system. \nSee also: dynamic analysis tool, \nNon-functional requirement A requirement that \ndescribes how well the component or system will \ndo what it is intended to do. \nNon-functional testing Testing conducted 1o \nevaluate the compliance of a component or system \nwith non-functional requirements, \nOperational acceptance testing  Operational \ntesting in the acceptance test phase, typically \nperformed in a (simulated) operational \nenvironment by operations \nand/or systems administration staff focusing on \noperational aspects, for example, recoverability, \nresource-behavior, installability and technical \ncompliance. \nSee also: aperational testing, \nSynonym: production acceptance testing \nOperational environment  Hardware and software \nproducts installed at users” or customers’ sites \nwhere the component or system under test will \nbe used. The software may include operating \nsystems, database management systems and other \napplications. \nOutput  Data transmitted by a component or system \nto an external destination. \nPass Á test is deemed to pass if its actual result \nmatches its expected result, \nPath A sequence of events, for example, executable \nstatements, of a component or system from an \nentry point to an exit point. \nSynonym: control flow path \nPeer review A form of review of work products \nperformed by others qualified to do the same work. \nPerformance efficiency The degree tọ which a \ncomponent of system uses time, resources and \ncapacity when accomplishing its designated \nfunctions. \nSynonyms: time behavior, performance \nG N Crmpoge Lowming. A3 K Bt vt My et b o, s, o4 e, s 18 . o, (b 3 i e, rn sty st rế b gy B y el b At el e M e gyl oot s e Sy o el g pericncs Cengage Liwing TGS e (W 14 Y adlbmnd o o o € g 30 (s g \nPerformance indicator A high-level metric of \neffectiveness and/or efficiency used to guide and \ncontrol progressive development, for example, \nlead-time slip for software development. \nSynonym: key performance indicator \nPerformance testing Testing to determine the \nperformance of a software product. \nPerformance testing tool - A test tool that generates \nload for a designated test item and that measures \nand records its performance during test execution. \nA review technique \nwhereby reviewers evaluate the work product \nfrom different viewpoints. See also role-based \nreviewing. \nPlanning poker A consensus-based estimation \ntechnigue, mostly used to estimate effort or relative \nsize of user stories in Agile software development, \nIt is a variation of the Wideband Delphi method \nusing a deck of cards with values representing the \nunits in which the team estimates, See also: Agile \nsoftware development, Wideband Delphi. \nPortability Thecase with which the software \nproduct can be transferred from one hardware or \nsoftware environment to another. \nPortability testing Testing to determine the \nportability of a software product. \nSynonym: configuration testing \nPostcondition “The expected state Of a test item \nand its environment at the end of test case \nexecution, \nPrecondition The required state of a test item and \nits environment prior to test case execution. \nPriority The level of (business) importance \nassigned to an item, for example, a defect, \nProbe effect The effect on the component or \nsystem by the measurement instrument when \nthe component oc system is being measured, for \nexample, by a performance testing tool or monitor. \nFor example, performance may be slightly worse \nwhen performance testing tools are being used. \nProblem  An unknown underlying cause of one or \nmore incidents. \nProcess A sct of interrelated activitics, which \ntransform inputs into outputs. \nProcess improvement A program of activities \ndesigned to improve the performance and maturity \nOf the organization's processes, and the result of \nsuch a program. \nProduct risk A risk impacting the quality of a \nproduct. See also: risk.",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 259,
            "page_label": "260"
        }
    },
    {
        "page_content": "Project A project is a unique set of coordinated \nand controlled activities with start and finish dates \nundertaken to achieve an objective conforming to \nspecific requirements, including the constraints of \ntime, cost and resources. \nProjectrisk Arisk that impacts project success. \nSee also: risk. \nQuality The degree to which a component, system \nor process meets specified requirements and/or \nuser/customer needs and expectations. \nQuality assurance Part of quality management \nfocused on providing confidence that quality \nrequirements will be fulfilled. \nQuality characteristic A category of product \nattributes that bears on quality. \nSynonyms: software product characteristic, \nsoftware quality characteristic, quality attribute \nQuality control The operational techniques and \nactivities, part of quality management, that are \nfocused on fulfilling quality requirements. \nQuality management  Coordinated activities to \ndirect and control an organization with regard \nto quality. Direction and control with regard to \nquality generally includes the establishment of \nthe quality policy and quality objectives, quality \nplanning, quality control, quality assurance and \nquality improvement. \nQuality risk A product risk related to a quality \ncharacteristic, See also: quality characteristic, \nproduct risk. \nRational Unified Process (RUP) A proprictary \nadaptable iterative software development process \nframework consisting of four project life cycle \nphases: inception, claboration, construction and \ntransition. \nRegression A degradation in the quality of a \ncomponent or system due to a change. \nRegression testing Testing of a previously tested \ncomponent or system following modification to \nensure that defects have not been introduced or \nhave been uncovered in unchanged areas of the \nsoftware as a result of the changes made, \nRegulatory acceptance testing  Acceptance testing \nconducted to verify whether a system conforms to \nrelevant laws, policies and regulations. \nReliability The degree to which a component \nor system performs specified functions under \nspecified conditions for a specified period of time. \nGlossary 247 \nReliability growth model A model that shows \nthe growth in reliability over time during \ncontinuous testing of a component or system as \na result of the removal of defects that result in \nreliability failures. \nRequirement A provision that contains criteria to \nbe fulfilled. \nRequirements management tool -. A toôi \nthat supports the recording of requirements, \nrequirements attributes (for example, priority. \nknowledge responsible) and annotation, \nand facilitates traceability through layers \nof requirements and requirements change \nmanagement. Some requirements management \ntools also provide facilities for static analysis, such \nas consistency checking and violations to pre- \ndefined requirements rules, \nResult  The consequence/outcome of the execution \nof a test. It includes outputs to screens, changes to \ndata, reports and communication messages sent \nout. See also: actual result, expected result. \nSynonyms: outcosne, test outcome, test result \nRetrospective meeting A meeting at the end of a \nproject during which the project team members \nevaluate the project and learn lessons that can be \napplied to the next project. \nSynonym: post-project meeting \nReview A type of static testing during which a \nwork product or process is evaluated by one or \nmore individuals to detect issues and to provide \nimprovements. \nReview plan - A document describing the approach, \nresources and schedule of intended review \nactivities. It identifies, amongst others: documents \nand code to be reviewed, review types to b used, \nparticipants, as well as entry and exit criteria to be \napplied in case of formal reviews, and the rationale \nfor their choice. It ís a record of the review \nplanning process. \nReviewer A participant in a review, who identifies \nissues in the work product, \nSynonyms: checker, inspector \nRisk A factor that could result in future negative \nconsequences. \nRisk analysis The overall process of risk \nidentification and risk assessment. \nRisk-based testing Testingin which the \nmanagement, selection, prioritization and use \nof testing activities and resources are based on \ncorresponding risk types and risk levels. \nyy N Crmpogs Loswming. A3 K Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st rế b n ee B y el snbs At s n H €mpert n a n s e ree Vnse: H T t đ t t o € N N U",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 260,
            "page_label": "261"
        }
    },
    {
        "page_content": "248 Glossary \nRisklevel The qualitative or quantitative measure \nof a risk defined by impact and likelibood. \nSynonym: risk exposure \nRisk management The coordinated activities to \ndirect and control an organization with regard to risk, \nRisk mitigation The process through which \ndecisions are reached and protective measures are \nimplemented for reducing or maintaining risks to \nspecified levels. Synonym: risk control \nRisk type A set of risks grouped by one or more \ncommon factors. \nSynonym: risk category \nRobustness The degree to which a component oc \nsystem can function correctly in the presence of \ninvalid inputs or stressful environmental conditions. \nSee alsoc error-tolerance, fault-tolerance, \nRole-based reviewing A review technique where \nreviewers evaluate a work product from the \nperspective of different stakeholder roles. \nRootcause A source of a defect such that Íf it is \nremoved. the occurrence of the defect type is \ndecreased or removed, \nRoot cause analysis An analysis technique aimed \nat identifying the root causes of defects. By \ndirecting corrective measures at root causes, it is \nhoped that the likelihood of defect recurrence will \nbe minimized. \nSynonym: causal analysis \nSafety The capability that a system will not, \nunder defined conditions, lead to a state ín which \nhuman life, health, property or the environment is \nendangered. \nScenario-based reviewing A review technique \nwhere the review is guided by determining the \nability of the work product to address specific \nscenarios. \nScribe A person who records information during \nthe review meetings. \nSynonym: recorder \nScrum - An iterative incremental framework for \nmanaging projects commonly used with Agile \nsoftware development. See also: Agile software \ndevelopment. \nSecurity Thedcgrec to which a component or \nsystem protects information and data so that \npersons or other components or systems have the \ndegree of access appropriate to their types and \nlevels of authorization. \nSecurity testing  Testing to determine the security \nof the software product. See also: functionality \ntesting, \nA type of \ndevelopment life cycle model ín which a complete \nsystem is developed in a linear way of several \ndiscrete and successive phases with no overlap \nbetween them, \nSession-based testing  An approach to testing in \nwhich test activities are planned as uninterrupted \nsessions of test design and execution, often used in \nconjunction with exploratory testing, \nSeverity The degree of impact that a defect has on \nthe development or operation of a component or \nsystem. \nSimulation The representation of selected \nbehavioral characteristics of one physical or \nabstract system by another system. \nSimulator A device, computer program or system \nused during testing, which behaves or operates \nlike a given system when provided with a set of \ncontrolled inputs. See also: emulator. \nSoftware Computer programs, procedures and \npossibly associated documentation and data \npertaining to the operation of a computer system. \nSoftware development life cycle  The activities \nperformed at each stage in software development, \nand how they relate to one another logically and \nchronologically. \nSoftware Hfe cycle  The period of time that begins \nwhen a software product ís conceived and ends \nwhen the software is no longer available for \nuse. The software life cycle typically includes a \nconcept phase, requirements phase, design phase, \nimplementation phase, test phase, installation \nand checkout phase, operation and maintenance \nphase and sometimes, retirement phase, Note \nthese phases may overlap or be performed \niteratively. \nSoftware quality The totality of functionality and \nfeatures Of a software product that bear on its \nability to satisfy stated or implied needs. \nSee also: quality. \nSpecification - A document that specifies, ideally \nin a complete, precise and verifiable manner, \nthe requirements, design, behavior, or other \ncharacteristics Of a component or system, and, \noften, the procedures for determining whether \nthese provisions have been satisfied. \nG N Crmpoge Loswming. A3 K Bt vt My et b o, s, o4 e i, s 18 . o, U 3 i, g, s sty conto rế b gy B y el smbs At \nh HH €mpert e e ee ety T b s e (e t t N t o s o € n n N N g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 261,
            "page_label": "262"
        }
    },
    {
        "page_content": "Stability The degree to which a component or \nsystem can be effectively and efficiently modified \nwithout introducing defects or degrading existing \nproduct quality. \nStandard  Formal, possibly mandatory, set of \nrequirements developed and used to prescribe \nconsistent approaches to the way of working or \nto provide guidelines (for example, ISO/EC \nstandards, IEEE standards and organizational \nstandards). \nState diagram - A diagram that depicts the states \nthat a component or system can assume, and shows \nthe events or circumstances that cause and/or result \nfrom a change from one state to another. \nSynonym: state transition diagram \nState transition - A transition between two states of \na component or system. \nState transition testing A black-box test techinique \nusing a state transition diagram or state table to \nderive test cases to evaluate whether the test item \nsuccessfully executes valid transitions and blocks \ninvalid transitions. See also: N-switch festing. \nSynonym: finite state testing \nStatement  An entity in a programming language, \nwhich is typically the smallest indivisible unit of \nexecution. \nSynonym: source statement \nStatement coverage The percentage of executable \nstatements that have been exercised by a test suite. \nStatement testing A white-box test technique \nin which test cases are designed to execute \nstatements, \nStatic analysis  The process of evaluating a \ncomponent or system without executing it, based \non its form, structure, content or documentation. \nStatic testing Testing a work product without code \nbeing cxecuted. \nStructural coverage Coverage measures based on \nthe internal structure of a component or system. \nStub A skeletal or special purpose implementation \nof a software component, used to develop or test a \ncomponent that calls or is otherwise dependent on \nit. H replaces a called component, \nSystem A collection of interacting elements \norganized to accomplish a specific function or set \nof functions. \nSystem integration testing Testing the combination \nand interaction of systems. \nGlossary 249 \nSystem testing Testing an integrated system to \nverify that it meets specified requirements. \nSystem under test (SUT) - A type of test object that \nis a system. \nTechnical review A formal review type by a \nteam of technically-qualified personnel that \nexamines the suitability of a work product for its \nintended use and identifies discrepancies from \nspecifications and standards, \nTest A set of one or more test cases, \nTest analysis The activity that identifies test \nconditions by analyzing the test basis. \nTestapproach The implementation of the test \nstrategy for a specific project. \nTest automation The use of software to perform \nor support test activities, for example, test \nmanagement, test design, test execution and results \nchecking. \nTestbasis Thebody of knowledge used as the basis \nfor test analysis and design. \nTestcase A xet of preconditions, inputs, actions \n(where applicable). expected results and \nPpostconditions, developed based on test conditions. \nTest case specification  Documentation of a set of \none or more test cases. \nTest charter  Documentation Of test activities \nín session-based exploratory testing. See alsoc \nexploratory testing. \nSynonym: charter \nTestcompletion The activity that makes \ntest assets available for later use, leaves test \nenvironments in a satisfactory condition and \ncommunicates the results of testing to relevant \nstakeholders. \nTest condition An aspect of the test basis that \nis relevant in order to achieve specific test \nobjectives, \nTestcontrol A test management task that deals \nwith developing and applying a set Of corrective \nactions to get a test project on track when \nmonitoring shows a deviation from what was \nplanned. \nSee also: test management. \nTest cycle Execution of the test process against a \nsingle identifiable release of the test object. \nTestdata Data created or selected to satisfy the \nexecution preconditions and inputs to execute one \nÓf more test cases. \nG N Compuge 1o2:92g A3 g Bt vt My et n o, s, o4 e n 0 s 18 . o, U 3 st e, s sty commr rê b n s n n el snbs At \nH €mpert e e b s s 8 even oy s e (e t đ o o € n 30 (s U",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 262,
            "page_label": "263"
        }
    },
    {
        "page_content": "250 Glossary \nTest data preparation tool - A type of test tool that \ncnables data to be selected from existing databases \nor created, generated, manipulated and edited for \nuse in testing. \nSynonym: test generator \nTest design The activity of deriving and specifying \ntest cases from test conditions, \nTest design tool - A tool that supports the test \ndesign activity by generating test inputs from a \nspecification that may be held in a CASE tool \nrepository, ©.g.„. requirements management tool, \nfrom specified test conditions held in the tool \nitself, or from code. \nTest environment An environment containing \nhardware, instrumentation, simulators, software \ntools and other support elements needed to conduct \na fest. \nSynonyms: test bed, test rig \nTest estimation The cakculated approximation \nOf a result relaled to various aspects of testing \n(for example, effort spent. completion date, costs \ninvolved, number of test cases, etc), which is \nusable even if input data may be incomplete, \nuncertain or noisy. \nTest execution The process Of running a test on the \ncomponent r system under test, producing actual \nresuli(s). \nTest execution schedule A schedule for the \nexecution of test suites within a test cycle. \nTest execution tool A test 1ool that executes tests \nagainst a designated test item and cvaluates \nthe outcomes against expected results and \npostconditions. \nTest harness A test environment comprised of \nstubs and drivers needed 10 execute a test. \nTestimplementation The activity that prepares the \ntestware needed for test execution based on test \nanalysis and design. \nTest infrastructure  The organizational artefacts \nneeded to perform testing, consisting of test \nenvironments, test tools, office environment and \nprocedures. \nTestinput The data received from an external \nsource by the test object during test execution, \nThe external source can be hardware, software or \nhuman. \nTestitem A part Of a test object used in the test \nprocess. \nSee also: test object \nTest leader  On large projects, the person who reports \nto the test manager and is responsible for project \nmanagement of a particular test level or a particular \nset of testing activities. See also: test manager. \nSynonym: lead tester \nTestlevel A specific instantiation of a test process. \nSynonym: test stage \nTest management The planning, scheduling, \nestimating. monitoring, reporting, control and \ncompletion of test activities, \nTest management tool A tool that provides support \nto the test management and control part of a test \nprocess. It often has several capabilities, such as \ntestware management, scheduling of tests, the \nlogging of results, progress tracking, incident \nmanagement and test reporting. \nTest manager The person responsible for project \nmanagement of testing activities and resources, \nand evaluation of a test object. The individual who \ndirects, controls, administers, plans and regulates \nthe evaluation of a test object. \nTest monitoring A test management activity \nthat involves checking the status of testing \nactivities, identifying any variances from the \nplanned or expected status and reporting status to \nstakeholders. See also: test management. \nTest object The component or system 1o be tested, \nSee also: test item. \nTest objective A reason or purpose for designing \nand executing a test. \nTestoracle A source to determine expected results \n1o compare with the actual result of the system \nunder test. \nSynonym: oracle \nTestplan  Documentation describing the test \nobjectives to be achieved and the means and \nthe schedule for achieving them, organized to \ncoordinate testing activities. \nTest planning “The activity of establishing or \nupdating a test plan. \nTestpolicy A high-level document describing the \nprinciples, approach and major objectives of the \norganization regarding testing. \nSynonym: organizational test policy \nTest procedure A sequence of test cases in \nexecution order, and any associated actions that \nmay be required to set up the initial preconditions \nand any wrap-up activities post execution. \nSee also: test script. \nG N Crmpoge Lowming. A3 gt Bt vt My et b o, s, o4 e, 0 s 18 . e, (b 3 i g, s sty st ey b gy B y el b At el i M e gyl oot s e Sy o el g ericncs Cengage Lewing BTG e (W 14 Y adlbmnd o o e o € g 30 (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 263,
            "page_label": "264"
        }
    },
    {
        "page_content": "Test process The set Of interrelated activities \ncomprising of test planning, test monitoring \nand control, test analysis, test design, test \nimplementation, test execution and test completion. \nTest process improvement A program of activities \ndesigned to improve the performance and maturity \nOf the organization's test processes and the results \nof such a program. \nTest progress report -A test report produced \nat regular intervals about the progress of test \nactivities against a baseline, risks, and alternatives \nrequiring a decision. \nSynonym: test status report \nTestreport [ocumentation summarizing test \nactivities and results. \nTest reporting Collecting and analyzing \ndata from testing activities and subsequently \nconsolidating the data in a report to inform \nstakeholders. Sec also: fest process. \nTest schedule A list of activities, tasks or \nevents of the test process, identifying their \nintended start and finish dates and/or times, and \ninterdependencies. \nTest script A sequence of instructions for the \nexecution Of a test. See also: test procedure. \nTest session  An uninterrupted period of time spent \nin executing tests. In exploratory testing, each \ntest session is focused on a charter, but testers can \nalso explore new opportunities or issues during a \nsession. The tester creates and executes on the fly \nand records their progress. See also: exploratory \nlesting. \nTest strategy Documentation that expresses the \ngeneric requirements for testing one or more \nprojects run within an organization, providing \ndetail on how testing is to be performed, and is \naligned with the test policy. \nSynonym: organizational test strategy \nTest suite A set Of test cases or test procedures to \nbe executed in a specific test cycle. \nSynonyms: test case suite, test set \nTest summary report - A test repoyt that provides an \nevaluation of the corresponding test items against \nexit criteria, \nSynonym: test report \nTest technique A procedure used to derive and/or \nselect test cases. \nSynonyms: test case design technique, test \nspecification technique, test technique, test design \ntechnique \nGlossary 251 \nTest tool - Á software product that supports one or \nmore test activities, such as planning and control, \nspecification, building initial files and data, test \nexecution and test analysis. \nTesttype A group of test activities based on \nspecific test objectives aimed ot specific \ncharacteristics of a component or system. \nTestability The degree of effectiveness and \nefficiency with which tests can be designed and \nexecuted for a component or system. \nTestable requirement - A requirements that is stated \nin terms that permit establishment of test designs \n(and subsequently test cases) and execution of tests \nto determine whether the requirement has been met. \nTester A skilled professional who is involved in the \ntesting of a component or system. \nTesting The process consisting of all life cycle \nactivities, both static and dynamic, concerned \nwith planning. preparation and evaluation of \nsoftware products and related work products o \ndetermine that they satisfy specified requirements, \n1o demonstrate that they are fit for purpose and to \ndetect defects. \nTestware Work products produced during the test \nprocess for use in planning, designing, executing, \nevaluating and reporting on testing. \nTraceability The degree to which a relationship can \nbe established between two or more work products. \nUnderstandability The capability of the software \nproduct to enable the user to understand whether \nthe software is suitable, and how it can be used for \nparticular tasks and conditions of use. See alsoc \nusabiliry. \nUnit test framework - A tool that provides an \nenvironment for unit or component testing in \nwhich a component can be tested in isolation or \nwith suitable stubs and drivers. It also provides \nother support for the developer, such as debugging \ncapabilities. \nUnreachable code  Code that cannot be reached and \ntherefore is impossible to execute. \nSynonym: dead code \nUsability The degree to which a component or \nsystem can be used by specified users to achieve \nspecified goals in a specified context of use. \nUsability testing Testing to evaluate the degree to \nwhich the system can be used by specified users \nwith effectiveness, efficiency and satisfaction in a \nspecified context of use. \nyy N Crmpogs Loswming. A3 K Bt vt My et b o, s, o4 b, 0 s 10 . o, U 3 i e, s sty st rế b n ee B y el snbs At s n H €mpert n a n H T t đ t t o € N N U s e ree Vnse:",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 264,
            "page_label": "265"
        }
    },
    {
        "page_content": "252 Glossary \nUse case - A sequence of transactions in a dialogue \nbetween an actor and a component or system with \na tangible result, where an actor can be a user or \nanything that can exchange information with the \nsystem. \nUse case testing A black-box test technique in \nwhich test cases are designed to execute scenarios \n0f use cases, \nSynonyms: scenario testing, user scenario testing \nUser acceptance testing Acceptancetesting \nconducted in a real or simulated operational \nenvironment by intended users focusing on their \nneeds, requirements and business processes., \nSce also: acceptance testing. \nUser interface  All components of a system that \nprovide information and controls for the user to \naccomplish specific tasks with the system. \nUser story A high-level user or business \nrequirement commonly used ín Agile software \ndevelopment, typically consisting of one sentence \nin the everyday or business language capturing \nwhat functionality a user needs and the reason \nbehind this, any non-functional criteria and also \nincludes acceptance criteria. See also: Agile \nsoftware development, requirement, \nV-model A sequential development life cycle model \ndescribing a one-for-one relationship between \nmajor phases of software development from \nbusiness requirements specification to delivery, \nand corresponding test levels from acceptance \ntesting to component testing. \nValidation Confirmation by examination and \nthrough provision of objective evidence that \nthe requirements for a specific intended use or \napplication have been fulfilled. \nVariable An clement of storage ín a computer that \nis accessible by a software program by referring to \nit by a name. \nVerification Confirmation by examination and \nthrough provision of objective evidence that \nspecified requirements have been fulfilled. \nWalkthrough A type of review in which an author \nleads members of the review through a work \nproduct and the members ask questions and make \ncomments about possible issues. See also peer \nreview. \nSynonym: structured walkthrough \nWhite-box test technique A procedure to derive \nand/or select test cases based on an analysis of the \ninternal structure of a component or system. \nSynonyms: structural test technique. structure- \nbased test technique, structure-based technique, \nwhite-box technique \nWhite-boxtesting Tesving based on an analysis of \nthe internal structure of the component or system. \nSynonyms: clear-box testing, code-based testing, \ntesting, structural testing, structure-based testing \nWideband Delphi An expert-based test estimation \ntechnique that aims at making an accurate \nestimation using the collective wisdom of the team \nmembers, \no N Compoge Lossming. A3 Kgh Bt vt My et b o, s, o4 b, s 10 . o, U 3 ot e, s s sty comtos rế b gy B n el smbs At el i M e gyt oot s o Sy s G el ey pericnce Cengag Liwng BTG e (W 14 s n kh Gd o o o € g 30 e , g :",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 265,
            "page_label": "266"
        }
    },
    {
        "page_content": "ANSWERS TO SAMPLE EXAM QUESTIONS \nThis section contains the answers and the learning objectives for the sample questions in each chapter and for the \nfull mock exam in Chapter 7. \n1f you get any of the questions wrong or Íf you were not sure about the answer, then the learning objective tells \nyou which part of the Syllabus to go back to ín order to help you understand why the correct answer is the right \none, The learning objectives are listed at the beginning of each section. For example, if you got Question 3 in \nChapter | wrong, then go to Chapter 1 and read Learning Objective 1.2.2. Then re-read the section in the chapter \nwhich deals with that topic. \nCHAPTER 1 FUNDAMENTALS OF TESTING \nQuestion | Answer | Learning objective \n1 a 121 \n2 b Keywords \n3 € 122 \nM a 111 \n5 a 13.1 \n6 (5 144 \n7 b 1.5.1 \n8 d 143 \nCHAPTER 2 TESTING THROUGHOUT THE \nSOFTWARE DEVELOPMENT LIFE CYCLE \nQuestion | Answer | Learning objective \n1 d 211 \n2 d 221 \n3 b Keywords \n4 b 241 \n5 c 231 \n6 d 23.3 \n7 c 233 \n8 b 23.1 \n9 a 221 \nG N Crmpuge Lowming A3 gt Bt vt My et b o, s, o4 B, 0 s 18 . o, U 3 ot e, s sty st ey b gy B n el b A e e n n H st — ì sO Lonmag murses e 14 10wy koo o o ey Vo € whacaes A (70T s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 266,
            "page_label": "267"
        }
    },
    {
        "page_content": "254 Answers to sample exam questions \nCHAPTER 3 STATIC TECHNIQUES \nQuestion | Answer | Loarning objective \n1 d 311 \n2 a 321 \n3 d 313 \n4 a 323 \n5 d 322 \n6 b 323 \n7 a 325 \n8 c 324 \n9 c 324 \nCHAPTER 4 TEST TECHNIQUES \nQuestion | Answer | Learning objective \n1 d 443 \n2 a 422 \n3 c 433 \n4 a 411 \n5 b 411 \n6 c 423 \n7 d 424 \n8 b 421 \n9 b 42 \n10 c 43 \n1\" a 425 \n12 L 43 \n13 a 44 \n14 C 431 \n15 d 43 \n16 b 411 \n17 a 424 \not S04 Compoge Loy N3 Kighn Bt vl My et x o s o4 Bl 0 b 18 o (b 3 o, . ot oty nti rr l ree s n n ol smd Akt s el i s e gyt oo s e uly S8 Tl g n peneser (g Linng AT e ( M 1 e nh ee o 4 ) o € b nghệ s g n",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 267,
            "page_label": "268"
        }
    },
    {
        "page_content": "Answers to sample exam questons 255 \nCHAPTER 5 TEST MANAGEMENT \nLearning objective \n511 \n512 \nKeywords \n521 \n525 \n522 \n523 \nKeywords \n531 \n532 \n561 \n541 \n55.2 \nS \n552 \n553 \n526 \n561 \n524 \n523 \nI \nị \nwiœ|=i|ơ|lvt|e|us|m2|— sS „ = w ® ¬ s s \ns„ialalzlsl|a|zla|lalalz|alalala|nll|z|lalz \n~ L \nCHAPTER 6 TOOL SUPPORT FOR TESTING \nQuestion | Answer | Learning objective \n611 \n611 \n6.1.2 \n6.13 \n6.2.1 \n622 \n623 \n¬ \nle|lu|e|lvulez|= \n„lœelzglelơinle \no S Compoge Loy N3 Kighn Btk My et x o s o4 Bl 0 b 18 o (o 3 s, . ot oty nti re l ree s n n ol smd g i el e i e s gyt oo s et Sy S8 el e peTience Cengage L K e (W 14 m n ol o o o € b 30 eev s",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 268,
            "page_label": "269"
        }
    },
    {
        "page_content": "256 Answers to sample exam questions \nCHAPTER 7 MOCK EXAM \nQuestion | Answer | Learning objective \n1 b 421 \n2 a 112 \n3 a 212 \n4 d 425 \n5 C 5.5.2 \n6 b 233 \n7 5 Chapter 4 Keywords \n8 b 5.6.1 \n9 c 512 \n10 < 141 \n\" d 321 \n12 b 142 \n13 a 152 \n14 b 531 \n15 a 432 \n16 a 611 \n17 c 441 \n18 b 526 \n19 a 421 \n20 b Chapters 1/4 Keyword \n21 b 3.1.1 \n22 b 623 \n23 a 123 \n24 b 221 \n25 ( 124 \n26 (5 422 \n27 c 423 \n28 d 442 \n29 b 2341 \n30 C 521 \n3 b 411 \n32 [ 313 \n33 d 111 \n34 d 541 \n35 (3 424 \n36 c 524 \n37 b 325 \n38 a 242 \n39 c 324 \n40 d 5.1.1 \not 309 Comprgs Lo A3 Boghts Bt My et b o, e ee 4 gl 18 nh n 0 8 .V n ot (g, sl oty et ey s g b l ol s oot 1| el s Nầ e gyt oo s e Sy 48 Tl Wiy (erience Cenge Liwng AT e (W 1 v K Gn o o ) o € s nghệ s gt n",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 269,
            "page_label": "270"
        }
    },
    {
        "page_content": "REFERENCES \nKey: \nIn Syllabus \nExtra to Syllabus \nCHAPTER 1 FUNDAMENTALS OF TESTING \nCHAPTER 2 TESTING THROUGHOUT THE SOFTWARE \nDEVELOPMENT LIFE CYCLE \nBerard, Edward V. (1993) Essays ơn Object-oriented Software Engineering (Volume 1), Prentice Hall: Englewood Cliffs, NJ \nBlack, R. (2017) Agile Testing Fowidations, BCS Leaming & Development Lid: Swindon, UK \nBochm. B. (1986) ‘A Sparal Modcl of Software Development and Enhancement’. ACM SIGSOFT Softwore Engineering Nowes, \nACM, 1144):14-25, August 1986 \nMars jpl.nasa.gov (1999) Mars climate orbiter failure boand releases report, munerous NASA actions underway ín response. \nIOnlmc) Avlilhle s h1lg>s||ilntsjpI n.ul.govfn-p?ﬂm:wvml I IO lmnl leud 02 Awﬂ 20]9] \nvan Veenendaal, È. (1999) ‘Practical Quality Assurance for Embedded Software', in Software Quality Professional, Vol 1, \nno. 3, American Socmy for anluy Jnnc 1999 \nvan V:emﬂnl E and v der ann. M_ 120'1)) '(K)M Based Impe«wns in l'nmadmx: o/ the l Ith Eumpmn Software \nControl and Almm (‘nqﬁmu ư:xom. Munich, Mty 20(!) \n257 \nG N Crmpoge Lowming. A3 Kt Bt vt My et e o, s, o4 A, 0 s 18 n o, U 3 i, g, s e paty comtr rế b E ee n y el b At v e e s / et — H SGeGHDSRSINGMNNSUNUN A o o 10} Vo € whacacm IS (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 270,
            "page_label": "271"
        }
    },
    {
        "page_content": "258 Relerences \nCHAPTER 4 TEST TECHNIQUES \nĐll<t R (Zﬂ.'n) ng-uhrSoﬂmwr TISIII‘. lnhu W’Iky & Son& N<vJ Yad( NY \nBrockman. B. and Nownboom E mm› Testing Embedded Sa[nmn Addison Weslty L \nPol M. Tznm-m R. ami van V«nndnl E (ZDDI) Saﬁumr hmu A Gude m dur Tﬂnp Aypmad: Addm Wd:y \nHarlow, UK \nwwwomg.org/spec/UML⁄2,.5,1/ [Accesscd 02 April 20191 \nWhittaker, J. A. (2003) How tơ Break Software: A Practical Guide to Tesling, Addison Wesley: Reading, MA \nCHAPTER 5 TEST MANAGEMENT \nPol. M. Ten-men L and van WmduL E (M) Saﬂ-mr Tumvp A GM: m m TMap Appmurh Addnnn Wedey. \nReading, MA \nvan Veenendaal, E. and Wells, B. (2012) Test Maturity Model integration TMMi: (Guidelines for Test Process Improvement), \nUTN Publishers: Den Bosch, The Netherlands \nWhittaker, 3. (2002) How to Break Software; A Practical Guide tơ Testing, Addison Weskey: Reading, MA \nWhittaker, J. and Thompson, H.H. (2003) How ¡o Break Sofrware Securiry, Adđisan Wesley: Reading, MA \nCHAPTER 6 TOOL SUPPORT FOR TESTING \nAdzic, G. (2011) Specification by Example, Manning Publications Co: Shelter lsland, NY \nAxclrod, A. (2018) Complete Guide to Test Automation: Techniques, Practices and Patterns for Building and Maintaining \n%vﬁr Soﬁ-mr Pm/mlr qnccs New York, NY \not S04 Crmpoge Loy N3 B E E HƯ s o4 A 0 b 18 o (b 3 o, . ot Ư K ey b E ol Ư s el e M e gyt oot s o Sy o el e pericncs Cengage Lewing x e (W 14 Y khn ee o o n o € g 430 e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 271,
            "page_label": "272"
        }
    },
    {
        "page_content": "AUTHORS \nDOROTHY GRAHAM \nDorothy Graham (Dot) has been involved in software testing for roughly 50 years, since 1970, when her first \njob (for Bell Labs ín the US) was as a programmer in a testing group, and her task was to write two testing \ntools (test cxccution and comparison). After emigrating to the UK (with her British husband), she worked for \nFerranti Computer Systems, developing software for UK Police forces. She then joined the National Computing \nCentre as a trainer and courseware developer, and later became an independent consultant. \nAt that time, software testing was not a respected profession; in fact, in the carly 1990s, many thought of testing \nat best as a necessary evil (if they thought of testing at all!). There were few people who specialized in testing, \nand it was seen as a second-class activity, and not well thought of. There was a general perception that testing \nwas easy, that anyone could do it, and that you were rather strange if you liked it. It was then that Dot decided to \nspecialize in testing, secing great scope for improvement in testing activities in industry, not only ín imparting \nfundamental knowledge about testing (basic principles and techniques) but also in improving the view testers had \nof themselves, and the perceptions of testers in their companies. She developed training courses in testing, and \nbegan Grove Consultants, named after her house in Macclesfield, UK. One of ber most popular talks at the time \nwas called Test ís a four-letter word, reflecting the prevailing culture about testing. \nIt was into this context that the initiative to create a qualification for testers was born. Although not the initiator, \nDot was involved from the first meetings and the earliest working groups that developed the first Foundation \nSyllabus, donating many hours of time to help progress this effort. This work was carried out with support from \nISEB (Information Systems Examination Board) of the British Computer Society, and the testing qualification \nwas modelled on ISEB's successful qualifications in Project Management and Information Systems Infrastructure. \nOne of the aims at this time was to give people a common vocabulary to talk about testing, since people seemed \n1o be using many different terms for the same thing. \nGrove Consultants (Dorothy Graham and Mark Fewster at that time) gave the first course based on the ISEB \nFoundation Syllabus in October 1998 and the first Foundation Certificates ín Software Testing were awarded, In \nher 20 years with them, Grove went on to be highly respected for the quality of their training material, particularly \nfor ISTQB courses. Grove continues to licence high-quality ISTQB training material to organizations wishing to \nprovide training without expending a significant effort in course development (www.grove.co.uk). \nThe success of the Foundation qualification took everyone by surprise. There seemed to be a hunger for a \nqualification that gave testers more respect, both for themselves and from their employers. It also gave testers a \ncommon vocabulary and more confidence in their work. The Foundation qualification had met its main objective \nof ‘removing the bottom layer of ignorance” about software testing. \nWaork then began on extending the ISEB qualification to a more advanced level (which became the ISEB \nPractitioner qualification) and also to extending it to other countries, as news of the qualification spread in the \ninternational community. Dot was a facilitator at the meeting that formed ISTQB in 2001 in Sollentuna, Sweden. \nShe has not been actively involved in the ISTQB Advanced levels. \nDorothy's other activities over the years include being Programme Chair for the first European testing confer- \nence (EuroSTAR 1993); she was Programme Chair again in 2009. During the 1990s, she started and later co-au- \nthored The CAST Report, a summary of commercial testing tools (in the days before the imternet!), In addition \nto all editions of this book on Foundations of Software Testing, she was co-author of four other books: Software \nInspection (1993) with Tom Gilb, Software Test Automation (1998) and Experiences of Software Test Automation \n(2012), both with Mark Fewster, and A Journey Through Test Automation Patterns (2018) with Seretta Gamba. \nHer book with Seretta is the story of a team using the test automation patterns wiki, TestAutomationPatterns.org. \nThis wiki, developed by Seretta and Dot, provides solutions that have worked for others for a number of issues and \nproblems in test automation. The issues and patterns are organized into four sections: Process, Management, Design \nand Execution. The wiki was first published ín 2013 and is a popular source of advice about test automation. \nyy N Crmpogs Lowming. A3 Khện Bt vt My et b o, s, o4 A, 0 s 18 . o, U 3 i, e, s sty comto rế b gy B n el snbs At s H e e a n — ì SGeHRSRSINUGMNNSUNIUN GSU GG GEGSDI ID GSC",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 272,
            "page_label": "273"
        }
    },
    {
        "page_content": "260 Authors \nDuring her career, Dot has spoken at numerous testing conferences and events worldwide. She was awarded \nthe second European Excellence Award in Software Testing in 1999 and was awarded the first ISTQB Excellence \nAward in Software Testing ín 2012, \nHer main non-testing activities include singing (choirs, madrigal groups and solos) and enjoyable holidays with \nher husband, Roger. They have two children, Sarah (married to Tim) and James. \nDorothy can be contacted on LinkedIn and www.DorothyGraham co.uk. \nREX BLACK \nWith over 35 years of software and systems engineering experience, Rex Black is President of RBCS \n(www.rbcs-us.com), a leader in software, hardware and systems testing. For 25 years, RBCS has delivered \nconsulting, training and expert services for software and hardware testing. Employing the industry's most \nexperienced and recognized consultants, RBCS builds and improves testing groups, trains teams and provides \ntesting experts for hundreds of clients worldwide. Ranging from Fortune 100 companies to start-ups. RBCS clients \nsave time and money through improved product development, decreased tech support calls, improved corporate \nreputation and more. As the leader of RBCS, Rex is the most prolific author practising in the field of software \ntesting today. More about RBCS Ís shown after Rex's biography. \nRex's popular first book, Managing the Testing Process, has sold over 100,000 copies around the world, \nincluding Japanese, Chinese, and Indian releases and is now in its third edition. In addition to Managing the \nTesting Process and Foundations of Sofrware Testing, Rex has written 12 other books on testing, including \nAdvanced Software Testing: Volume I, Advanced Software Testing: Volume 11, Advanced Software Testing: \nVolume I, Critical Testing Processes, Pragmatic Software Testing, Agile Testing Foundations, Mobile Testing, \nExpert Test Manager and Fundamentos de Prueba de Software. These works have also sold tens of thousands \nof copies, including Hebrew, Indian, Chinese, Japanese and Russian editions. He has written over 30 articles: \npresented hundreds of papers, workshops, and seminars; and given about 50 keynotes and other speeches at \nconferences and cvents around the world. \nRex is the past President of the International Software Testing Qualifications Board and the past President of \nthe American Software Testing Qualifications Board. He remains involved in the ISTQB, having served as Project \nManager of the Foundation 2018 Syllabus effort and as Chair of the Agile Working Group. \nRex is married to his college girfriend, Laurel Becker. They met in 1987 at the University of California, Los \nAngeles. They have two children, Emma and Charlotte, and three dogs, Kibo, Mmink and Roscoe. They live in \nBulverde, Texas and Lake Taboe, California. \nCOMPANY PROFILE: RBCS, INC. \nRex Black Consulting Services (RBCS) is a premier international testing consultancy specializing in consulting, \ntraining and expert services, Led by one of the most recognized and published industry leaders, Rex Black, RBCS \nis an established company you can trust to show you results, with the track record to prove it. \nSince 1994, RBCS has been both a pioneer and leader in quality hardware and software testing, Through its \ntraining, consulting and expert services, RBCS has helped hundreds of companies improve their test practices \nand achieve quality results. RBCS is based ín the US, and works with partners around the world. \nRBCS utilizes deep industry experience to solve testing problems and improve testing processes. It helps cli- \nents reduce risk and save money through higher-quality products and services. The consultancy’s goal is to help \nclients avoid the costs associated with poor product quality (such as tech support calls, loss of business, customer \ndissatisfaction, lawsuits and reputation damage) by helping them understand and solve testing and performance \nissues and build better testing teams. Whether it's customizing a program to fit your company's needs, or provid- \ning the hands-on experience and resources that will allow your testing team to grow professionally, RBCS helps \ncompanies produce better products and increase ROL RBCS pours its resources into ensuring our clients become \nsuccessful, \nG N Crmpoge Loswming. A3 K Bt vt My et b o, s, o4 M s, 0 s 18 . o, U 3 i, g, s ety comto rế b gy B n el smbs /Ch N h HH e e b n s 8 cven earwny S Lowmay murses e 1y 10 ey kbacnd o o a1 wow € whacacs YA (70T ts gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 273,
            "page_label": "274"
        }
    },
    {
        "page_content": "Authors 261 \nEmploying the industry's most experienced and recognized consuants, RBCS builds and improves testing groups, \ntrains teams and provides testing experts for hundreds of clients worldwide. RBCS’s principals include published, \ninternational experts n the area of hardware and software testing, Their skilled test managers and engineers all hold \nInternational Software Testing Qualifications Board (ISTQB) certifications and are highly trained. \nRBCS belicves ín the value of real-life experience and leadership which ís why the RBCS team is always \nstriving to improve and perfect the testing process. Every trainer and consultant is ISTQB certified and encouraged \nto write articles and books, share leading ideas through presentations, and continuously research new ideas. In \naddition, as past President of the ISTQB and the American Software Testing Qualifications Board (ASTQB), Rex \nBlack has more than 35 years of software and systems engineering experience and strongly leads the company \nwith the most recent, proven testing methodologies and strategies. \nRBCS' client-centred approach helps companies deliver better quality products and reduce costs by improving \ntheir test practices. The difference? RBCS's recognized industry experts started their careers ín our customers' \nshoes; they have the real-world knowledge required to deliver the best services with proven practices. \nFrom Fortune 100 to small and mid-size organizations, RBCS works with a varicty of companies, ín industries \nsuch as technology, finance, communications, retail, government and education, \nRBCS offers customized consulting services that will not only help you solve your testing challenges, but \nprovide you with the tools and framework for managing a successful testing organization, Our consulting \nmethodology is based on successful, peer-reviewed publications with a customized approach that focuses on the \nindividual needs of cach client. Plus we have a long list of happy, referenceable customers around the globe that \nhave benefited from RBCS expertise. \nRBCS, Inc. \n31.520 Beck Road \nBulverde, TX 78163 \nUSA \nwww.rbes-us.com \nemail: info@rbcs-us.com \nWeb: www.rbcs-uscom \nLinkedin: www.linkedin.com/in/rex-black \nYouTube: www.youtube com/uses/RBCSINC \nFacebook: www.fb.me/TestinglmprovedbyRBCS \nTwitter: @RBCS \nERIK VAN VEENENDAAL \nErik van Veenendaal is an internationally recognized testing expert, author of a number of books and has published \na large number of papers within the profession. He is currently working as an independent consultant and as the \nChief Executive Officer (CEO) of the TMMi Foundation, \nDr Enik van Veenendaal CISA graduated at the University of Tilburg in Business Economics. He has been \nworking as a practitioner and manager in the IT industry since 1987. Afier a carcer in software development, he \nmoved to the area Of software quality, where he specializes in software testing. \nAs a test manager and test consultant he has been involved in a great number and variety of projects, He has \nimplemented structured testing and reviews and inspections, and as a consultant has contributed to many test \nprocess improvement projects. He worked for Sogeti as manager of operations and was one the core developers \nof the TMap testing methodology. He is the author of numerous papers and 4 number of books on testing and \nsoftware, including the best sellers The Testing Practitioner, The Litrle TMMi, Foundations of Software Testing \nand Testing according to TMap. \nErik van Veenendaal founded Improve Quality Services BV, a company that provides consultancy and training \nservices in the areas of testing, requirements engineering and quality management. He has been the company \ndirector for over 12 years, Within this period Improve Quality Services became a leading testing company in \nG N Crmpoge Lowming. A3 g Bt vt My et b o, s, o4 e n 0 s 18 . o, U 3 e, s sty conto ey b gyt B n el b At e e a et n s et s 8 cven oy Conpuge Lonmay muries e 1y 10 ey kbacnd o o sy wow € whacacm YIS (70T ns gt",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 274,
            "page_label": "275"
        }
    },
    {
        "page_content": "262 Authors \nThe Netlerlands focused on innovative and high-quality testing services. Customers were especially to be found \nin the areas of embedded softwarc (e.g.. Philips, Océ cn Assembléon) and in the finance domain (c.g.. Rabobank, \nING and Triodos Bank). Improve Quality Services was market leader for test training in The Netherlands both \nin terms of quamity and quality. \nNowadays Erik is living ín Bonaire (Caribbean Netherlands) and is occupied as an independent consultant \ndoing consultancy and training in the areas of testing (especially based on ISTQB Syllabi) and requirements \nengincering. He also publishes and delivers keynote presentations on a regular basis. In addition, he is actively \ninvolved in the TMMi Foundation, International Software Testing Qualifications Board and the IREB require- \nments engineering organization. \nEnk, being one the initiators to found the TESTNET organization, is now an honorary member of TESTNET \n(the Dutch Special Interest Group in Software Testing). Erik was the first person to receive the ISEB Practitioner \ncertificate with distinction and is also a Certified Information Systems Auditor (CISA). Erik has also been a senior \nlecturer at the Eindhoven University of Technology. Faculty of Technology Management for almost ten years. \nSince its foundation in 2002, Erik has been strongly involved in the International Software Testing Qualifi- \ncations Board (1STQB). From 2005 till 2009 he was the vice-president of the ISTQB organization and he is the \nfounder of the local Belgium and The Netherlands board: the Belgium Netherdands Testing Qualifications Board \n(BNTQB). For many years, he was the editor of the ISTQB Standard Glossary of Terms used ín Software Testing \nand chair for the ISTQB Expert level working party. Today, he is the president of the Curacao Testing Quali- \nfications Board (CTQB). For his major contribution to the field of testing, Erik received the European Testing \nExcellence Award in December 2007 and the ISTQB International Testing Excellence Award in October 2015, \nErik can be contacted via email at erik@erikvanveenendaalnl and through his website www.erikvanveenendaal.nl. \nG N Crmpoge Lossming. A3 K Bt vt My et b o, s, o4 b . 0 s 10 . o, U 3 st g, s s paty costo rế b E ee B n el smbs /Ch n bl i Nầ e gyt oo s e Sy S Tl g pericncs CEngage L x e (W 14 Y oS o o n o € g ngh e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 275,
            "page_label": "276"
        }
    },
    {
        "page_content": "INDEX \nabsence-of-errors fallacy 11, 15 \nacceptance criteria 76 \nacceptance test-driven development \n(ATDD) 20 \ntwols 210 \nacceplance testing 39, 55-9 \ndifferent forms 56-7 \nobjectives 56, 60 \ntest level characteristics 60-1, 68 \naccessibility testing tools 214 \nad học review 92 \nAgile development 43-5. 46, 162, 167 \nbenefits 45 \ndevelopment process characteristics 172 \nfeedback 169 \nplanning poker 174 \nregression tests 66 \nteams 5, 15, 44, 45, 155, 158 \ntest-driven development 209-10 \ntest progress reporting 181 \ntesting 11, 12,45 \nAgile manifesto 44 \nALM see application lifecycle management \nalpha testing 57 \nanalysis and design of tests 163-4 \nanalytical strategy 165, 166 \nanti-malware software 67 \napplication lifecycle management (ALM) \n206-7.221 \nassigning risk level 188-9 \nATDD see acceptance test-driven development \nattitudes 29, 30, 156, 185 \nauthor \nformal review 86 \nrole/responsibility 86 \nautomation see test automation \nof regression tests 66 \nBDD see behaviour-driven development \nblack-box test techniques 10911, 112-32 \nboundary value analysis 115-21 \ndecision table testing 121-7 \nequivalence partitioning 113-15 \nstate transition testing 127-30 \nuse case testing 130-2 \nblack-box testing 55, 63, 64, 107 \nbottom-up estimation 173-4 \nbottom-up integration 52 \nboundary value analysis (BVA) 64-5, 115-21 \napplying more than once |18 \napplying to more than human inputs 119 \napplying to more than numbers 117-18 \napplying to output 118 \ncoverage 134 \ndesigning test cases 120 \nexercise 148, 149 \nextending 117 \nreasons for doing 120-1 \ntwo- and three-value analysis 119-20 \nusing with equivalence partitions 116 \nbrain, what to do with 163-4 \nbuddy check 88 \nbudget, choosing test design technique 108 \nbumdown charts 174 \nbusiness considerations/continuity 167 \nbusiness-process-based testing 63 \nBVA see boundary value analysis \ncapture/replay tools 210, 219 \ncaptured test 219-20 \nchallenges \nAgile testing 45 \nchecking rate (in reviews) 83 \nchange-related testing 66~7, 68 \nchecklist \nof past risks 188 \nof project risks 184 \nchecklist-based reviewing 92-3 \nbehaviour-driven development (BDD) 20 checklist-based testing 142 \nt001s 210 code 76,77, 78 \nbenefits of test automation 215-16 code coverage tools 79, 211 \nbeta testing 57 code smells 79 \nbi-directional traceability 27, 71 collapsed decision table 126, 151 \nbias 28-9 commercial off-the-shelf (COTS) 36, 57, 59, 164 \nbig-bang integration 52 communication 29-30, 78, 83, 98 \not S04 Crmpoge Loty N3 Kighh Bt vl My et b oo s o4 bl 0 b 18 ot (o 3 o, . ot ety st re b ree s B n ol sndis gt s el e A v gyt oot s e Sy S el g ericncs CEngage Ly BTG e (W 14 s n nh ee o o o € g 30 e , g n",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 276,
            "page_label": "277"
        }
    },
    {
        "page_content": "264 Index \ncomponent complexity 107 \ncomponent integration testing 50 \ncomponent testing 38, 48-50 \nobjectives 48 \ntest level characteristics 60-1, 67, 68 \nwhite-box 65 \nconfiguration management 182 \ntools 207 \nconfirmation testing (re-testing) 66 \nconsultative strategy 166 \ncontext-dependent test process 11, 14-16 \ncontingency plans 186, 187 \ncontinuous improvement (reviews) 99 \ncontinuous integration 44, 45, 50, 51, 53, 68, 186, \n207-8, 211,218, 225 \ncontinuous integration tools 207-8 \ncontracts 76 \ncontractual acceptance testing 56-7 \ncontractual issues (suppliers) 186 \ncontractual requirements 56, 108 \ncontrol actions 175-6 \ncontrol flow diagram 139, 146, 148, 153, 234 \nCOTS see commercial off-the-shelf \ncoverage 17, 132 \nboundary value analysis 134 \ndecision tables 134 \ndescription 133-4 \nequivalence partitioning 134 \nexit criteria 168 \nmeasurement 135 \nstate transition testing 134 \ntools 65, 211 \ntypes 134-5 \nwhite-box test techniques 111 \ncredit card example 125-7 \ncross-training 189 \ncustomer/contractor requirements 108 \ndata conversion/migration tools 214 \ndata quality assessment tools 213-14 \ndata-driven script 219 \ndata-driven testing 220 \nDDP see defect detection percentage \ndebugging 4-5, 6, 220 \ntools 49, 172, 206, 216 \ndecision coverage 138-9 \ndecision table \ncollapsing 126 \ncoverage 134 \nexercise 148, 1501 \not S04 Compoge Loy N3 Kighh Bt vl My et o s o4 Bl 0 b 18 ot (e 3 s, . ot ety st ey l ee s B n ol sndis Ak o1 bl i M e gyt oot s ot Sy S el ey periencs CEngg 12 veng BTG e (W 14 s n i Gg o o n o € g 30 e , g \ndecision table testing 121-7 \ncredit card worked example 125-7 \nusing for test design 122-5 \ndecision testing 138-9 \nvalue of 139-40 \ndefect density 14, 177-8 \ndefect detection percentage (DDP) 10, 32, 191 \nsee phase containment \ndefect management 190-5 \ntools 207 \ndefect removal models 174 \ndefect reports 85 \ncontents 1934 \nexercise 200, 202 \nlife cycle 195 \nobjectives 191-2 \nreasons for 190-1 \nwhat happens after filing 194-5 \nwriing 192-3 \ndefect types 9, 20, 92, 181 \ndefects 7 \nacceptance testing 58 \nbenefits of static testing 77, 78 \ncauses 7-9 \ncluster together 11, 13-14 \ncomponent testing 49 \nexit criteria 168 \nexpected types 109 \nfinding 28 \nfixing 85 \nintegration testing 51-2 \nopen and closed chart 178 \nrisk analysis 188 \nseverity of 83-4 \nsystem testing 54 \ntesting shows presence not absence 10, 11 \ntypical scenarios 8 \nwelcoming found defects 98 \ndefinition of done (DoD) 19, 158, 167, 181, 185 \ndefinition of ready 24, 167 \ndeveloper mindset 31-2 \ndevelopment life cycle models 36-7, 172 \nin context 46 \nexit criteria 168 \niterative/incremental 3945, 108 \nsequential 37-9, 108 \ndevelopment process characteristics 172 \nlife cycle model 172 \nstability/maturity of organization 172 \ntest approach 172",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 277,
            "page_label": "278"
        }
    },
    {
        "page_content": "test process 172 \ntime pressure 172 \n1001s used 172 \nDevOps 208 \ndirected strategy 166 \ndisaster recovery 56, 57, 58, 59 \ndocumentation 108, 171 \nDoD see definition of done \ndrivers 48, 52, 212 \ndry runs 93 \ndynamic analysis tools 213, 215 \n'dynamic comparison 221 \ndynamic strategy 166, 167 \ndynamic testing 76 \ndifference from static testing 78-9 \ncarly testing 11, 12-14, 37, 166 \neffective use of tools 222-4 \nend-to-end test 55 \nentry criteria 167-8 \nplanning stage 81 \nreviews 81 \ntest plan 24, 41 \nEP see equivalence partitioning \nepics 76 \nequivalence partitioning (EP) 113-15 \napplying more than once 118 \napplying to more than human inputs 119 \napplying to more than numbers 118-19 \napplying to output 118 \ncharacteristics 114-15 \ncoverage 134 \ndesigning test cases 120 \nexercise 148, 149 \nextending 117 \nreasons for doing 120-1 \nequivalence partitions (equivalence classes) 113-14 \nusing with boundary value analysis 116 \nerror 7 \nerror guessing 140-1 \nexam \nhow to approach multiple choice questions 230 \nmock exam 232-9 \npreparing for 228-9 \nrecognized exam providers 229 \nstudying for 228-9 \nSyllabus 229 \ntaking the exam 230-1 \ntrick questions 230-1 \nwhere to take cxam 229 \nIndex 265 \nexercises 1034, 148-153, 200-202 \nexam questions 34-35, 73-74, 101-102, 144-147, \n197-199, 227, 232-239 \nexhaustive testing 5, 11, 12,231 \nexit criteria 19, 168-9 \ncoverage 168 \ndefects 168 \nmeeting 86 \nmoney 168 \nplanning stage 81 \nquality 168 \nnsk 168 \nschedule 168 \ntest plan 24, 41 \ntests 168 \nexperience-based test techniques 63-4, 112, 140-2 \nchecklist-based testing 142 \nerror guessing 140-1 \nexploratory testing 141-2 \nexpert-based estimation 173, 174 \nexploratory testing 20, 22, 141-2 \nExtreme Programming (XP) 50, 209 \nfacilitator 87 \nfailure rate 177, 178 \nfailures 7,22 \nacceplance testing 58 \ncauses 8 \ncomponent tesling 49 \nintegration testing 51—2 \nsystem testing 54 \nfalse negative 55, 186 \nfalse positive 22, 55, 186 \nfinite state machine 127, 221 \nfixing and reporting 85-6 \nformal review 80 \nauthor 86 \nfacilitator/moderator 87 \nmanagement 86-7 \nreview leader §7 \nreviewers §7-8 \nroles/responsibilitics 86-8 \nscribe/recorder 88 \nFP see function points \nfunction points (FP) 178 \nfunctional testing 63-4, 67-8 \nguard condition 127 \nhigh-level test cases 25, 26, 142 \nhuman resources (HR) 6 \not S04 Crmpoge Loty N3 Bt Bt vl My et x oo s, o4 bl 0 b 18 ot (o 3 i, e, ot ety st ey l ree s B n ol snbis g s bl e A e 1 gyt oot s e Sy s Tl g pericnce Cengage 12 BTG e (W 14 s n kk ee o o n o € g ngh e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 278,
            "page_label": "279"
        }
    },
    {
        "page_content": "266 Index \niceberg 187 \nignore risk 187 \nimpact analysis 69, 701 \nincremental development life cycle 208 \nincremental development model 40 \nexamples 42-5 \ntesting 41-2 \nincremental integration testing 52 \nbottom-up 52 \nfunctional incremental 52 \ntop-down 52 \nindependent testing 32, 154-7 \nbenefits 155-6 \ndrawbacks 156 \nindependence varies 156-7 \nlevels of independence 155 \nindividual reviewing (checking) 82-3, 92 \ninfinity 12 \ninformal review 80 \npurpose 88-9 \ninspection 90-1 \ninstrumenting the code 205-6 \nintegration testing 38, 50-3 \nobjectives 50, 60 \ntest level characteristics 60-1 \ninterface specifications 78 \nInternational Organization for Standardization (1SO) \nEP/BVA and designing tests/measuring \ncoverage (ISONEC/IEEE 29119-4) 120, 121, \n122,130, 140 \nreview processes (1SO/IEC 20246) 80 \nsoftware quality (ISOVIEC 25010) 165 \nsoftware testing (ISO/IEC/IEEE 1818 29119-1) 5 \ntest processes (1SO/TEC/IEEE 29119-2) 17 \ntest work products (1SO/TEC/IEEE 29119-3) 24, \n162, 165 \nInternational Software Testing Qualification Board \n(ISTQB) \ndefinition of software testing 2, 28, 53 \nFoundation Exam 228-31 \ntest estimation 173 \ntest process 169 \ntest strategy 164 \ninternationalization localization 214 \ninternationalization testing tools 211 \nInternet of Things (1oT) 2, 46 \ninvalid transition testing 129-30 \n10T see Internet of Things \nissue communication and analysis (ín reviews) 83 \nISTQB see International Software Testing \nQualification Board \niterative development life cycle 208 \niterative development model 40 \nexamples 42-5 \ntesting 41-2 \nJava Virtual Machine 67 \nKanban 36, 43 \nKey Performance Indicator (KPI) 17 \nkeyword-driven scripts 219 \nkeyword-driven testing 220 \nKPI see Key Performance Indicator \nKSLOC see thousands of source lines of code \nlegal acceptance testing 55 \nlegal compliance 171 \nlife cycle models see development life \ncycle models \nlinear script 219 \nload test 212 \nlocalization testing tools 214 \nlogging \ndefect 191, 196 \nforms 82 \nin review meeting 83-4, 88 \ntest 141, 206 \ntools 85, 210-12 \nlow-level test cases 25-6, 173 \nmaintenance testing 69-71, 79 \nimpact analysis 70-1 \nregression testing 70-1 \ntriggers 70 \nmanagement \nformal review 86-7 \nnegotiation with 174 \nrole/responsibilities 86-7 \nsupport as critical 96 \nmanagement tools see test management tools \nMBT see model-based testing \nmethodical strategy 165 \nmetrics in testing \ncommon 176-9 \npurpose 176 \nmetrics-based techniques 174 \nmigration 69, 70 \nmindset 31-2, 95 \nmitigation of risk 189 \no S Compogs Loty N3 Kighn Bt nd Moy et x oo, . o4 Bl 0 b 18 ot U 3 i, . ot st ey b n\"p ee B n n ol snbis A s bl s M e gyt oo s e Sy s Tl g pericncs (Engg Liwing BTG e (W 14 Y i Gd o o o € m n re ngh e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 279,
            "page_label": "280"
        }
    },
    {
        "page_content": "mock exam 232-9 \nabout 229 \nmock objects (stubs) 48, 52,212 \nmodel-based strategy 165, 167 \nmodel-based testing (MBT) 76, 221 \ntools 209 \nmoderator 87 \nmodification 69, 70 \nmoney, exit criteria 168 \nmonitoring of data/information 175 \nmonitoring tools 213 \nnatural language text 77 \nnon-functional testing 64-5, 68 \nOAT see operational acceptance testing \nobjectives \nacceptance testing 56, 60 \nchoosing testing strategy 167 \ncomponent testing 48, 60 \nintegration testing 50-1, 60 \nsystem testing 53, 60 \npilot project 223-4 \noperational acceptance testing (OAT) 56, 57-8 \norganizational issues 185 \npersonnel 185 \nskills/training 185 \nuses, business staff, subject matter 185 \norganizational stability/maturity 172 \npair programming 80, 155 \npair review 80, 88 \npair testing 80 \npairing 80, 88 \npattern recognition 205 \npeople characteristics 172-3 \nskills/experience 172 \nteam cohesion/leadership 172-3 \nperformance measurement timing 205 \nperformance measurement tool support 212-13 \nperformance testing tools 212-13 \nperformance tests 170 \nperspective-based reading 94-5 \npesticide paradox 11, 14 \nphase comainment 8 \nsee defect detection percentage \npilot project 158, 2234 \nplanning \nadvanced 182 \ndefinition 18 \nIndex 267 \nquality 6 \nquality control 6,7 \nsee test planning \nplanning review process 80-2 \ndefine scope 80 \nentry/exit criteria 81 \nestimate efforttimeframe 81 \nidentify characteristics of review 81 \nparticipant selection 81 \npolitical issues \nattitudes 185 \ncommunication 185 \ninformation 185 \nportability testing tools 215 \npost-execution comparison 221 \npresence of defects 10, 11 \nprobe effect 205-6 \nprocedural roles 94 \nprocess- or standard-compliant strategy 165-6 \ncharacteristics 171 \nchoosing test stralegy 167 \nrisk 184 \nproduct characteriscs 171 \ncomplexity of product domain 171 \ngeographical distribution of team 171 \nlegal/regulatory compliance 171 \nquality characteristics 171 \nquality of test basis 171 \nrisks associated with 171 \nsize of product 171 \ntest documentation 171 \nproduct risk 184 \ncomputation performance 184 \nloop control structure 184 \nresponse times 184 \nsoftware performance 184 \nuser experience feedback 184 \nproduction environment 8, 53-5, 56, 59, 61, 67, 170, 175 \nproject issues \ndelays 185 \ninaccurate estimates/reallocation of funds 185 \nlate changes 185 \nproject risk 184-6 \norganizational issues 185 \npolitical issues 185 \nproject issues 185 \nsupplier issues 185-6 \ntechnical issues 185-6 \not S04 Crmpoge Loty N3 Kt Bt vl My et x oo s o4 bl 0 b 18 ot (o 3 i, . ot ety st ey l n\"p Ị s B n ol snbis Akt s el i M e 1 n H neeý oot s e Sy s ool g pericncs Cengage Ly x e (W 14 m n i ee o o n o € g nghn (s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 280,
            "page_label": "281"
        }
    },
    {
        "page_content": "268 Index \nproof-of-concept evaluation 223 \nproof of correctness 11 \npsychology of testing 28 \nattitudes 29 \nbias 28-9 \ncommunication 29-31 \nfinding defects 28 \nreview/test own work 29 \ntester's/developer’s mindsets 31-2 \nquality 6 \ncharacteristics 171 \nexit criteria 168 \nof test basis 171 \nquality assurance (QA) 6-7 \nquality control 6, 7 \nquality management 6 \nRational Unified Process (RUP) 36, 42 \nreactive strategy 166, 167 \nrecorded script 219 \nrecorder 88, see scribe \nrecord/playback tools 210 \nregression testing 66, 70-1 \nregression-averse strategy 166 \nregulations, choosing test strategy 167 \nregulatory \nacceptance testing 55, 56-7 \ncompliance 171 \nstandards 107 \nrepetitive work 215 \nrequirements coverage tools 211 \nfequirements management tools 207 \nrequirements-based analytical strategy 167 \nrequirements-based testing 12, 63 \nretirement 70 \nreview 80 \nexercise 103-5 \nfixing/reporting 85-6 \npurpose 91 \nroles 86 \ntool support 208 \nreview champion 95 \nreview culture 95 \nreview meeting 83-5 \ndecision-making 84-5 \ndiscussion 84 \nlogging 834 \nmanaging 98 \nTeview process \ncommunication/analysis 83 \nindividual 82-3 \ninitiate 82 \nplanning 80-2 \nreview role 80, 94 \nreview support tools 208 \nreview techniques \nad hoc 92 \napplying 92-5 \nchecklist-based 92-3 \nperspective-based 94-5 \nrole-based 93-4 \nscenanio-based 93 \nreview types 88-91 \ninformal 88-9 \ninspection 90-1 \npeer 91 \ntechnical 90 \nwalkthrough 89 \nreviewers \npicking right people 97 \nrole/responsibilities 87-8 \nreviews, success factors 95-9 \nclear objectives 95 \ncommunication 98 \ncontinsously improve processfiools 99 \ndoing the work well 97 \nfollow rules but keep it simple 98 \njust do it 99 \nlimit scope 96 \nlimit scope/pick things that count 97-8 \nmanagement support 96-7 \norganizational 95 \npeople-related 97 \npick right type/ftechniques 95-6 \npicking right reviewers 97 \ntime scheduling 96 \ntrain participants 99 \ntrust is critical 98 \nup-to-date materials 96 \nusing testers 97 \nwelcome defects found 98 \nwell-managed meetings 98 \nnsk 4, 69 \nassociated with product 171 \ndefinition 183 \nexit criteria 168 \nlevelsftypes 108, 167, 183, 188-9 \not S04 Crmpoge Loy N3 Bt Bt vl My et x oo s o4 Bl 0 b 18 ot U 3 s, . ot ety st ey l ee s B n ol snbis kg i bl e A e gyt oot s o Sy 4 el g ericncs Cengage Ly BTG e (W 14 s n kh ee o o n o € g 30 e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 281,
            "page_label": "282"
        }
    },
    {
        "page_content": "mitigation options 186, 189 \nproduct 184 \nproject 184-6 \nregression 167 \nsummary 189-90 \nusing tools 210-12 \nrisk analysis 188 \nbrainstorming 188 \nchecklist of typicalipast risks 188 \nchoosing test strategy 167 \nclose reading of documentation 188 \none-to-one or small group sessions 188 \nquality characteristics/sub-characteristics 188 \nreview test failures 188 \nspecific risks 188 \nstructure 188 \nteam-based approach 188 \ntriage risks 188 \nrisk management 167, 186-7 \nactivities 187 \ncontingency 186 \nignore 187 \nmitigate 186 \ntransfer 187 \nrisk-based testing 12, 179, 187 \nrisks of test automation 215-19 \nrole-based reviewing 93-4 \nroles/responsibilities (formal reviews) \n86-8 \nauthor 86 \nfacilitator’moderator 87 \nmanagement 86-7 \nreview leader 87 \nreviewers 87-8 \nscribe/recorder 88 \nroot couse 9-10 \nrules of formal review 98 \nRUP see Rational Unified Process \nscenario-based review 93 \nscheduling of tests 168, 169 \nseribe 88, 90 \nscripting 219-20 \ncapturing 219-20 \nlevels 219 \nseript-free/code-free 219 \nSerum 36, 42-3 \nsecurity testing 63, 214-5 \nsecurity vulnerabilities 79 \nIndex 269 \nsequential development models 37-9, 174 \nseverity of defect 834 \ncritical 83 \nmajor 84 \nminor 84 \nshared scripts 219 \nshelf-ware 211, 217 \nshift left 11, 13 \nSIT see system integration testing \nskills 160 \napplication/business domain 160 \nchoosing test strategy 167 \ndevelopment process characteristics 172 \norganizational issue 185 \npeople characteristics 172 \nteam 160, 164 \ntechnology 160 \ntesting 160 \ntesting staff 160 \nsoftware 1-2 \ndevelopment life cycle models see development \nlife cycle models \nexpected use of 108 \npackage 4 \nspecialized testing needs tool support 213-15 \nspecifications 71, 78 \nSpiral (or prototyping) 36, 43, 46 \nstandard-compliant strategy 167 \nstate diagram 60, 127-8, 147, 148, 151-2, \n209, 221 \nstate table 127, 129-30, 134, 148, 152 \nstate transition testing 127-30 \ncoverage 134 \nexamples 127 \nexercise 148, 151-2 \ninvalid transitions 129-30 \nmodel 128 \nvalid 128-9 \nstatement and decision testing exercise \n148, 153 \nstatic analysis 76 \ntools 208, 215 \nstatic techniques 75-99 \nreview process 79-99 \ntest process 75-9 \no S0 Crmpoge Loty N3 B Bt vl My et x o s o4 bl 0 s 18 ot U 3 s, . ot ety st ey b ree s B n ol snbis Ak o1 bl i A e gyt oot s et Sy 4 G el g ericncs Cengage 12 BTG e (W 14 Y i n o o n o € g 30 e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 282,
            "page_label": "283"
        }
    },
    {
        "page_content": "270 Index \nstatic testing 76 \nbenefits 77-8 \ndifference with dynamic testing 78-9 \ntool support 208 \nwork products 76-7 \nstrategy see test strategy \nstress test 212 \nstructured scripts 219 \nstubs see mock objects \nsuccess factors for reviews see reviews, SUCCCSS \nfactors \nsuccess factors for tools 224 \nsupplier issues \ncontractual 186 \nthird-party failure to deliver 186 \nSUT see system under test \nsystem complexity 107 \nsystem integration testing (SIT) 50, 59, 68 \nsystem testing 39, 53-5 \nobjectives 53, 60 \ntest level characteristics 60-1, 67-8 \nsystem under test (SUT) 54, 58 \nTDD see test-driven development \nteams \nAgile 5, 15, 44, 45, 155, 158 \nattitudes 29, 30, 156 \nbenefits/challenges 45 \ncharacteristics 44 \ncohesion 172-3 \ncommunication 29-30, 46, 78 \ncross-functional 12, 44 \ndefect-detection effectiveness 32 \nexpectations 19 \nexpertise 16, 155 \nformal/informal reviews 80, 86, 90 \ngeographical distribution 171 \ninternal or third-party 54, 55 \nKanban 43 \nleaders 157, 172-3 \nlevels of independence 155-7, 159 \nmission 163 \nmultiple 18 \norganization 155 \nroot cause analysis 9 \nScrum 42-3 \nsize of team 13 \nskills 160, 164 \nsupporUtraining 158 \ntest planning process 162 \ntesting/debugging 4-5, 7, 10 \ntechnical debt 186 \ntechnical issues \ndata conversion 186 \npoor defect management 186 \nrequirements not met 185 \nrequirements/user stories 185 \ntest environment 186 \nweaknesses in development process 186 \ntechnical review 90 \ntechnology \nadvances in 1-2 \noverestimation of knowledge in 160 \nskills needed 160 \ntest activities 2, 3-4, 11, 12-13, 14, 15, 16, 17-23, \n37,46.47, 63, 157, 158, 163, 167, 175, 176, \n180-1, 187, 204, 205, 210, 215 \ntest analysis 19, 20 \ncapture bi-directional traceability between each \nelement of test basis 20 \nevaluate test basisAest items 20 \nidentify features 20 \nidentify/prioritize test conditions 20 \ntest basis appropriate to test level 19 \ntest approach 164-7 \ndevelopment process characteristics 172 \nsee test strategy \ntest automation 215-19 \ntest automation (benefits of using wols) 215 \ncasier access to information about testing 216 \ngreater consistency/repeatability 216 \nmore objective assessment 216 \nreduction in repetitive manual work 215 \ntest automation (risks of using wols) 216 \nmaintaining test assets may be underestimated 217 \nnew platformAechnology may not be supported 218 \nno clear ownership of tool 218 \nopen source project may be suspended 218 \nskills of tester not the same as skills of user 219 \ntime, cost, effort may be underestimated 217 \ntime/effort may be underestimated 217 \ntool may be relied on too much 217 \ntool vendor may go out of business 218 \nunrealistic expectations 216 \nvendor may provide poor response for support, \nupgrades, defect fixes 218 \nversion control of test assets may be neglected \n217-18 \not S04 Crmpoge Loy N3 Kighh Bt vl My et x oo x ee o4 Bl 0 b 18 ot U n s, . ot ety st n l ee s B n ol smd kg i bl e A e gyt oot s o Sy 4 el g ericncs Cengage Ly BTG e (W 14 s n kh ee o o n o € g 30 e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 283,
            "page_label": "284"
        }
    },
    {
        "page_content": "test basis 17, 19-20, 47, 49 \nacceptance testing 57-8 \ncomponent testing 49 \nintegration testing 51 \nsystem testing 54 \ntest case 12,20, 21,22, 23, 24, 25-6, 27, 44-5, 47,48, \n50, 63-4, 66-7. 69. 71, 78,97, 106-7, 109-15, \n118, 120, 121,122, 125, 127, 128-38, 141, \n148-51, 159, 161, 169, 171, 174, 176-8, 180, \n185, 186, 192, 194, 196, 204, 209, 212, 219, 221 \ntest case summary worksheet 177 \ntest completion 23, 24, 26-7, 47, 169, 176, 181 \ntest conditions 12, 13, 19-21, 22, 23, 24-6, 27, 43,47, \n63-4,76,78, 97, 106, 107, 110, 111, 112, 113, \n116, 117, 118, 120, 129, 141, 142, 159, 165, 178 \ntest control 18, 158, 175-6, 180, 207 \n1651 coveruge see coverage \ntest data 21 \npreparation tools 209 \ntest design 21 \ntooÌ support 209-10 \nusing decision tables 122-5 \ntest-driven development (TDD) 50 \nt001s 209-10 \ntest driver tools 212 \ntest effort 11, 12, 14, 18, 122, 141, 165, 169-73, 176, 181 \ntest environment 8, 21-2, 23, 25, 41, 47-8, 50, 53, 55, \n57,59, 158, 159, 164, 167, 170, 172, 175, 176, \n178, 182, 184, 186, 191, 193, 202, 206, 212,215 \ntest exit criteria 4, 19, 24, 41, 59, 65, 81, 82, 83, \n84-6, 91,95, 158, 161, 164, 167-9, 175, 176, \n178, 179, 180, 181, 185 \ntest estimation 161, 169-74 \nbottom-up/top-down 174 \nbumdown charts 174 \ndefect removal models 174 \nexpert-based 173, 174 \nmathematical models 174 \nmetrics-based 174 \nnegotiation with management 174 \nplanning poker 174 \nproject classification 174 \ntechniques 1734 \ntester-to-developer ratio 174 \nWideband Delphi 174 \ntest execution 22-3, 41 \nschedule 21, 169 \nschedule exercise 200, 201 \ntools 210-12, 219-21 \nIndex 271 \ntest harnesses 48, 212 \ntest implementation 18, 21-2, 25-6, 39, 41, 106, \n169, 170 \ntest levels 36, 38, 47-8 \nacceptance testing 55-9 \nchange-related 68 \ncharacteristics 59-61 \ncomponent testing 48-50 \ncoordination between 163-4 \nfunctional 67-8 \nintegration testing 50-3 \nnon-functional 68 \nsystem testing 53-5 \nwhite-box 68 \ntest management 154-95 \nconfiguration management 181-2 \ndefect management 190-5 \nmonitoring/control 175-81 \norganization 154-60 \nplanning/estimation 161-73 \nrisks and testing 183-90 \ntest management tools 206-8, 221-2 \napplication lifecycle management 206-7 \nconfiguration 207 \ncontinuous integration 207-8 \ndefect 207 \nrequirements 207 \ntest manager 157 \ntasks 157-8 \nTest Maturity Model integration \n(TMMi) 165 \ncommon 176-7 \ntest metrics 176 \ntest monitoring 18-19, 24, 163, 169, 175, 176 \ntest objectives 34, 47, 62 \nacceptance testing 56 \nchoosing test technique 108 \ncomponent testing 48 \nintegration testing 50-1 \nsystems testing 53 \ntest objects 4, 47,49 \nacceptance testing 58 \ncomponent testing 49 \nintegration testing 51 \nsystem testing 54 \ntest oracle 26, 111,209 \ntest organization 154-9 \nindependent testing 154-7 \ntasks Of test managerfiester 157-9 \no N4 Compogs Loy N3 Kighn Bt vl Moy et b oo, . o4 B 0 b 18 ot L 3 i, e, ot sty st re b n\"g ee B n ol snbis rCvv i bl s M e gyt oot s o Sy s Tl g pericnce CEngg Ly BTN e (W 14 Y n Gd o o o € g ngh e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 284,
            "page_label": "285"
        }
    },
    {
        "page_content": "272 Index \ntest plan 18 tooÌ versus people 205 \ncommunication 162 tools that affect their own results 205-6 \ndecision-making 164 types 205 \ninformation gathering 164 test tools \nintegration/coordination 164 benefits/risks 215-19 \nmanage change 162 considerations 203-15 \nmultiple 162-3 effective use of 222-4 \npurpose/content 161-4 performance timing measurement 205 \nresources required 164 probe effect 205-6 \ntemplates 164 selection 222 \nwriting 161-2 special considerations 219-222 \ntest planning 18 changed-related 66~7 \nprocess 162 functional 63-4, 67-8, 68 \nwhat to do with your brain 163-4 non-functional 64-5, 68 \ntest procedure 21-2, 23, 25, 26, 76, 159, 161, 169, 204 test levels 67-8 \ntest process 16 white-box 65, 68 \nactivities/tasks 17-23 test types 62-8 \nin context 16-17 test work products 16, 17, 23-7, 30, 181 \ntest progress report 180 tester-to-developer ratio 174 \ntest reports testers 6, 97, 157 \naudience/effect on 181 knowledge/skills 108 \ncontent 180-1 mindset 31-2 \npurpose 179-80 previous experience using test technigues 108 \ntest results 173 skills needed 160 \namount of rework required 173 tasks 158-60 \nnumber/severity of defects found 173 testing \ntestseript 21, 26, 76, 141, 170, 182, 204, 223 accessibility 214 \ntest specification tool support 209-10 contribution to success 5-6 \ntest strategy 164-7, 171 definition 1-3 \nanalytical 165 dynamic 4 \nchoice of 168-9 exit criteria 168-9 \ndirected/consultative 166 factors affecting 170-3 \nfactors to consider 167 independent 154-7 \nmethodical 165 1ocalization 214 \nmodel-based 165 necessity of 5-10 \nprocess- or standard-compliant 165-6 objectives 3-4 \nreactive/dynamic 166 pervasive 160 \nregression-averse 166 portability 215 \ntest suites 21 process 2 \ntest summary report 180 psychology of 28-32 \ntest techniques 106 purpose of 163 \ncategories/characteristics 109-12 security 214-15 \nchoosing 106-9 splitting in various levels 163 \nexercises 148-53 time pressures 168 \ntest tool classification 204-6 underestimating knowledge required 160 \nactivities 205 usability 214 \ncategories 206 testing principles 10-15 \nlicencing model 205 absence-of-errors fallacy 11, 15 \npurpose 205 defects cluster together 11, 13-14 \no N4 Compogs Loy N3 Kighn Bt vl Moy et b oo, . o4 B 0 b 18 ot L 3 i, e, ot sty st re b n\"g ee B n ol snbis rCvv i bl s M e gyt oot s o Sy s Tl g pericnce CEngg Ly BTN e (W 14 Y n Gd o o o € g ngh e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 285,
            "page_label": "286"
        }
    },
    {
        "page_content": "early testing saves time/money 11, 12-13 \nexhaustive testing is impossible 11, 12 \npesticide paradox 11, 14 \ntesting is context dependent 11, 14-15 \ntesting shows presence of defects 10, 11 \ntestware 22, 76, 205, 206-8 \nthousands of source lines of code (KSLOC) 178 \nthree-value boundary analysis 119-20, 121 \ntime \nchoosing test technigue 108 \nplanning review process 81 \npressure 172 \nTMMi see Test Maturity Model integration \ntool support 71, 205 \ndynamic analysis 212-13 \nlogging 210-12 \nmanagement of testing/testware 206-8 \nperformance measurement 212-13 \nspecialized testing needs 213-15 \nstatic testing 206 \ntest design/specification 209- 10 \ntest execution 210-12 \ntools \navailability 108 \nbenefits of using 215-16 \ndevelopment process characteristics 172 \neffective use 222-4 \nprinciples of selection 222-3 \nrisks of using 216-19 \nselectionimplementation 158 \nsuccess factors 224 \nversus people 205 \ntop-down estimation 174 \ntop-down integration 52 \ntraceability 20-1,27,79 \ntraining \ndevelopers 189, 190 \norganizational issue 185 \nperformance testing 170 \nreview participants 87, 99 \nfor reviews 96, 97, 99 \nteams 158 \ntesters 156, 158 \nunderestimating time, cost, \neffort 217 \ntransfer risk 187 \ntriggers for maintenance 70 \nTrojan horse 107 \ntwo-value boundary analysis 119-20, 121 \nIndex 273 \nUAT see user acceplance testing \nUl see User Interface \nunit test framework tools 212 \nusability testing tools 214-15 \nuse case testing 130-2 \nuser acceptance testing (UAT) 56 \nuser experience (UX) feedback 184 \nuser guides 76 \nUser Interface (UI) 46 \nuser stories 76 \nUX see user experience \nV-model 38-9, 46 \nvalid transition testing 128-9 \nvalidation 3 \nverification 3 \nviewpoint roles 94 \nvirtualization 48, 170 \nvolume test 212 \nwalkthrough 89-90 \nwaterfall model 37, 38 \nweb pages 76 \nwhite-box testing 65, 68, 107, 132-40 \ncoverage 111, 133-5 \ndecision testing/coverage 138-9 \ndesign 135-6 \nstatement testing/coverage 136-7 \ntechniques 111-12, 132-140 \nvalue of statement/decision testing 139-40 \nWideband Delphi estimation 174 \nwork product roles 934 \nwork product testing 23-4 \nanalysis 24-5 \ncompletion 26-7 \ndesign 25 \nexecution 26 \nimplementation 25-6 \nmonitoring/control 24 \nplanning 24 \nwork products \nreview process 80-6 \nstatic testing of 76-7 \ntypes 76 \nwork-breakdown structures 170 \nworking backward 170 \nXP see Extreme Programming \nZeno's paradox 10 \no N4 Compogs Loy N3 Kighn Bt vl Moy et b oo, . o4 B 0 b 18 ot L 3 i, e, ot sty st re b n\"g ee B n ol snbis rCvv i bl s M e gyt oot s o Sy s Tl g pericnce CEngg Ly BTN e (W 14 Y n Gd o o o € g ngh e , g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 286,
            "page_label": "287"
        }
    },
    {
        "page_content": "ot 309 075 e Lo A3 Woghts o My n b o, e ee 4 gl 8 nh b 8 n .V n n e (g, sl oty et ey l g b l et s o Mgt 11 el e s b gyl oo s e o Tl W g n peneser (g L .ạ AT e (W 1 e Kh ee o o ) e € g nghệ s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 287,
            "page_label": "288"
        }
    },
    {
        "page_content": "ot 309 075 e Lo A3 Woghts o My n b o, e ee 4 gl 8 nh b 8 n .V n n e (g, sl oty et ey l g b l et s o Mgt 11 el e s b gyl oo s e o Tl W g n peneser (g L .ạ AT e (W 1 e Kh ee o o ) e € g nghệ s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 288,
            "page_label": "289"
        }
    },
    {
        "page_content": "ot 309 075 e Lo A3 Woghts o My n b o, e ee 4 gl 8 nh b 8 n .V n n e (g, sl oty et ey l g b l et s o Mgt 11 el e s b gyl oo s e o Tl W g n peneser (g L .ạ AT e (W 1 e Kh ee o o ) e € g nghệ s g",
        "metadata": {
            "producer": "Pdftools SDK",
            "creator": "PyPDF",
            "creationdate": "",
            "moddate": "2025-07-11T08:53:29+00:00",
            "source": "data/Foundations+of+SW+Testing+ISTQB+Certification+by+Erik+van+Veenendaal+2019-compressed.pdf",
            "total_pages": 290,
            "page": 289,
            "page_label": "290"
        }
    }
]